import json
import logging
from typing import List, Tuple, Dict, Union, Optional
from rank_bm25 import BM25Okapi
import pickle
from pathlib import Path
import faiss
from src.api_requests import BaseQwenProcessor,BaseGeminiProcessor
from dotenv import load_dotenv
import os
import re
import numpy as np
from src.reranking import LLMReranker
from src.financial_glossary import (
    find_financial_concepts,
    format_concepts_for_prompt,
)
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock

_log = logging.getLogger(__name__)


def extract_years_from_question(question: str, expand_window: bool = True) -> List[int]:
    """
    ä»é—®é¢˜ä¸­æå–å¹´ä»½ä¿¡æ¯ï¼Œå¹¶å¯é€‰åœ°æ‰©å±•æ—¶é—´çª—å£
    æ”¯æŒæ ¼å¼ï¼š
    - æ˜ç¡®å¹´ä»½: "2025å¹´", "2023å¹´ç¬¬ä¸€å­£åº¦"
    - æ—¥æœŸæ ¼å¼: "2025å¹´9æœˆ30æ—¥"
    - å¤šå¹´ä»½æ¯”è¾ƒ: "2024å¹´ç›¸æ¯”2023å¹´" â†’ [2023, 2024]
    
    Args:
        question: ç”¨æˆ·é—®é¢˜
        expand_window: æ˜¯å¦æ‰©å±•æ—¶é—´çª—å£ï¼ˆåœ¨å¹´ä»½èŒƒå›´å‰åå„åŠ 1å¹´ï¼‰
    
    Returns:
        List[int]: æå–åˆ°çš„å¹´ä»½åˆ—è¡¨ï¼ˆå»é‡å¹¶æ’åºï¼‰
        
    ç¤ºä¾‹ï¼š
        é—®"2024å¹´xxx" + expand_window=True â†’ [2023, 2024, 2025]
        é—®"2024å¹´ç›¸æ¯”2023å¹´" + expand_window=True â†’ [2022, 2023, 2024, 2025] ï¼ˆèŒƒå›´æ‰©å±•è€Œéé€ä¸ªæ‰©å±•ï¼‰
        é—®"2024å¹´xxx" + expand_window=False â†’ [2024]
    """
    # æ­£åˆ™åŒ¹é… 20XXå¹´ æ ¼å¼
    year_pattern = r'(20\d{2})å¹´'
    matches = re.findall(year_pattern, question)
    extracted_years = [int(y) for y in matches]
    
    if not extracted_years:
        return []
    
    if expand_window:
        # æ‰¾åˆ°å¹´ä»½èŒƒå›´çš„æœ€å°å’Œæœ€å¤§å€¼
        min_year = min(extracted_years)
        max_year = max(extracted_years)
        
        # åœ¨èŒƒå›´å‰åå„æ‰©å±•1å¹´ï¼Œç”Ÿæˆè¿ç»­å¹´ä»½åˆ—è¡¨
        years = list(range(min_year - 1, max_year + 2))  # +2 å› ä¸º range ä¸åŒ…å«ç»“æŸå€¼
        
        print(f"[DEBUG] ğŸ“… æå–å¹´ä»½: {sorted(set(extracted_years))} â†’ æ‰©å±•èŒƒå›´: [{min_year-1}, {max_year+1}]")
    else:
        years = extracted_years
    
    return sorted(list(set(years)))  # å»é‡å¹¶æ’åº


def route_reports_by_time(
    company_name: str, 
    question: str, 
    all_reports: List[Dict],
    fallback_strategy: str = "all",  # "all" æˆ– "latest"
    selected_years: List[int] = None  # å¯é€‰ï¼šå‰ç«¯æŒ‡å®šçš„å¹´ä»½åˆ—è¡¨
) -> List[Dict]:
    """
    åŸºäºå…¬å¸åå’Œå¯é€‰å¹´ä»½ä¿¡æ¯è·¯ç”±åˆ°åˆé€‚çš„æ–‡æ¡£
    
    Args:
        company_name: å…¬å¸åç§°
        question: ç”¨æˆ·é—®é¢˜ï¼ˆä¸å†ç”¨äºæå–å¹´ä»½ï¼‰
        all_reports: æ‰€æœ‰å¯ç”¨çš„æŠ¥å‘Š
        fallback_strategy: å½“æ²¡æœ‰æŒ‡å®šå¹´ä»½æ—¶çš„å›é€€ç­–ç•¥
            - "all": è¿”å›è¯¥å…¬å¸æ‰€æœ‰æ–‡æ¡£ï¼ˆé»˜è®¤ï¼‰
            - "latest": åªè¿”å›æœ€æ–°å¹´ä»½çš„æ–‡æ¡£
        selected_years: å¯é€‰ï¼Œå‰ç«¯æŒ‡å®šçš„å¹´ä»½åˆ—è¡¨ã€‚å¦‚æœæä¾›ï¼Œåªè¿”å›è¿™äº›å¹´ä»½çš„æ–‡æ¡£
    
    Returns:
        List[Dict]: åŒ¹é…çš„æŠ¥å‘Šåˆ—è¡¨
    """
    # 1. å…ˆæŒ‰å…¬å¸åè¿‡æ»¤
    company_reports = []
    for report in all_reports:
        document = report.get("document", {})
        metainfo = document.get("metainfo", {})
        if metainfo.get("company_name") == company_name:
            company_reports.append(report)
    
    if not company_reports:
        return []
    
    # 2. å¦‚æœæŒ‡å®šäº†å¹´ä»½ï¼ŒæŒ‰å¹´ä»½è¿‡æ»¤
    if selected_years and len(selected_years) > 0:
        filtered_reports = []
        for report in company_reports:
            document = report.get("document", {})
            metainfo = document.get("metainfo", {})
            
            # ä¼˜å…ˆä» year å­—æ®µè·å–ï¼Œå¦åˆ™ä» sha1_name ä¸­æå–ï¼ˆå¦‚ "J2025" â†’ 2025ï¼‰
            report_year = metainfo.get("year")
            if report_year is None:
                sha1_name = metainfo.get("sha1_name", "")
                # ä» sha1_name ä¸­æå–å¹´ä»½ï¼šåŒ¹é… J20XX æˆ– 20XX æ ¼å¼
                year_match = re.search(r'[J]?(20\d{2})', sha1_name)
                if year_match:
                    report_year = int(year_match.group(1))
            
            # æ”¯æŒå­—ç¬¦ä¸²æˆ–æ•´æ•°æ ¼å¼çš„ year
            if report_year is not None:
                try:
                    report_year = int(report_year)
                    if report_year in selected_years:
                        filtered_reports.append(report)
                except (ValueError, TypeError):
                    pass
        
        if filtered_reports:
            print(f"[INFO] ğŸ¯ å¹´ä»½è¿‡æ»¤: é€‰æ‹©å¹´ä»½ {selected_years}ï¼ŒåŒ¹é…åˆ° {len(filtered_reports)} ä¸ªæ–‡æ¡£")
            return filtered_reports
        else:
            print(f"[WARNING] âš ï¸ æŒ‡å®šå¹´ä»½ {selected_years}ï¼Œä½†æœªæ‰¾åˆ°å¯¹åº”æ–‡æ¡£ï¼Œå›é€€åˆ°å…¨éƒ¨æ–‡æ¡£")
    
    # 3. æ²¡æœ‰æŒ‡å®šå¹´ä»½æ—¶çš„å›é€€ç­–ç•¥
    if fallback_strategy == "latest":
        # è¿”å›æœ€æ–°å¹´ä»½çš„æ–‡æ¡£
        latest_year = None
        latest_reports = []
        for report in company_reports:
            document = report.get("document", {})
            metainfo = document.get("metainfo", {})
            
            # ä¼˜å…ˆä» year å­—æ®µè·å–ï¼Œå¦åˆ™ä» sha1_name ä¸­æå–
            report_year = metainfo.get("year")
            if report_year is None:
                sha1_name = metainfo.get("sha1_name", "")
                year_match = re.search(r'[J]?(20\d{2})', sha1_name)
                if year_match:
                    report_year = int(year_match.group(1))
            
            if report_year is not None:
                try:
                    report_year = int(report_year)
                    if latest_year is None or report_year > latest_year:
                        latest_year = report_year
                        latest_reports = [report]
                    elif report_year == latest_year:
                        latest_reports.append(report)
                except (ValueError, TypeError):
                    pass
        
        if latest_reports:
            print(f"[INFO] ğŸ“… æ— æŒ‡å®šå¹´ä»½ï¼Œä½¿ç”¨æœ€æ–°å¹´ä»½ {latest_year} çš„æ–‡æ¡£")
            return latest_reports
    
    # 4. é»˜è®¤è¿”å›æ‰€æœ‰è¯¥å…¬å¸çš„æ–‡æ¡£ï¼ˆä¸å†æ ¹æ®é—®é¢˜ä¸­çš„å¹´ä»½è¿‡æ»¤ï¼‰
    print(f"[INFO] ğŸ“š ä½¿ç”¨è¯¥å…¬å¸æ‰€æœ‰ {len(company_reports)} ä¸ªæ–‡æ¡£ï¼ˆæ‰€æœ‰å¹´ä»½ï¼‰")
    return company_reports

class BM25Retriever:
    def __init__(self, bm25_db_dir: Path, documents_dir: Path, subset_path: Path = None):
        self.bm25_db_dir = bm25_db_dir
        self.documents_dir = documents_dir
        self.subset_path = subset_path
        self.year_lookup = self._load_year_lookup() if subset_path else {}
    
    def _load_year_lookup(self) -> dict:
        """ä» subset.csv åŠ è½½ sha1 -> year çš„æ˜ å°„"""
        import csv
        year_lookup = {}
        try:
            with open(self.subset_path, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    sha1 = row.get('sha1', '').strip()
                    year = row.get('year', '').strip()
                    if sha1 and year:
                        try:
                            year_lookup[sha1] = int(year)
                        except ValueError:
                            pass
            print(f"[INFO] ğŸ“… BM25: ä» subset.csv åŠ è½½äº† {len(year_lookup)} ä¸ªæ–‡æ¡£çš„å¹´ä»½ä¿¡æ¯")
        except Exception as e:
            print(f"[WARNING] âš ï¸ BM25: æ— æ³•åŠ è½½ subset.csv å¹´ä»½ä¿¡æ¯: {e}")
        return year_lookup
        
    def retrieve_by_company_name(self, company_name: str, query: str, top_n: int = 3, return_parent_pages: bool = False, selected_years: List[int] = None) -> List[Dict]:
        print("BM25Retriever retrieve_by_company_name is called")
        
        # ğŸ¯ å…ˆæ”¶é›†æ‰€æœ‰æ–‡æ¡£ï¼Œç„¶åä½¿ç”¨è·¯ç”±å‡½æ•°
        all_documents = []
        for path in self.documents_dir.glob("*.json"):
            with open(path, 'r', encoding='utf-8') as f:
                doc = json.load(f)
                all_documents.append({
                    "path": path,
                    "document": doc,
                    "sha1": doc["metainfo"]["sha1_name"]
                })
        
        # è½¬æ¢ä¸º route_reports_by_time éœ€è¦çš„æ ¼å¼
        all_reports = []
        for doc_info in all_documents:
            all_reports.append({
                "document": doc_info["document"],
                "name": doc_info["sha1"]
            })
        
        # ä½¿ç”¨è·¯ç”±å‡½æ•°è¿‡æ»¤æ–‡æ¡£ï¼ˆé»˜è®¤åœ¨æ‰€æœ‰å¹´ä»½ä¸­æ£€ç´¢ï¼Œé™¤éæŒ‡å®šäº† selected_yearsï¼‰
        matching_reports = route_reports_by_time(
            company_name=company_name,
            question=query,
            all_reports=all_reports,
            fallback_strategy="all",
            selected_years=selected_years
        )
        
        # è½¬æ¢å›åŸæ¥çš„æ ¼å¼
        matching_documents = []
        matching_sha1s = {rep["name"] for rep in matching_reports}
        for doc_info in all_documents:
            if doc_info["sha1"] in matching_sha1s:
                matching_documents.append(doc_info)
        
        if not matching_documents:
            raise ValueError(f"No report found with '{company_name}' company name.")
        
        if selected_years and len(selected_years) > 0:
            print(f"[INFO] ğŸ¯ BM25å¹´ä»½è¿‡æ»¤: é€‰æ‹©å¹´ä»½ {selected_years}ï¼ŒåŒ¹é…åˆ° {len(matching_documents)} ä¸ªæ–‡æ¡£")
        elif len(matching_documents) > 1:
            print(f"[INFO] Found {len(matching_documents)} reports for '{company_name}', retrieving from all")
            
        # Retrieve from all matching documents and aggregate results
        all_retrieval_results = []
        
        for doc_info in matching_documents:
            document = doc_info["document"]
            sha1 = doc_info["sha1"]
            
            # Load corresponding BM25 index
            bm25_path = self.bm25_db_dir / f"{sha1}.pkl"
            if not bm25_path.exists():
                print(f"[WARNING] BM25 index not found for {sha1}, skipping")
                continue
                
            with open(bm25_path, 'rb') as f:
                bm25_index = pickle.load(f)
                
            # Get the document content and BM25 index
            chunks = document["content"]["chunks"]
            pages = document["content"]["pages"]
            
            # Get BM25 scores for the query
            tokenized_query = query.split()
            scores = bm25_index.get_scores(tokenized_query)
            
            actual_top_n = min(top_n, len(scores))
            top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:actual_top_n]
            
            seen_pages = set()
            
            for index in top_indices:
                score = round(float(scores[index]), 4)
                chunk = chunks[index]
                parent_page = next(page for page in pages if page["page"] == chunk["page"])
                
                if return_parent_pages:
                    if parent_page["page"] not in seen_pages:
                        seen_pages.add(parent_page["page"])
                        result = {
                            "vector_similarity": score,
                            "page": parent_page["page"],
                            "text": parent_page["text"],
                            "source_sha1": sha1  # Add source document identifier
                        }
                        all_retrieval_results.append(result)
                else:
                    result = {
                        "vector_similarity": score,
                        "page": chunk["page"],
                        "text": chunk["text"],
                        "source_sha1": sha1  # Add source document identifier
                    }
                    all_retrieval_results.append(result)
        
        # Sort by score and return top_n results across all documents
        all_retrieval_results.sort(key=lambda x: x["vector_similarity"], reverse=True)
        return all_retrieval_results[:top_n]

class HybridRetriever:
    def __init__(
        self,
        vector_db_dir: Path,
        documents_dir: Path,
        use_hyde: bool = True,
        use_multi_query: bool = True,
        subset_path: Path = None,
        parallel_workers: int = 4,
        multi_query_methods: Optional[Dict[str, bool]] = None,
        retrieval_method: str = "basic",
        max_hops: int = 4,
        neighbor_k: int = 30,
    ):
        self.vector_retriever = VectorRetriever(
            vector_db_dir,
            documents_dir,
            use_hyde=use_hyde,
            use_multi_query=use_multi_query,
            subset_path=subset_path,
            parallel_workers=parallel_workers,
            multi_query_methods=multi_query_methods,
            retrieval_method=retrieval_method,
            max_hops=max_hops,
            neighbor_k=neighbor_k,
        )
        self.reranker = LLMReranker()
        
    def retrieve_by_company_name(
        self, 
        company_name: str, 
        query: str, 
        llm_reranking_sample_size: int = 28,
        documents_batch_size: int = 2,
        top_n: int = 6,
        llm_weight: float = 0.7,
        return_parent_pages: bool = False,
        use_hyde: bool = None,
        use_multi_query: bool = None,
        progress_callback=None,
        selected_years: List[int] = None,
        multi_query_config: Optional[Dict[str, bool]] = None,
        retrieval_method: str = "basic",
        max_hops: int = 4,
        neighbor_k: int = 30
    ) -> List[Dict]:
        """
        Retrieve and rerank documents using hybrid approach.
        
        Args:
            company_name: Name of the company to search documents for
            query: Search query
            llm_reranking_sample_size: Number of initial results to retrieve from vector DB
            documents_batch_size: Number of documents to analyze in one LLM prompt
            top_n: Number of final results to return after reranking
            llm_weight: Weight given to LLM scores (0-1)
            return_parent_pages: Whether to return full pages instead of chunks
            selected_years: Optional list of years to filter documents
            
        Returns:
            List of reranked document dictionaries with scores
        """
        import time
        
        timing_info = {
            'hyde_expansion': 0.0,
            'multi_query_expansion': 0.0,
            'vector_search': 0.0,
            'llm_reranking': 0.0
        }
        
        # Get initial results from vector retriever
        vector_retrieval_result = self.vector_retriever.retrieve_by_company_name(
            company_name=company_name,
            query=query,
            top_n=llm_reranking_sample_size,
            return_parent_pages=return_parent_pages,
            use_hyde=use_hyde,
            use_multi_query=use_multi_query,
            progress_callback=progress_callback,
            selected_years=selected_years,
            multi_query_config=multi_query_config,
            retrieval_method=retrieval_method,
            max_hops=max_hops,
            neighbor_k=neighbor_k
        )
        
        # å¤„ç†è¿”å›ç»“æœï¼ˆå¯èƒ½æ˜¯dictæˆ–listï¼‰
        expansion_texts = {}
        algorithm_contribution = None
        if isinstance(vector_retrieval_result, dict) and 'timing' in vector_retrieval_result:
            timing_info.update(vector_retrieval_result['timing'])
            vector_results = vector_retrieval_result['results']
            # æå–æ‰©å±•æ–‡æœ¬ä¿¡æ¯
            if 'expansion_texts' in vector_retrieval_result:
                expansion_texts = vector_retrieval_result['expansion_texts']
            # æå–ç®—æ³•è´¡çŒ®ç»Ÿè®¡ï¼ˆä»…hybrid_expansionï¼‰
            if 'algorithm_contribution' in vector_retrieval_result:
                algorithm_contribution = vector_retrieval_result['algorithm_contribution']
        else:
            vector_results = vector_retrieval_result
        
        print(f"[DEBUG] Initial vector results count: {len(vector_results)}")

        # é‡æ’åºé˜¶æ®µï¼ˆè¿™æ˜¯æœ€è€—æ—¶çš„éƒ¨åˆ†ï¼‰
        if progress_callback:
            progress_callback("ğŸ¯ LLM é‡æ’åºä¸­ï¼ˆè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰...", 58)
        
        # Rerank results using LLM
        rerank_start = time.time()
        reranked_results = self.reranker.rerank_documents(
            query=query,
            documents=vector_results,
            documents_batch_size=documents_batch_size,
            llm_weight=llm_weight
        )
        timing_info['llm_reranking'] = time.time() - rerank_start

        print(f"[DEBUG] Reranked results count: {len(reranked_results)}")
        #print("[DEBUG] HybridRetriever retrieve_by_company_name is called")
        print(f"[DEBUG] Final top_n: {top_n}")
        
        final_results = reranked_results[:top_n]
        
        # è¿”å›ç»“æœã€æ—¶é—´ä¿¡æ¯å’Œæ‰©å±•æ–‡æœ¬
        return {
            'results': final_results,
            'initial_retrieval_results': vector_results,  # ä¿å­˜åˆå§‹å¬å›ç»“æœï¼ˆrerankingå‰ï¼‰
            'timing': timing_info,
            'expansion_texts': expansion_texts,
            'reranker_stats': self.reranker.get_stats(),
            'algorithm_contribution': algorithm_contribution  # ä¼ é€’ç®—æ³•è´¡çŒ®ç»Ÿè®¡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        }

class VectorRetriever:
    def __init__(
        self,
        vector_db_dir: Path,
        documents_dir: Path,
        use_hyde: bool = True,
        use_multi_query: bool = True,
        subset_path: Path = None,
        parallel_workers: int = 4,
        multi_query_methods: Optional[Dict[str, bool]] = None,
        retrieval_method: str = "basic",
        max_hops: int = 4,
        neighbor_k: int = 30,
    ):
        self.vector_db_dir = vector_db_dir
        self.documents_dir = documents_dir
        self.subset_path = subset_path
        self.year_lookup = self._load_year_lookup() if subset_path else {}
        self.all_dbs = self._load_dbs()
        self.qwen = BaseQwenProcessor()
        self.use_hyde = use_hyde
        self.use_multi_query = use_multi_query
        self.parallel_workers = max(1, parallel_workers)
        self.multi_query_methods = multi_query_methods or {
            'synonym': True,
            'subquestion': True,
            'variant': True
        }
        self.retrieval_method = retrieval_method
        self.max_hops = max_hops
        self.neighbor_k = neighbor_k
        #print(f"[DEBUG][VectorRetriever.__init__] use_hyde={self.use_hyde}, use_multi_query={self.use_multi_query}")
    
    def _load_year_lookup(self) -> dict:
        """ä» subset.csv åŠ è½½ sha1 -> year çš„æ˜ å°„"""
        import csv
        year_lookup = {}
        try:
            with open(self.subset_path, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    sha1 = row.get('sha1', '').strip()
                    year = row.get('year', '').strip()
                    if sha1 and year:
                        try:
                            year_lookup[sha1] = int(year)
                        except ValueError:
                            pass
            print(f"[INFO] ğŸ“… ä» subset.csv åŠ è½½äº† {len(year_lookup)} ä¸ªæ–‡æ¡£çš„å¹´ä»½ä¿¡æ¯")
        except Exception as e:
            print(f"[WARNING] âš ï¸ æ— æ³•åŠ è½½ subset.csv å¹´ä»½ä¿¡æ¯: {e}")
        return year_lookup

    # Qwen embedding ä¸éœ€è¦ set_up_llm
    
    # Qwen embedding ä¸éœ€è¦ set_up_llm

    def _load_dbs(self):
        all_dbs = []
        company_names = []  # ç”¨äºæ”¶é›†company_name
        # Get list of JSON document paths
        all_documents_paths = list(self.documents_dir.glob('*.json'))
        vector_db_files = {db_path.stem: db_path for db_path in self.vector_db_dir.glob('*.faiss')}

        for document_path in all_documents_paths:
            #print(f"[DEBUG] Loading document: {document_path.name}")
            stem = document_path.stem
            if stem not in vector_db_files:
                _log.warning(f"No matching vector DB found for document {document_path.name}")
                continue
            try:
                with open(document_path, 'r', encoding='utf-8') as f:
                    document = json.load(f)
            except Exception as e:
                _log.error(f"Error loading JSON from {document_path.name}: {e}")
                continue

            # Validate that the document meets the expected schema
            if not (isinstance(document, dict) and "metainfo" in document and "content" in document):
                _log.warning(f"Skipping {document_path.name}: does not match the expected schema.")
                continue

            # æ”¶é›†company_name
            company_name = document.get("metainfo", {}).get("company_name", None)
            if company_name:
                company_names.append(company_name)
            
            # ğŸ†• ä» year_lookup æ³¨å…¥ year ä¿¡æ¯åˆ° metainfo
            sha1_name = document.get("metainfo", {}).get("sha1_name", stem)
            if sha1_name in self.year_lookup:
                document["metainfo"]["year"] = self.year_lookup[sha1_name]

            try:
                vector_db = faiss.read_index(str(vector_db_files[stem]))
            except Exception as e:
                _log.error(f"Error reading vector DB for {document_path.name}: {e}")
                continue

            report = {
                "name": stem,
                "vector_db": vector_db,
                "document": document
            }
            all_dbs.append(report)

        # print("[DEBUG] å½“å‰å¯ç”¨çš„company_nameæœ‰:")
        # for name in company_names:
        #     print(f"  - {name}")
        # print("[DEBUG] å½“å‰å¯ç”¨çš„company_nameä»¥ä¸Š")

        return all_dbs

    @staticmethod
    def get_strings_cosine_similarity(str1, str2):
        qwen = BaseQwenProcessor()
        emb1 = qwen.get_embeddings([str1])["embeddings"][0]
        emb2 = qwen.get_embeddings([str2])["embeddings"][0]
        similarity_score = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
        similarity_score = round(similarity_score, 4)
        return similarity_score

    
   
    def _safe_flush(self):
        """å®‰å…¨åœ°åˆ·æ–°æ ‡å‡†è¾“å‡ºï¼Œå¿½ç•¥ BrokenPipeError"""
        import sys
        try:
            sys.stdout.flush()
        except (BrokenPipeError, OSError):
            pass  # å¿½ç•¥ BrokenPipeErrorï¼Œåœ¨ Streamlit ç¯å¢ƒä¸­å¯èƒ½å‘ç”Ÿ
    
    def _safe_print(self, *args, **kwargs):
        """å®‰å…¨åœ°æ‰“å°ï¼Œå¿½ç•¥ BrokenPipeError"""
        try:
            print(*args, **kwargs)
            self._safe_flush()
        except (BrokenPipeError, OSError):
            pass  # å¿½ç•¥ BrokenPipeErrorï¼Œåœ¨ Streamlit ç¯å¢ƒä¸­å¯èƒ½å‘ç”Ÿ
    
    def _get_vector_by_id(self, vector_db, doc_id):
        """ä» FAISS ç´¢å¼•ä¸­è·å–æŒ‡å®š ID çš„å‘é‡"""
        try:
            return vector_db.reconstruct(int(doc_id))
        except Exception as e:
            self._safe_print(f"[WARNING] Failed to reconstruct vector for ID {doc_id}: {e}")
            return None
    
    def _normalize_vector(self, vec):
        """Normalize a vector to unit length."""
        norm = np.linalg.norm(vec)
        if norm == 0:
            return vec
        return vec / norm
    
    def _ssg_search(self, vector_db, anchor_id, anchor_vec, max_hops=4, neighbor_k=30):
        """
        SSG Traversal Algorithm implementation.
        Returns a dictionary with results and detailed traversal information.
        
        Args:
            vector_db: FAISS index
            anchor_id: Starting chunk index
            anchor_vec: Starting chunk embedding vector
            max_hops: Maximum number of hops to traverse
            neighbor_k: Number of neighbors to consider at each hop
        
        Returns:
            Dictionary with "results" (list of (score, idx) tuples) and "traversal_details"
        """
        visited = set([int(anchor_id)])
        results = []  # List of (score, index)
        
        # è¯¦ç»†è¿½è¸ªä¿¡æ¯
        traversal_details = {
            "anchor": {"idx": int(anchor_id), "score": None},
            "hops": [],
            "path": [int(anchor_id)],
            "total_hops": 0,
            "total_discovered": 1
        }
        
        current_idx = int(anchor_id)
        current_vec = anchor_vec
        previous_similarity = 1.0  # Anchor similarity with itself
        
        # æ·»åŠ anchoråˆ°ç»“æœ
        results.append((1.0, current_idx))
        
        for hop_num in range(1, max_hops + 1):
            # ä½¿ç”¨å½“å‰chunkçš„å‘é‡æœç´¢é‚»å±…
            current_vec_reshaped = current_vec.reshape(1, -1)
            distances, indices = vector_db.search(x=current_vec_reshaped, k=neighbor_k + 1)  # +1 to exclude self
            
            candidates = []
            for d, idx in zip(distances[0], indices[0]):
                idx = int(idx)
                if idx == -1 or idx in visited:
                    continue
                
                # è·å–å€™é€‰chunkçš„å‘é‡
                candidate_vec = self._get_vector_by_id(vector_db, idx)
                if candidate_vec is None:
                    continue
                
                # è®¡ç®—chunk-to-chunkç›¸ä¼¼åº¦
                chunk_similarity = float(np.dot(current_vec.flatten(), candidate_vec.flatten()))
                
                candidates.append({
                    "idx": idx,
                    "score": chunk_similarity,
                    "selected": False
                })
            
            if not candidates:
                break
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œé€‰æ‹©æœ€ä½³å€™é€‰
            candidates.sort(key=lambda x: x["score"], reverse=True)
            best_candidate = candidates[0]
            best_idx = best_candidate["idx"]
            best_similarity = best_candidate["score"]
            
            # æ—©åœæ£€æŸ¥ï¼šå¦‚æœç›¸ä¼¼åº¦ä¸å†æå‡ï¼Œåœæ­¢éå†
            if best_similarity <= previous_similarity:
                break
            
            # æ ‡è®°é€‰ä¸­çš„å€™é€‰
            best_candidate["selected"] = True
            
            # è®°å½•è¿™ä¸€è·³çš„è¯¦ç»†ä¿¡æ¯
            hop_info = {
                "hop_number": hop_num,
                "current_chunk": current_idx,
                "candidates": candidates[:10],  # åªè®°å½•å‰10ä¸ªå€™é€‰
                "selected_idx": best_idx,
                "selected_score": best_similarity
            }
            traversal_details["hops"].append(hop_info)
            
            # è·³è½¬åˆ°æ–°chunk
            visited.add(best_idx)
            traversal_details["path"].append(best_idx)
            current_idx = best_idx
            current_vec = self._get_vector_by_id(vector_db, best_idx)
            if current_vec is None:
                break
            
            # æ·»åŠ æ–°chunkåˆ°ç»“æœï¼ˆä½¿ç”¨chunk-to-chunkç›¸ä¼¼åº¦ä½œä¸ºåˆå§‹åˆ†æ•°ï¼‰
            results.append((best_similarity, best_idx))
            previous_similarity = best_similarity
            traversal_details["total_discovered"] += 1
        
        traversal_details["total_hops"] = len(traversal_details["hops"])
        
        return {
            "results": results,
            "traversal_details": traversal_details
        }
    
    def _triangulation_search(self, vector_db, query_vec, anchor_id, anchor_vec, max_hops=4, neighbor_k=30):
        """
        Triangulation FullDim Algorithm implementation.
        Uses geometric triangulation in full embedding space to select next chunk.
        
        Args:
            vector_db: FAISS index
            query_vec: Query embedding vector
            anchor_id: Starting chunk index
            anchor_vec: Starting chunk embedding vector
            max_hops: Maximum number of hops to traverse
            neighbor_k: Number of neighbors to consider at each hop
        
        Returns:
            Dictionary with "results" (list of (score, idx) tuples) and "traversal_details"
        """
        visited = set([int(anchor_id)])
        results = []  # List of (centroid_score, index)
        
        query_vec_flat = query_vec.flatten()
        
        # è¯¦ç»†è¿½è¸ªä¿¡æ¯
        traversal_details = {
            "anchor": {"idx": int(anchor_id), "score": None},
            "hops": [],
            "path": [int(anchor_id)],
            "total_hops": 0
        }
        
        # è®¡ç®—anchorçš„query-to-chunkç›¸ä¼¼åº¦
        anchor_query_sim = float(np.dot(query_vec_flat, anchor_vec.flatten()))
        traversal_details["anchor"]["score"] = anchor_query_sim
        results.append((anchor_query_sim, int(anchor_id)))
        
        current_idx = int(anchor_id)
        current_vec = anchor_vec
        
        for hop_num in range(1, max_hops + 1):
            # ä½¿ç”¨å½“å‰chunkçš„å‘é‡æœç´¢é‚»å±…
            current_vec_reshaped = current_vec.reshape(1, -1)
            distances, indices = vector_db.search(x=current_vec_reshaped, k=neighbor_k + 1)
            
            candidates = []
            for d, idx in zip(distances[0], indices[0]):
                idx = int(idx)
                if idx == -1 or idx in visited:
                    continue
                
                candidate_vec = self._get_vector_by_id(vector_db, idx)
                if candidate_vec is None:
                    continue
                
                candidate_vec_flat = candidate_vec.flatten()
                current_vec_flat = current_vec.flatten()
                
                # è®¡ç®—query-to-candidateç›¸ä¼¼åº¦
                query_to_candidate = float(np.dot(query_vec_flat, candidate_vec_flat))
                
                # æ„å»ºå‡ ä½•ä¸‰è§’å½¢ï¼šquery, current_chunk, candidate
                # è®¡ç®—ä¸‰è§’å½¢è´¨å¿ƒ
                centroid = (query_vec_flat + current_vec_flat + candidate_vec_flat) / 3.0
                
                # è®¡ç®—è´¨å¿ƒåˆ°æŸ¥è¯¢çš„è·ç¦»ï¼ˆä½¿ç”¨æ¬§æ°è·ç¦»ï¼‰
                centroid_distance = float(np.linalg.norm(centroid - query_vec_flat))
                
                # è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆè·ç¦»è¶Šå°ï¼Œåˆ†æ•°è¶Šé«˜ï¼‰
                # ä½¿ç”¨è´Ÿè·ç¦»æˆ–è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°
                centroid_score = 1.0 / (1.0 + centroid_distance)  # ç®€å•çš„è½¬æ¢
                
                candidates.append({
                    "idx": idx,
                    "query_to_candidate": query_to_candidate,
                    "centroid_score": centroid_score,
                    "centroid_distance": centroid_distance,
                    "selected": False
                })
            
            if not candidates:
                break
            
            # æŒ‰è´¨å¿ƒåˆ†æ•°æ’åºï¼ˆåˆ†æ•°è¶Šé«˜è¶Šå¥½ï¼Œå³è·ç¦»è¶Šå°è¶Šå¥½ï¼‰
            candidates.sort(key=lambda x: x["centroid_score"], reverse=True)
            best_candidate = candidates[0]
            best_idx = best_candidate["idx"]
            best_centroid_score = best_candidate["centroid_score"]
            
            # æ ‡è®°é€‰ä¸­çš„å€™é€‰
            best_candidate["selected"] = True
            
            # è®°å½•è¿™ä¸€è·³çš„è¯¦ç»†ä¿¡æ¯
            hop_info = {
                "hop_number": hop_num,
                "current_chunk": current_idx,
                "candidates": candidates[:10],  # åªè®°å½•å‰10ä¸ªå€™é€‰
                "selected_idx": best_idx,
                "centroid_score": best_centroid_score,
                "selection_reason": "è´¨å¿ƒè·ç¦»æœ€ä¼˜"
            }
            traversal_details["hops"].append(hop_info)
            
            # è·³è½¬åˆ°æ–°chunk
            visited.add(best_idx)
            traversal_details["path"].append(best_idx)
            current_idx = best_idx
            current_vec = self._get_vector_by_id(vector_db, best_idx)
            if current_vec is None:
                break
            
            # æ·»åŠ æ–°chunkåˆ°ç»“æœï¼ˆä½¿ç”¨è´¨å¿ƒåˆ†æ•°ï¼‰
            results.append((best_centroid_score, best_idx))
        
        traversal_details["total_hops"] = len(traversal_details["hops"])
        
        return {
            "results": results,
            "traversal_details": traversal_details
        }
    
    def retrieve_by_company_name(self, company_name: str, query: str, llm_reranking_sample_size: int = None, top_n: int = 3, return_parent_pages: bool = False, use_hyde: bool = None, use_multi_query: bool = None, progress_callback=None, selected_years: List[int] = None, multi_query_config: Optional[Dict[str, bool]] = None, retrieval_method: str = "basic", max_hops: int = 2, neighbor_k: int = 10) -> List[Tuple[str, float]]:
        import sys
        import time
        
        # åˆå§‹åŒ–æ—¶é—´ç»Ÿè®¡å’Œæ‰©å±•æ–‡æœ¬ä¿¡æ¯
        timing_info = {
            'hyde_expansion': 0.0,
            'multi_query_expansion': 0.0,
            'embedding_generation': 0.0,
            'vector_search': 0.0
        }
        
        # ä¿å­˜æ‰©å±•ç”Ÿæˆçš„æ–‡æœ¬
        expansion_texts = {
            'hyde_text': None,
            'multi_query_texts': [],
            'glossary_context': None,
            'multi_query_methods': {}
        }
        
        self._safe_print("[DEBUG] VectorRetriever retrieve_by_company_name is called")

        # ğŸ¯ ä½¿ç”¨è·¯ç”±å‡½æ•°å®šä½æ–‡æ¡£ï¼ˆé»˜è®¤åœ¨æ‰€æœ‰å¹´ä»½ä¸­æ£€ç´¢ï¼Œé™¤éæŒ‡å®šäº† selected_yearsï¼‰
        if progress_callback:
            progress_callback("ğŸ“š å®šä½ç›¸å…³æ–‡æ¡£ä¸­...", 28)
        
        matching_reports = route_reports_by_time(
            company_name=company_name,
            question=query,
            all_reports=self.all_dbs,
            fallback_strategy="all",  # æ— æŒ‡å®šå¹´ä»½æ—¶ä½¿ç”¨æ‰€æœ‰æ–‡æ¡£
            selected_years=selected_years  # å‰ç«¯æŒ‡å®šçš„å¹´ä»½åˆ—è¡¨
        )
        
        if not matching_reports:
            _log.error(f"No report found with '{company_name}' company name.")
            raise ValueError(f"No report found with '{company_name}' company name.")
        
        if len(matching_reports) > 1:
            self._safe_print(f"[INFO] Found {len(matching_reports)} reports for '{company_name}', retrieving from all")
            for rep in matching_reports:
                doc = rep.get("document", {})
                metainfo = doc.get("metainfo", {})
                year = metainfo.get("year", "unknown")
                self._safe_print(f"  - Report: {rep['name']} (Year: {year})")
        
        # Priority parameters
        use_hyde = self.use_hyde if use_hyde is None else use_hyde
        use_multi_query = self.use_multi_query if use_multi_query is None else use_multi_query
        multi_query_config = multi_query_config or self.multi_query_methods or {}
        expansion_texts['multi_query_methods'] = multi_query_config
        self._safe_print(f"[DEBUG] multi_query_config = {multi_query_config}")
        if use_multi_query and not any(multi_query_config.values()):
            self._safe_print("[INFO] Multi-Query enabled but no methods selected; skipping expansion.")
            use_multi_query = False
        # å¤„ç†æ£€ç´¢æ–¹æ³•å‚æ•°ï¼šä¼˜å…ˆä½¿ç”¨ä¼ å…¥å‚æ•°ï¼ˆå¦‚æœæ˜ç¡®ä¼ å…¥éé»˜è®¤å€¼ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨å®ä¾‹å˜é‡
        # å…³é”®é€»è¾‘ï¼šå¦‚æœä¼ å…¥çš„å‚æ•°æ˜¯é»˜è®¤å€¼"basic"ï¼Œä½†å®ä¾‹å˜é‡ä¸­æœ‰éé»˜è®¤å€¼ï¼Œåˆ™ä½¿ç”¨å®ä¾‹å˜é‡
        # è¿™æ ·å¯ä»¥æ”¯æŒåŠ¨æ€æ›´æ–°ï¼ˆä¾‹å¦‚ä»UIæ›´æ–°processor.retrieval_methodï¼‰
        self._safe_print(f"[DEBUG] å‚æ•°å¤„ç†å‰: ä¼ å…¥retrieval_method={retrieval_method}, å®ä¾‹self.retrieval_method={getattr(self, 'retrieval_method', 'N/A')}")
        
        if retrieval_method == "basic":
            # å¦‚æœä¼ å…¥çš„æ˜¯é»˜è®¤å€¼"basic"ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å®ä¾‹å˜é‡ï¼ˆå¯èƒ½æ˜¯ä»UIæ›´æ–°çš„ï¼‰
            if hasattr(self, 'retrieval_method') and self.retrieval_method != "basic":
                retrieval_method = self.retrieval_method
                self._safe_print(f"[DEBUG] âœ… ä½¿ç”¨å®ä¾‹å˜é‡ä¸­çš„retrieval_method: {retrieval_method} (è¦†ç›–é»˜è®¤å€¼'basic')")
            else:
                self._safe_print(f"[DEBUG] ä½¿ç”¨é»˜è®¤å€¼'basic'")
        else:
            # å¦‚æœä¼ å…¥çš„ä¸æ˜¯"basic"ï¼Œç›´æ¥ä½¿ç”¨ä¼ å…¥çš„å€¼ï¼ˆè¿™æ˜¯æ­£ç¡®çš„è¡Œä¸ºï¼‰
            self._safe_print(f"[DEBUG] âœ… ä½¿ç”¨ä¼ å…¥çš„retrieval_methodå‚æ•°: {retrieval_method}")
        
        # å¯¹äºmax_hopså’Œneighbor_kï¼Œå¦‚æœä¼ å…¥çš„æ˜¯é»˜è®¤å€¼ï¼Œåˆ™ä½¿ç”¨å®ä¾‹å˜é‡
        if max_hops == 4 and hasattr(self, 'max_hops') and self.max_hops != 4:
            max_hops = self.max_hops
            self._safe_print(f"[DEBUG] ä½¿ç”¨å®ä¾‹å˜é‡ä¸­çš„max_hops: {max_hops}")
        if neighbor_k == 30 and hasattr(self, 'neighbor_k') and self.neighbor_k != 30:
            neighbor_k = self.neighbor_k
            self._safe_print(f"[DEBUG] ä½¿ç”¨å®ä¾‹å˜é‡ä¸­çš„neighbor_k: {neighbor_k}")
        self._safe_print(f"[DEBUG][retrieve_by_company_name] use_hyde={use_hyde}, use_multi_query={use_multi_query}, retrieval_method={retrieval_method}, max_hops={max_hops}, neighbor_k={neighbor_k}")
        
        qwen = BaseQwenProcessor()
        # æ§åˆ¶multi_queryå’Œhydeæ‰©å……
        queries = [query]

        if use_hyde:
            if progress_callback:
                progress_callback("ğŸ”® HYDE æŸ¥è¯¢æ‰©å±•ä¸­...", 32)
            self._safe_print(f"[DEBUG] å¼€å§‹ HYDE æ‰©å±•...")
            hyde_start = time.time()
            try:
                self._safe_print(f"[DEBUG] è°ƒç”¨ Qwen API ç”Ÿæˆå‡è®¾ç­”æ¡ˆ...")
                fake_answer = qwen.send_message(
                    model="qwen-turbo",
                    system_content=(
                        "You are a financial report analyst. Your task is to generate a hypothetical markdown table "
                        "that could plausibly appear in a company's annual report or financial statement to answer the given query. "
                        "\n\n"
                        "**Requirements:**\n"
                        "1. Generate a markdown-format table (using | and - for formatting)\n"
                        "2. The table should be relevant to the question and contain typical fields/columns that would appear in such a table\n"
                        "3. Include appropriate table headers (such as: ç±»å‹, é¡¹ç›®, é‡‘é¢, å•ä½, å¤‡æ³¨, å¹´ä»½, å­£åº¦, æ¯”ä¾‹, etc.)\n"
                        "4. Add a unit specification if applicable (e.g., 'å•ä½ï¼šä¸‡å…ƒ' or 'å•ä½ï¼šå…ƒ')\n"
                        "5. Include sample data rows that demonstrate the table structure\n"
                        "6. The table structure should match what would typically appear in Chinese financial reports\n"
                        "\n"
                        "**Table Format Example:**\n"
                        "```\n"
                        "å•ä½ï¼šä¸‡å…ƒ\n\n"
                        "| ç±»å‹ | é¡¹ç›® | é‡‘é¢ | å¤‡æ³¨ |\n"
                        "|------|------|------|------|\n"
                        "| ...  | ...  | ...  | ...  |\n"
                        "```\n"
                        "\n"
                        "**Important:**\n"
                        "- Focus on creating a realistic table structure, not accurate data\n"
                        "- The table should help retrieve similar tables from financial reports\n"
                        "- Use Chinese column names appropriate for financial statements\n"
                        "- Include calculation formulas or notes if relevant (e.g., 'â‘  â‘¡ â‘¢ = +' or 'â‘¥ â‘  â‘£ â‘¤ = - -')"
                    ),
                    human_content=f"Generate a markdown-format table that could appear in a company's financial report to answer this question: {query}\n\n"
                                 f"The table should include:\n"
                                 f"- Appropriate unit specification (if applicable)\n"
                                 f"- Relevant column headers based on the question\n"
                                 f"- Sample data rows showing the table structure\n"
                                 f"- Any relevant notes or calculation formulas",
                    is_structured=False
                )
                if isinstance(fake_answer, list):
                    fake_answer_str = ''.join(fake_answer)
                else:
                    fake_answer_str = str(fake_answer)
                queries.append(fake_answer_str)
                expansion_texts['hyde_text'] = fake_answer_str  # ä¿å­˜HYDEç”Ÿæˆçš„æ–‡æœ¬
                self._safe_print(f"[DEBUG] HYDE æ‰©å±•æˆåŠŸï¼Œç”Ÿæˆå‡è®¾ç­”æ¡ˆé•¿åº¦: {len(fake_answer_str)}")
            except Exception as e:
                self._safe_print(f"[ERROR] HYDE expansion failed: {e}")
            timing_info['hyde_expansion'] = time.time() - hyde_start

        if use_multi_query:
            if progress_callback:
                progress_callback("ğŸ”„ Multi-Query æŸ¥è¯¢æ‰©å±•ä¸­...", 38)
            self._safe_print(f"[DEBUG] å¼€å§‹ Multi-Query æ‰©å±•...")
            multi_query_start = time.time()
            matched_concepts = find_financial_concepts(query, limit=5)
            concept_terms = [concept["term"] for concept in matched_concepts]
            concept_context_text = format_concepts_for_prompt(matched_concepts)
            glossary_instruction = (
                "Financial glossary context.\n"
                "Foræ¯ä¸ªå‘½ä¸­çš„æœ¯è¯­ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼é€æ¡è¿½åŠ è§£é‡Šï¼š\n"
                "1) Termå + ä¸»è¦åˆ«å/è¿‘ä¹‰è¯\n"
                "2) å®šä¹‰ï¼ˆè‡³å°‘ä¸€å¥ï¼‰\n"
                "3) è®¡ç®—æ–¹æ³•/å…¸å‹å•ä½/æ•°æ®æ¥æºï¼ˆè‹¥é€‚ç”¨ï¼‰\n"
                "æ ¼å¼ç¤ºä¾‹ï¼š\n"
                "\"æ¯›åˆ©ç‡\n"
                "- åˆ«åï¼šç»¼åˆæ¯›åˆ©ç‡\n"
                "- å®šä¹‰ï¼šä½“ç°äº§å“ç›ˆåˆ©ç©ºé—´çš„æ¯”ä¾‹â€¦â€¦\n"
                "- è®¡ç®—æ–¹å¼ï¼šæ¯›åˆ©ç‡ = (è¥ä¸šæ”¶å…¥ - è¥ä¸šæˆæœ¬) Ã· è¥ä¸šæ”¶å…¥\"\n"
                "åœ¨ç”Ÿæˆæ–°çš„æŸ¥è¯¢æ—¶ï¼Œå°†ä¸Šè¿°è§£é‡Šé™„åŠ åœ¨åŸé—®é¢˜åæ–¹çš„ç‹¬ç«‹æ®µè½ä¸­ï¼Œè€Œä¸æ˜¯å†™åœ¨æ‹¬å·é‡Œã€‚\n"
                f"{concept_context_text}"
            )
            expansion_texts['glossary_context'] = concept_context_text
            expansion_texts['multi_query_methods'] = multi_query_config
            method_definitions = [
                (
                    1,
                    'synonym',
                    "ä½ çš„ä»»åŠ¡æ˜¯ä¸ºé—®é¢˜ä¸­çš„è´¢åŠ¡ä¸“ä¸šåè¯è¡¥å……è¯¦ç»†è§£é‡Šã€‚"
                    "ä¸Šé¢å·²æä¾›äº†è´¢åŠ¡æœ¯è¯­è¯æ±‡è¡¨(Financial glossary)ï¼ŒåŒ…å«æ¯ä¸ªæœ¯è¯­çš„åˆ«åã€å®šä¹‰å’Œè®¡ç®—æ–¹å¼ã€‚"
                    "ä»»åŠ¡è¦æ±‚ï¼šè¯†åˆ«é—®é¢˜ä¸­åŒ…å«çš„è´¢åŠ¡æœ¯è¯­ï¼Œå‚è€ƒ glossary ä¸­çš„ä¿¡æ¯ï¼Œåœ¨åŸé—®é¢˜ä¹‹åå•ç‹¬åˆ—å‡ºæ¯ä¸ªæœ¯è¯­çš„å®šä¹‰ã€è¿‘ä¹‰è¯ã€è®¡ç®—æ–¹æ³•ã€‚"
                    "æ ¼å¼ï¼š<åŸé—®é¢˜ åè¯è§£é‡Šï¼šæœ¯è¯­åç§° å®šä¹‰...è¿‘ä¹‰è¯...è®¡ç®—æ–¹æ³•...>"
                    "ç¤ºä¾‹ï¼šé‡‘ç›˜ç§‘æŠ€2024å¹´çš„æ¯›åˆ©ç‡æ˜¯å¤šå°‘ -> "
                    "<é‡‘ç›˜ç§‘æŠ€2024å¹´çš„æ¯›åˆ©ç‡æ˜¯å¤šå°‘ åè¯è§£é‡Šï¼šæ¯›åˆ©ç‡ å®šä¹‰ï¼šæ¯›åˆ©ä¸è¥ä¸šæ”¶å…¥ä¹‹æ¯”ï¼Œåæ˜ äº§å“æˆ–ä¸šåŠ¡çš„ç›ˆåˆ©ç©ºé—´ è¿‘ä¹‰è¯ï¼šç»¼åˆæ¯›åˆ©ç‡ è®¡ç®—æ–¹æ³•ï¼šæ¯›åˆ©ç‡=(è¥ä¸šæ”¶å…¥-è¥ä¸šæˆæœ¬)/è¥ä¸šæ”¶å…¥>"
                    "ä¼˜å…ˆä½¿ç”¨ glossary ä¸­æä¾›çš„å®šä¹‰ã€è¿‘ä¹‰è¯å’Œè®¡ç®—æ–¹å¼ã€‚å¦‚æœé—®é¢˜æ¶‰åŠè´¢åŠ¡æœ¯è¯­ä½† glossary ä¸­æ²¡æœ‰ï¼Œå¯ä»¥ç”¨ä½ è‡ªå·±çš„çŸ¥è¯†è¡¥å……ã€‚"
                    "åªæœ‰åœ¨é—®é¢˜å®Œå…¨ä¸æ¶‰åŠä»»ä½•è´¢åŠ¡æœ¯è¯­æ—¶ï¼Œæ‰è¿”å› <SKIP>ã€‚å¯ç”Ÿæˆ1-2ä¸ªå¸¦åè¯è§£é‡Šçš„æŸ¥è¯¢ï¼Œæ¯ä¸ªç”¨å°–æ‹¬å·åŒ…è£¹ã€‚"
                ),
                (
                    2,
                    'subquestion',
                    "æ ¹æ®è´¢åŠ¡æŒ‡æ ‡å°†é—®é¢˜æ‹†åˆ†ä¸º0-Nä¸ªç²’åº¦æ›´ç»†çš„å­é—®é¢˜ã€‚"
                    "æ¯ä¸ªå­é—®é¢˜ä¸“æ³¨äºå•ä¸€æŒ‡æ ‡/æ—¶é—´æ®µ/ä¸šåŠ¡æ¿å—ï¼Œå¹¶ç»“åˆ glossary é‡Œçš„æœ¯è¯­æˆ–å•ä½ã€‚"
                    "è‹¥æ²¡æœ‰åˆé€‚çš„æ‹†åˆ†åˆ™è¿”å› <SKIP>ï¼›å¦åˆ™æ¯ä¸ªå­é—®é¢˜å•ç‹¬ç”¨ <> åŒ…è£¹ã€‚"
                ),
                (
                    3,
                    'variant',
                    "ä»…å½“åŸé—®é¢˜åå¼€æ”¾æˆ–ä¿¡æ¯ä¸è¶³æ—¶ï¼Œç”Ÿæˆæƒ…æ™¯åŒ–/å˜ä½“æé—®ï¼Œæ¢ç´¢ä¸åŒè§’åº¦ï¼ˆå¦‚ç›ˆåˆ©è´¨é‡ã€ç°é‡‘å®‰å…¨å«ã€æµ·å¤–æ‰©å¼ ã€è¡¥è´´æŒç»­æ€§ç­‰ï¼‰ã€‚"
                    "è‹¥é—®é¢˜æœ¬å°±æ˜ç¡®ï¼Œåˆ™è¾“å‡º <SKIP>ï¼›è‹¥éœ€è¦æ”¹å†™ï¼Œå¯ç”Ÿæˆ1-2ä¸ªæŸ¥è¯¢ï¼Œæ¯ä¸ªç”¨ <> åŒ…è£¹ï¼Œå¹¶ä¿æŒä¸»ä½“ä¸ºé‡‘ç›˜ç§‘æŠ€ã€‚"
                )
            ]
            import re
            for method_id, method_key, prompt in method_definitions:
                if not multi_query_config.get(method_key, False):
                    continue
                self._safe_print(f"[DEBUG] Multi-Query æ–¹æ³• {method_id}...")
                try:
                    self._safe_print(f"[DEBUG] è°ƒç”¨ Qwen API æ‰©å±•æŸ¥è¯¢...")
                    response = qwen.send_message(
                        model="qwen-turbo",
                        system_content=(
                            "You are assisting in an Enterprise RAG Challenge focused on annual reports. "
                            "Always maintain financial rigor and keep the company name unchanged."
                        ),
                        human_content=(
                            f"{prompt}\n\n"
                            f"{glossary_instruction}\n\n"
                            f"Original question: {query}"
                        ),
                        is_structured=False
                    )
                    extracted_queries = re.findall(r"<(.*?)>", response, flags=re.DOTALL)
                    self._safe_print(f"[DEBUG] ========== Multi-Query æ–¹æ³• {method_id} ({method_key}) ==========")
                    self._safe_print(f"[DEBUG] åŸå§‹å“åº” (å‰500å­—ç¬¦): {response[:500]}...")
                    self._safe_print(f"[DEBUG] æå–çš„æŸ¥è¯¢æ•°é‡: {len(extracted_queries)}")
                    self._safe_print(f"[DEBUG] æå–çš„æŸ¥è¯¢åˆ—è¡¨: {extracted_queries}")
                    
                    added_count = 0
                    skipped_count = 0
                    for q in extracted_queries:
                        q_stripped = q.strip()
                        is_skip = not q_stripped or q_stripped.upper() == "SKIP"
                        self._safe_print(f"[DEBUG] å¤„ç†æŸ¥è¯¢: '{q_stripped[:80]}...' (é•¿åº¦={len(q_stripped)}, SKIP={is_skip})")
                        if is_skip:
                            skipped_count += 1
                            continue
                        queries.append(q_stripped)
                        expansion_texts['multi_query_texts'].append({
                            'method_id': method_id,
                            'query': q_stripped,
                            'concepts': concept_terms
                        })
                        added_count += 1
                    
                    self._safe_print(f"[DEBUG] Multi-Query æ–¹æ³• {method_id} ç»Ÿè®¡:")
                    self._safe_print(f"[DEBUG]   - æå–çš„æŸ¥è¯¢æ€»æ•°: {len(extracted_queries)}")
                    self._safe_print(f"[DEBUG]   - è·³è¿‡(SKIP): {skipped_count}")
                    self._safe_print(f"[DEBUG]   - å®é™…æ·»åŠ : {added_count}")
                    self._safe_print(f"[DEBUG] ========================================================")
                except Exception as e:
                    self._safe_print(f"Expansion method {method_id} failed: {e}")
            timing_info['multi_query_expansion'] = time.time() - multi_query_start
        
        # å»é‡å¹¶æ¸…æ´—æŸ¥è¯¢ï¼Œé¿å…é‡å¤ embedding è®¡ç®—
        self._safe_print(f"[DEBUG] ========== æŸ¥è¯¢å»é‡å¤„ç† ==========")
        self._safe_print(f"[DEBUG] å»é‡å‰çš„æŸ¥è¯¢æ€»æ•°: {len(queries)}")
        self._safe_print(f"[DEBUG] å»é‡å‰çš„æŸ¥è¯¢åˆ—è¡¨:")
        for idx, q in enumerate(queries, 1):
            self._safe_print(f"[DEBUG]   {idx}. {q[:100]}...")
        
        deduped_queries = []
        seen_queries = set()
        duplicate_count = 0
        for q in queries:
            normalized_q = q.strip()
            if not normalized_q:
                continue
            if normalized_q in seen_queries:
                duplicate_count += 1
                self._safe_print(f"[DEBUG]   å‘ç°é‡å¤æŸ¥è¯¢: '{normalized_q[:80]}...'")
                continue
            deduped_queries.append(normalized_q)
            seen_queries.add(normalized_q)
        queries = deduped_queries

        self._safe_print(f"[DEBUG] å»é‡åçš„æŸ¥è¯¢æ€»æ•°: {len(queries)}")
        self._safe_print(f"[DEBUG] é‡å¤æŸ¥è¯¢æ•°é‡: {duplicate_count}")
        self._safe_print(f"[DEBUG] å»é‡åçš„æŸ¥è¯¢åˆ—è¡¨:")
        for idx, q in enumerate(queries, 1):
            self._safe_print(f"[DEBUG]   {idx}. {q[:100]}...")
        self._safe_print(f"[DEBUG] ==================================")

        inner_factor = 1.0

        # é¢„å…ˆç”Ÿæˆ embeddingsï¼Œé¿å…åœ¨ä¸åŒæ–‡æ¡£ä¹‹é—´é‡å¤è¯·æ±‚
        query_embeddings = {}
        embedding_start = time.time()
        for q in queries:
            try:
                emb_result = self.qwen.get_embeddings([q])
                if (
                    not emb_result
                    or not isinstance(emb_result, list)
                    or not emb_result[0]
                    or 'embedding' not in emb_result[0]
                ):
                    self._safe_print(f"[ERROR] embedding result is empty or invalid for query: {q[:80]}")
                    continue
                embedding = emb_result[0]['embedding']
                query_embeddings[q] = np.array(embedding, dtype=np.float32).reshape(1, -1)
            except Exception as e:
                self._safe_print(f"[ERROR] Failed to get embedding for query snippet '{q[:50]}': {e}")
        timing_info['embedding_generation'] = time.time() - embedding_start

        if not query_embeddings:
            raise ValueError("Failed to generate embeddings for all queries.")
        
        # å‘½ä¸­ç»“æœå­˜å‚¨ï¼ˆç”¨å­—å…¸èšåˆï¼‰
        # key = (sha1, page_id or chunk_id), value = dict with similarities, count, text, retrieval_sources
        aggregated_results = {}
        aggregation_lock = Lock()

        # ğŸ¯ æ–°æ£€ç´¢ç­–ç•¥ï¼šæ¯ä¸ªæ–‡æ¡£å‡å¬å› top_n ä¸ªchunksï¼Œç„¶åç»Ÿä¸€æŒ‰å‘é‡ç›¸ä¼¼åº¦æ’åº
        # æ”¶é›†æ‰€æœ‰æ–‡æ¡£çš„æ£€ç´¢ç»“æœï¼ˆæ€»å…± num_reports * top_n ä¸ªç»“æœï¼‰ï¼Œ
        # ç»Ÿä¸€æŒ‰å‘é‡ç›¸ä¼¼åº¦ï¼ˆåŠ æƒåçš„distanceï¼‰æ’åºï¼Œæˆªæ–­å¼é€‰å–å‰ top_n ä¸ªç»“æœ
        num_reports = len(matching_reports)
        
        self._safe_print(f"[INFO] ğŸ“Š æ£€ç´¢ç­–ç•¥: {num_reports}ä¸ªæ–‡æ¡£, æ¯ä¸ªæ–‡æ¡£æ£€ç´¢ {top_n} ä¸ªchunks (æ€»è®¡æœ€å¤š {num_reports * top_n} ä¸ªç»“æœ)")

        # å‘é‡æ£€ç´¢é˜¶æ®µ
        if progress_callback:
            progress_callback("ğŸ” å‘é‡æ£€ç´¢ä¸­...", 45)

        def process_query_for_document(report, query_text, embedding_array):
            """
            ä¸ºå•ä¸ªæŸ¥è¯¢-æ–‡æ¡£å¯¹æ‰§è¡Œæ£€ç´¢
            è¿”å›æ ¼å¼ï¼š(key, page_id, text, vector_similarity, sha1, query_text, retrieval_source)
            å¢åŠ query_textå­—æ®µä»¥è¿½è¸ªå‘½ä¸­æ¥æºï¼Œå¢åŠ retrieval_sourceå­—æ®µä»¥è¿½è¸ªæ£€ç´¢æ–¹æ³•æ¥æº
            """
            local_hits = []
            traversal_details_list = []
            try:
                document = report["document"]
                vector_db = report["vector_db"]
                chunks = document["content"]["chunks"]
                pages = document["content"]["pages"]
                sha1 = document["metainfo"]["sha1_name"]
                actual_top_n = min(top_n, len(chunks))
                if actual_top_n == 0:
                    return []
                
                # æ ¹æ®æ£€ç´¢æ–¹æ³•é€‰æ‹©ä¸åŒçš„ç­–ç•¥
                if retrieval_method == "basic":
                    # Basic Retrieval - å®Œå…¨ä¿æŒåŸæœ‰é€»è¾‘ä¸å˜
                    distances, indices = vector_db.search(x=embedding_array, k=actual_top_n)
                
                    for distance, index in zip(distances[0], indices[0]):
                        vector_similarity = round(float(distance)*inner_factor, 4)
                        chunk = chunks[index]
                        parent_page = next(page for page in pages if page["page"] == chunk["page"])
                        
                        if return_parent_pages:
                            # Include sha1 in key to differentiate same page numbers across different reports
                            key = (sha1, "page", parent_page["page"])
                            text = parent_page["text"]
                            page_id = parent_page["page"]
                        else:
                            key = (sha1, "chunk", index)
                            text = chunk["text"]
                            page_id = chunk["page"]
                        
                        local_hits.append((key, page_id, text, vector_similarity, sha1, "basic"))
                
                elif retrieval_method in ["ssg", "triangulation"]:
                    # SSG / Triangulation - ç‹¬ç«‹è¿è¡Œï¼Œä¸ä¾èµ– basic search
                    # é¦–å…ˆæ‰¾åˆ°é”šç‚¹ï¼ˆåªæ‰¾1ä¸ªæœ€ç›¸ä¼¼çš„ä½œä¸ºèµ·å§‹ç‚¹ï¼‰
                    anchor_idx = None
                    anchor_score = None
                    anchor_vec = None
                    distances, indices = vector_db.search(x=embedding_array, k=1)
                    if len(distances[0]) > 0 and indices[0][0] != -1:
                        anchor_idx = int(indices[0][0])
                        anchor_score = float(distances[0][0])
                        anchor_vec = self._get_vector_by_id(vector_db, anchor_idx)
                    
                    if anchor_vec is not None and anchor_idx is not None:
                        expansion_result = None
                        if retrieval_method == "ssg":
                            expansion_result = self._ssg_search(
                                vector_db, anchor_idx, anchor_vec, 
                                max_hops=max_hops, neighbor_k=neighbor_k
                            )
                        elif retrieval_method == "triangulation":
                            expansion_result = self._triangulation_search(
                                vector_db, embedding_array, anchor_idx, anchor_vec,
                                max_hops=max_hops, neighbor_k=neighbor_k
                            )
                        
                        if expansion_result and isinstance(expansion_result, dict):
                            expanded_results = expansion_result.get("results", [])
                            traversal_details = expansion_result.get("traversal_details", None)
                            
                            # ä¿å­˜éå†è¯¦æƒ…
                            if traversal_details:
                                traversal_details_list.append(traversal_details)
                            
                            # è®¾ç½®é”šç‚¹åˆ†æ•°
                            if traversal_details:
                                traversal_details["anchor"]["score"] = anchor_score
                            
                            # å¤„ç†æ‰©å±•ç»“æœ
                            for score, idx in expanded_results:
                                if idx == -1:
                                    continue
                                
                                chunk = chunks[idx]
                                parent_page = next(page for page in pages if page["page"] == chunk["page"])
                                
                                if return_parent_pages:
                                    key = (sha1, "page", parent_page["page"])
                                    text = parent_page["text"]
                                    page_id = parent_page["page"]
                                else:
                                    key = (sha1, "chunk", idx)
                                    text = chunk["text"]
                                    page_id = chunk["page"]
                                
                                # å¯¹äº SSGï¼Œé‡æ–°è®¡ç®— query-to-chunk ç›¸ä¼¼åº¦
                                if retrieval_method == "ssg":
                                    candidate_vec = self._get_vector_by_id(vector_db, idx)
                                    if candidate_vec is not None:
                                        query_vec_flat = embedding_array.flatten()
                                        candidate_vec_flat = candidate_vec.flatten()
                                        query_sim = np.dot(query_vec_flat, candidate_vec_flat)
                                        vector_similarity = round(float(query_sim) * inner_factor, 4)
                                    else:
                                        vector_similarity = round(float(score) * inner_factor, 4)
                                    retrieval_source = "ssg"
                                else:  # triangulation
                                    # Triangulation: ä½¿ç”¨è´¨å¿ƒåˆ†æ•°ï¼Œæˆ–è€…é‡æ–°è®¡ç®— query-to-chunk ç›¸ä¼¼åº¦
                                    candidate_vec = self._get_vector_by_id(vector_db, idx)
                                    if candidate_vec is not None:
                                        query_vec_flat = embedding_array.flatten()
                                        candidate_vec_flat = candidate_vec.flatten()
                                        query_sim = np.dot(query_vec_flat, candidate_vec_flat)
                                        vector_similarity = round(float(query_sim) * inner_factor, 4)
                                    else:
                                        vector_similarity = round(float(score) * inner_factor, 4)
                                    retrieval_source = "triangulation"
                                
                                local_hits.append((key, page_id, text, vector_similarity, sha1, retrieval_source))
                        else:
                            # å¦‚æœæ‰©å±•å¤±è´¥ï¼Œè‡³å°‘è¿”å›é”šç‚¹
                            if anchor_idx is not None:
                                chunk = chunks[anchor_idx]
                                parent_page = next(page for page in pages if page["page"] == chunk["page"])
                                
                                if return_parent_pages:
                                    key = (sha1, "page", parent_page["page"])
                                    text = parent_page["text"]
                                    page_id = parent_page["page"]
                                else:
                                    key = (sha1, "chunk", anchor_idx)
                                    text = chunk["text"]
                                    page_id = chunk["page"]
                                
                                vector_similarity = round(float(anchor_score) * inner_factor, 4)
                                retrieval_source = retrieval_method  # "ssg" or "triangulation"
                                local_hits.append((key, page_id, text, vector_similarity, sha1, retrieval_source))
                    else:
                        # é”šç‚¹æŸ¥æ‰¾å¤±è´¥ï¼Œè¿”å›ç©º
                        return []
                elif retrieval_method == "hybrid_expansion":
                    # Hybrid Expansion: Basic Retrieval -> Top-K -> SSGæ‰©å±•(Top-10) + Triangulationæ‰©å±•(Top-20)
                    basic_top_k = 50  # å¯é…ç½®å‚æ•°
                    distances, indices = vector_db.search(x=embedding_array, k=min(basic_top_k, len(chunks)))
                    
                    basic_results = []
                    basic_keys_set = set()  # ç”¨äºå¿«é€Ÿæ£€æŸ¥chunkæ˜¯å¦åœ¨basic Top-50ä¸­
                    # ç”¨äºè¿½è¸ªç®—æ³•ç‰¹å®šçš„å¬å›ç»“æœï¼ˆä»…æ–°å‘ç°çš„chunkï¼‰
                    ssg_new_chunks = []  # SSGæ–°å‘ç°çš„chunkï¼ˆä¸åœ¨basic Top-50ä¸­ï¼‰
                    tri_new_chunks = []  # Triangulationæ–°å‘ç°çš„chunkï¼ˆä¸åœ¨basic Top-50ä¸­ï¼‰
                    
                    for distance, index in zip(distances[0], indices[0]):
                        vector_similarity = round(float(distance)*inner_factor, 4)
                        chunk = chunks[index]
                        parent_page = next(page for page in pages if page["page"] == chunk["page"])
                        
                        if return_parent_pages:
                            key = (sha1, "page", parent_page["page"])
                            text = parent_page["text"]
                            page_id = parent_page["page"]
                        else:
                            key = (sha1, "chunk", index)
                            text = chunk["text"]
                            page_id = chunk["page"]
                        
                        basic_results.append((key, page_id, text, vector_similarity, sha1, index))
                        basic_keys_set.add(key)  # è®°å½•basic Top-50çš„keys
                        local_hits.append((key, page_id, text, vector_similarity, sha1, "basic"))
                    
                    # å¯¹Top-10è¿›è¡ŒSSGæ‰©å±•
                    ssg_top_k = 10
                    ssg_total_expanded = 0  # ç»Ÿè®¡SSGæ‰©å±•çš„æ€»æ•°
                    ssg_new_only = 0  # ç»Ÿè®¡ä»…ç”±SSGå¬å›çš„chunkæ•°
                    for key, page_id, text, vector_similarity, sha1, idx in basic_results[:ssg_top_k]:
                        chunk_vec = self._get_vector_by_id(vector_db, idx)
                        if chunk_vec is not None:
                            ssg_result = self._ssg_search(
                                vector_db, idx, chunk_vec, 
                                max_hops=max_hops, neighbor_k=neighbor_k
                            )
                            if ssg_result and isinstance(ssg_result, dict):
                                expanded_results = ssg_result.get("results", [])
                                self._safe_print(f"[DEBUG] SSGæ‰©å±•: anchor page={page_id}, æ‰©å±•ç»“æœæ•°={len(expanded_results)}")
                                for score, expanded_idx in expanded_results:
                                    if expanded_idx == -1:
                                        continue
                                    
                                    expanded_chunk = chunks[expanded_idx]
                                    expanded_parent_page = next(page for page in pages if page["page"] == expanded_chunk["page"])
                                    
                                    if return_parent_pages:
                                        expanded_key = (sha1, "page", expanded_parent_page["page"])
                                        expanded_text = expanded_parent_page["text"]
                                        expanded_page_id = expanded_parent_page["page"]
                                    else:
                                        expanded_key = (sha1, "chunk", expanded_idx)
                                        expanded_text = expanded_chunk["text"]
                                        expanded_page_id = expanded_chunk["page"]
                                    
                                    # æ£€æŸ¥è¿™ä¸ªchunkæ˜¯å¦å·²ç»åœ¨basic_resultsä¸­ï¼ˆé¿å…é‡å¤æ ‡è®°ï¼‰
                                    # å¦‚æœå·²ç»åœ¨basic_resultsä¸­ï¼Œæˆ‘ä»¬ä»ç„¶æ·»åŠ å®ƒï¼Œä½†æ ‡è®°ä¸º"ssg"ï¼Œè¿™æ ·åœ¨èšåˆæ—¶ä¼šæ­£ç¡®æ˜¾ç¤ºæ–¹æ³•å¤šæ ·æ€§
                                    # é‡æ–°è®¡ç®— query-to-chunk ç›¸ä¼¼åº¦
                                    candidate_vec = self._get_vector_by_id(vector_db, expanded_idx)
                                    if candidate_vec is not None:
                                        query_vec_flat = embedding_array.flatten()
                                        candidate_vec_flat = candidate_vec.flatten()
                                        query_sim = np.dot(query_vec_flat, candidate_vec_flat)
                                        expanded_similarity = round(float(query_sim) * inner_factor, 4)
                                    else:
                                        expanded_similarity = round(float(score) * inner_factor, 4)
                                    
                                    # æ— è®ºchunkæ˜¯å¦åœ¨basic Top-50ä¸­ï¼Œéƒ½æ·»åŠ ä¸º"ssg"ï¼Œè¿™æ ·èšåˆæ—¶æ‰èƒ½æ­£ç¡®æ˜¾ç¤ºæ–¹æ³•å¤šæ ·æ€§
                                    # å¦‚æœå·²ç»åœ¨basicä¸­ï¼Œèšåˆé€»è¾‘ä¼šåˆå¹¶ï¼ˆbasic + ssg = 2ç§æ–¹æ³•ï¼‰
                                    local_hits.append((expanded_key, expanded_page_id, expanded_text, expanded_similarity, sha1, "ssg"))
                                    ssg_total_expanded += 1
                                    
                                    # åªæœ‰å½“è¿™ä¸ªchunkä¸åœ¨basicçš„Top-50ä¸­æ—¶ï¼Œæ‰è®°å½•ä¸º"æ–°å‘ç°çš„chunk"ï¼ˆç”¨äºç®—æ³•è´¡çŒ®åˆ†æï¼‰
                                    if expanded_key not in basic_keys_set:
                                        # æ–°å‘ç°çš„chunkï¼Œè®°å½•åˆ°ssg_new_chunksï¼ˆç”¨äºç®—æ³•è´¡çŒ®åˆ†æï¼‰
                                        ssg_new_chunks.append({
                                            "key": expanded_key,
                                            "page": expanded_page_id,
                                            "text": expanded_text,
                                            "vector_similarity": expanded_similarity,
                                            "source_sha1": sha1,
                                            "anchor_page": page_id,  # ä»å“ªä¸ªanchoræ‰©å±•è€Œæ¥
                                            "score": score  # SSGå†…éƒ¨å¾—åˆ†
                                        })
                                        ssg_new_only += 1
                                        self._safe_print(f"[DEBUG] SSGæ–°å‘ç°chunk: page={expanded_page_id}, similarity={expanded_similarity:.4f}, anchor={page_id}")
                                    else:
                                        self._safe_print(f"[DEBUG] SSGæ‰©å±•chunkï¼ˆå·²åœ¨Basic Top-50ä¸­ï¼‰: page={expanded_page_id}, similarity={expanded_similarity:.4f}, anchor={page_id}")
                                    # å¦‚æœexpanded_keyåœ¨basic_keys_setä¸­ï¼Œè¯´æ˜å®ƒä¹Ÿåœ¨basic Top-50ä¸­ï¼Œèšåˆæ—¶ä¼šæ˜¾ç¤ºæ–¹æ³•å¤šæ ·æ€§ï¼ˆbasic + ssgï¼‰
                    
                    self._safe_print(f"[DEBUG] SSGæ‰©å±•ç»Ÿè®¡: æ€»æ‰©å±•æ•°={ssg_total_expanded}, ä»…SSGå¬å›çš„chunkæ•°={ssg_new_only}, å·²åœ¨Basic Top-50ä¸­çš„chunkæ•°={ssg_total_expanded - ssg_new_only}")
                    
                    # å¯¹Top-20è¿›è¡ŒTriangulationæ‰©å±•
                    tri_top_k = 20
                    tri_total_expanded = 0  # ç»Ÿè®¡Triangulationæ‰©å±•çš„æ€»æ•°
                    tri_new_only = 0  # ç»Ÿè®¡ä»…ç”±Triangulationå¬å›çš„chunkæ•°
                    for key, page_id, text, vector_similarity, sha1, idx in basic_results[:tri_top_k]:
                        chunk_vec = self._get_vector_by_id(vector_db, idx)
                        if chunk_vec is not None:
                            tri_result = self._triangulation_search(
                                vector_db, embedding_array, idx, chunk_vec,
                                max_hops=max_hops, neighbor_k=neighbor_k
                            )
                            if tri_result and isinstance(tri_result, dict):
                                expanded_results = tri_result.get("results", [])
                                for score, expanded_idx in expanded_results:
                                    if expanded_idx == -1:
                                        continue
                                    
                                    expanded_chunk = chunks[expanded_idx]
                                    expanded_parent_page = next(page for page in pages if page["page"] == expanded_chunk["page"])
                                    
                                    if return_parent_pages:
                                        expanded_key = (sha1, "page", expanded_parent_page["page"])
                                        expanded_text = expanded_parent_page["text"]
                                        expanded_page_id = expanded_parent_page["page"]
                                    else:
                                        expanded_key = (sha1, "chunk", expanded_idx)
                                        expanded_text = expanded_chunk["text"]
                                        expanded_page_id = expanded_chunk["page"]
                                    
                                    # æ£€æŸ¥è¿™ä¸ªchunkæ˜¯å¦å·²ç»åœ¨basic_resultsä¸­ï¼ˆé¿å…é‡å¤æ ‡è®°ï¼‰
                                    # å¦‚æœå·²ç»åœ¨basic_resultsä¸­ï¼Œæˆ‘ä»¬ä»ç„¶æ·»åŠ å®ƒï¼Œä½†æ ‡è®°ä¸º"triangulation"ï¼Œè¿™æ ·åœ¨èšåˆæ—¶ä¼šæ­£ç¡®æ˜¾ç¤ºæ–¹æ³•å¤šæ ·æ€§
                                    # é‡æ–°è®¡ç®— query-to-chunk ç›¸ä¼¼åº¦
                                    candidate_vec = self._get_vector_by_id(vector_db, expanded_idx)
                                    if candidate_vec is not None:
                                        query_vec_flat = embedding_array.flatten()
                                        candidate_vec_flat = candidate_vec.flatten()
                                        query_sim = np.dot(query_vec_flat, candidate_vec_flat)
                                        expanded_similarity = round(float(query_sim) * inner_factor, 4)
                                    else:
                                        expanded_similarity = round(float(score) * inner_factor, 4)
                                    
                                    # æ— è®ºchunkæ˜¯å¦åœ¨basic Top-50ä¸­ï¼Œéƒ½æ·»åŠ ä¸º"triangulation"ï¼Œè¿™æ ·èšåˆæ—¶æ‰èƒ½æ­£ç¡®æ˜¾ç¤ºæ–¹æ³•å¤šæ ·æ€§
                                    # å¦‚æœå·²ç»åœ¨basicä¸­ï¼Œèšåˆé€»è¾‘ä¼šåˆå¹¶ï¼ˆbasic + triangulation = 2ç§æ–¹æ³•ï¼‰
                                    local_hits.append((expanded_key, expanded_page_id, expanded_text, expanded_similarity, sha1, "triangulation"))
                                    tri_total_expanded += 1
                                    
                                    # åªæœ‰å½“è¿™ä¸ªchunkä¸åœ¨basicçš„Top-50ä¸­æ—¶ï¼Œæ‰è®°å½•ä¸º"æ–°å‘ç°çš„chunk"ï¼ˆç”¨äºç®—æ³•è´¡çŒ®åˆ†æï¼‰
                                    if expanded_key not in basic_keys_set:
                                        # æ–°å‘ç°çš„chunkï¼Œè®°å½•åˆ°tri_new_chunksï¼ˆç”¨äºç®—æ³•è´¡çŒ®åˆ†æï¼‰
                                        tri_new_chunks.append({
                                            "key": expanded_key,
                                            "page": expanded_page_id,
                                            "text": expanded_text,
                                            "vector_similarity": expanded_similarity,
                                            "source_sha1": sha1,
                                            "anchor_page": page_id,  # ä»å“ªä¸ªanchoræ‰©å±•è€Œæ¥
                                            "score": score  # Triangulationå†…éƒ¨å¾—åˆ†ï¼ˆè´¨å¿ƒå¾—åˆ†ï¼‰
                                        })
                                        tri_new_only += 1
                                        self._safe_print(f"[DEBUG] Triangulationæ–°å‘ç°chunk: page={expanded_page_id}, similarity={expanded_similarity:.4f}, anchor={page_id}")
                                    else:
                                        self._safe_print(f"[DEBUG] Triangulationæ‰©å±•chunkï¼ˆå·²åœ¨Basic Top-50ä¸­ï¼‰: page={expanded_page_id}, similarity={expanded_similarity:.4f}, anchor={page_id}")
                                    # å¦‚æœexpanded_keyåœ¨basic_keys_setä¸­ï¼Œè¯´æ˜å®ƒä¹Ÿåœ¨basic Top-50ä¸­ï¼Œèšåˆæ—¶ä¼šæ˜¾ç¤ºæ–¹æ³•å¤šæ ·æ€§ï¼ˆbasic + triangulationï¼‰
                    
                    self._safe_print(f"[DEBUG] Triangulationæ‰©å±•ç»Ÿè®¡: æ€»æ‰©å±•æ•°={tri_total_expanded}, ä»…Triangulationå¬å›çš„chunkæ•°={tri_new_only}, å·²åœ¨Basic Top-50ä¸­çš„chunkæ•°={tri_total_expanded - tri_new_only}")
                
                else:
                    # æœªçŸ¥çš„æ£€ç´¢æ–¹æ³•ï¼Œå›é€€åˆ° basic
                    self._safe_print(f"[WARNING] Unknown retrieval_method '{retrieval_method}', falling back to basic")
                    distances, indices = vector_db.search(x=embedding_array, k=actual_top_n)
                
                    for distance, index in zip(distances[0], indices[0]):
                        vector_similarity = round(float(distance)*inner_factor, 4)
                        chunk = chunks[index]
                        parent_page = next(page for page in pages if page["page"] == chunk["page"])
                        
                        if return_parent_pages:
                            key = (sha1, "page", parent_page["page"])
                            text = parent_page["text"]
                            page_id = parent_page["page"]
                        else:
                            key = (sha1, "chunk", index)
                            text = chunk["text"]
                            page_id = chunk["page"]
                        
                        local_hits.append((key, page_id, text, vector_similarity, sha1, "basic"))
                
            except Exception as e:
                self._safe_print(f"[ERROR] Vector search failed for query '{query_text[:60]}' in report {report.get('name')}: {e}")
            
            # å¦‚æœä½¿ç”¨äº†æ–°ç®—æ³•ä¸”æœ‰éå†è¯¦æƒ…ï¼Œè¿”å›å­—å…¸ï¼›å¦åˆ™è¿”å›åˆ—è¡¨ï¼ˆä¿æŒå…¼å®¹ï¼‰
            # å¦‚æœæ˜¯hybrid_expansionï¼Œå³ä½¿æ²¡æœ‰traversal_detailsï¼Œä¹Ÿè¦è¿”å›å­—å…¸ä»¥åŒ…å«algorithm_specific_results
            if traversal_details_list or retrieval_method == "hybrid_expansion":
                result_dict = {
                    "hits": local_hits
                }
                # å¦‚æœæœ‰éå†è¯¦æƒ…ï¼Œæ·»åŠ åˆ°å­—å…¸ä¸­
                if traversal_details_list:
                    result_dict["traversal_details"] = traversal_details_list[0] if len(traversal_details_list) == 1 else traversal_details_list
                
                # å¦‚æœæ˜¯hybrid_expansionï¼Œæ·»åŠ ç®—æ³•ç‰¹å®šçš„å¬å›ä¿¡æ¯
                if retrieval_method == "hybrid_expansion":
                    result_dict["algorithm_specific_results"] = {
                        "ssg_new_chunks": ssg_new_chunks,  # SSGæ–°å‘ç°çš„chunkï¼ˆä¸åœ¨basic Top-50ä¸­ï¼‰
                        "triangulation_new_chunks": tri_new_chunks,  # Triangulationæ–°å‘ç°çš„chunkï¼ˆä¸åœ¨basic Top-50ä¸­ï¼‰
                        "basic_count": len(basic_results),  # Basicæ£€ç´¢çš„æ•°é‡
                        "ssg_stats": {  # SSGæ‰©å±•ç»Ÿè®¡
                            "total_expanded": ssg_total_expanded,  # æ€»æ‰©å±•æ•°
                            "new_only": ssg_new_only,  # ä»…SSGå¬å›çš„chunkæ•°
                            "in_basic_top50": ssg_total_expanded - ssg_new_only  # å·²åœ¨Basic Top-50ä¸­çš„chunkæ•°
                        },
                        "triangulation_stats": {  # Triangulationæ‰©å±•ç»Ÿè®¡
                            "total_expanded": tri_total_expanded,  # æ€»æ‰©å±•æ•°
                            "new_only": tri_new_only,  # ä»…Triangulationå¬å›çš„chunkæ•°
                            "in_basic_top50": tri_total_expanded - tri_new_only  # å·²åœ¨Basic Top-50ä¸­çš„chunkæ•°
                        }
                    }
                    self._safe_print(f"[DEBUG] è¿”å›algorithm_specific_results: ssg_stats={result_dict['algorithm_specific_results']['ssg_stats']}, tri_stats={result_dict['algorithm_specific_results']['triangulation_stats']}")
                return result_dict
            return local_hits

        total_tasks = len(query_embeddings) * num_reports
        max_workers = min(self.parallel_workers, total_tasks) if total_tasks > 0 else 1
        vector_search_start = time.time()

        # æ”¶é›†æ‰€æœ‰ traversal_detailsï¼ˆæ¯ä¸ª query-document å¯¹å¯èƒ½æœ‰ä¸€ä¸ªï¼‰
        all_traversal_details = []
        # æ”¶é›†ç®—æ³•ç‰¹å®šçš„å¬å›ä¿¡æ¯ï¼ˆä»…hybrid_expansionï¼‰
        all_algorithm_specific_results = {
            "ssg_new_chunks": [],
            "triangulation_new_chunks": [],
            "basic_count": 0,
            "ssg_stats": {
                "total_expanded": 0,
                "new_only": 0,
                "in_basic_top50": 0
            },
            "triangulation_stats": {
                "total_expanded": 0,
                "new_only": 0,
                "in_basic_top50": 0
            }
        }
        
        with ThreadPoolExecutor(max_workers=max(1, max_workers)) as executor:
            # åˆ›å»ºä¸€ä¸ªæ˜ å°„æ¥è¿½è¸ªæ¯ä¸ªfutureå¯¹åº”çš„æŸ¥è¯¢æ–‡æœ¬
            future_to_query = {}
            for report in matching_reports:
                for q_text, embedding_array in query_embeddings.items():
                    future = executor.submit(process_query_for_document, report, q_text, embedding_array)
                    future_to_query[future] = q_text
            
            for future in as_completed(future_to_query.keys()):
                query_text = future_to_query[future]  # è·å–å¯¹åº”çš„æŸ¥è¯¢æ–‡æœ¬
                result = future.result()
                if not result:
                    continue
                
                # æå– hits å’Œ traversal_details
                if isinstance(result, dict):
                    doc_hits = result.get("hits", [])
                    trav_details = result.get("traversal_details", None)
                    if trav_details:
                        all_traversal_details.append(trav_details)
                    # æå–ç®—æ³•ç‰¹å®šçš„å¬å›ä¿¡æ¯ï¼ˆä»…hybrid_expansionï¼‰
                    algo_results = result.get("algorithm_specific_results", None)
                    if algo_results:
                        all_algorithm_specific_results["ssg_new_chunks"].extend(algo_results.get("ssg_new_chunks", []))
                        all_algorithm_specific_results["triangulation_new_chunks"].extend(algo_results.get("triangulation_new_chunks", []))
                        all_algorithm_specific_results["basic_count"] += algo_results.get("basic_count", 0)
                        
                        # ç´¯åŠ SSGç»Ÿè®¡
                        ssg_stats = algo_results.get("ssg_stats", {})
                        if ssg_stats:
                            all_algorithm_specific_results["ssg_stats"]["total_expanded"] += ssg_stats.get("total_expanded", 0)
                            all_algorithm_specific_results["ssg_stats"]["new_only"] += ssg_stats.get("new_only", 0)
                            all_algorithm_specific_results["ssg_stats"]["in_basic_top50"] += ssg_stats.get("in_basic_top50", 0)
                            self._safe_print(f"[DEBUG] ç´¯åŠ SSGç»Ÿè®¡: å½“å‰æ–‡æ¡£={ssg_stats}, ç´¯åŠ åæ€»è®¡={all_algorithm_specific_results['ssg_stats']}")
                        
                        # ç´¯åŠ Triangulationç»Ÿè®¡
                        tri_stats = algo_results.get("triangulation_stats", {})
                        if tri_stats:
                            all_algorithm_specific_results["triangulation_stats"]["total_expanded"] += tri_stats.get("total_expanded", 0)
                            all_algorithm_specific_results["triangulation_stats"]["new_only"] += tri_stats.get("new_only", 0)
                            all_algorithm_specific_results["triangulation_stats"]["in_basic_top50"] += tri_stats.get("in_basic_top50", 0)
                            self._safe_print(f"[DEBUG] ç´¯åŠ Triangulationç»Ÿè®¡: å½“å‰æ–‡æ¡£={tri_stats}, ç´¯åŠ åæ€»è®¡={all_algorithm_specific_results['triangulation_stats']}")
                        
                        self._safe_print(f"[DEBUG] æ”¶é›†åˆ°ç®—æ³•ç‰¹å®šç»“æœ: basic_count={algo_results.get('basic_count', 0)}, ssg_new={len(algo_results.get('ssg_new_chunks', []))}, tri_new={len(algo_results.get('triangulation_new_chunks', []))}")
                else:
                    # å…¼å®¹æ—§æ ¼å¼ï¼ˆå…ƒç»„åˆ—è¡¨ï¼‰
                    doc_hits = result
                
                if not doc_hits:
                    continue
                
                # è®°å½•è¿™ä¸ªæŸ¥è¯¢æ£€ç´¢åˆ°çš„chunkæ•°é‡
                self._safe_print(f"[DEBUG] æŸ¥è¯¢ '{query_text[:80]}...' æ£€ç´¢åˆ° {len(doc_hits)} ä¸ªchunks")
                
                with aggregation_lock:
                    # å¤„ç†æ¯ä¸ªhitï¼ˆæ”¯æŒ5å…ƒç»„å’Œ6å…ƒç»„æ ¼å¼ï¼Œå‘åå…¼å®¹ï¼‰
                    for hit in doc_hits:
                        if len(hit) == 5:
                            # æ—§æ ¼å¼ï¼ˆå‘åå…¼å®¹ï¼‰ï¼š(key, page_id, text, vector_similarity, sha1)
                            key, page_id, text, vector_similarity, sha1 = hit
                            retrieval_source = "basic"  # é»˜è®¤å€¼
                        elif len(hit) == 6:
                            # æ–°æ ¼å¼ï¼š(key, page_id, text, vector_similarity, sha1, retrieval_source)
                            key, page_id, text, vector_similarity, sha1, retrieval_source = hit
                        else:
                            self._safe_print(f"[ERROR] Unexpected hit format with {len(hit)} elements: {hit}")
                            continue
                        
                        if key not in aggregated_results:
                            aggregated_results[key] = {
                                "page": page_id,
                                "text": text,
                                "similarities": [vector_similarity],  # æ”¹åï¼šdistances -> similarities
                                "query_sources": [query_text],  # è®°å½•å‘½ä¸­æ¥æº
                                "retrieval_sources": [retrieval_source],  # æ–°å¢ï¼šè®°å½•æ£€ç´¢æ–¹æ³•æ¥æº
                                "count": 1,
                                "source_sha1": sha1
                            }
                            # è°ƒè¯•ï¼šè®°å½•æ–°chunkçš„åˆå§‹æ–¹æ³•ï¼ˆä»…å¯¹SSGå’ŒTriangulationï¼‰
                            if retrieval_source in ["ssg", "triangulation"]:
                                self._safe_print(f"[DEBUG] èšåˆ: æ–°chunk page={page_id}, åˆå§‹æ–¹æ³•={retrieval_source} (ä»…ç”±{retrieval_source}å¬å›)")
                        else:
                            # è¿½åŠ ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆç”¨äºè®¡ç®—æœ€å¤§å€¼ï¼‰
                            aggregated_results[key]["similarities"].append(vector_similarity)
                            
                            # åªæœ‰å½“è¿™ä¸ªæ£€ç´¢æ–¹æ³•è¿˜æ²¡æœ‰è¢«è®°å½•æ—¶ï¼Œæ‰æ·»åŠ åˆ°retrieval_sources
                            # è¿™æ˜¯ä¸ºäº†é¿å…åŒä¸€ä¸ªchunkè¢«åŒä¸€ä¸ªæ–¹æ³•å¤šæ¬¡å‘½ä¸­æ—¶é‡å¤æ·»åŠ 
                            if retrieval_source not in aggregated_results[key]["retrieval_sources"]:
                                old_methods = aggregated_results[key]["retrieval_sources"].copy()
                                aggregated_results[key]["retrieval_sources"].append(retrieval_source)
                                new_methods = aggregated_results[key]["retrieval_sources"]
                                
                                # è°ƒè¯•ï¼šè®°å½•æ–¹æ³•æ·»åŠ ï¼ˆä»…å¯¹SSGå’ŒTriangulationï¼Œæˆ–æ–¹æ³•æ•°é‡å˜åŒ–æ—¶ï¼‰
                                if retrieval_source in ["ssg", "triangulation"] or len(new_methods) >= 2:
                                    method_change = f"{old_methods} -> {new_methods}"
                                    self._safe_print(f"[DEBUG] èšåˆ: chunk page={page_id}, æ·»åŠ æ–¹æ³•={retrieval_source}, æ–¹æ³•å˜åŒ–={method_change}")
                            
                            # åªæœ‰å½“è¿™ä¸ªæŸ¥è¯¢è¿˜æ²¡æœ‰è¢«è®¡æ•°æ—¶ï¼Œæ‰å¢åŠ count
                            # è¿™æ˜¯ä¸ºäº†é¿å… return_parent_pages=True æ—¶ï¼ŒåŒä¸€ä¸ªæŸ¥è¯¢å‘½ä¸­åŒä¸€pageçš„å¤šä¸ªchunkså¯¼è‡´é‡å¤è®¡æ•°
                            if query_text not in aggregated_results[key]["query_sources"]:
                                aggregated_results[key]["count"] += 1
                                aggregated_results[key]["query_sources"].append(query_text)

        timing_info['vector_search'] = time.time() - vector_search_start
        
        # æ·»åŠ èšåˆç»“æœçš„è¯¦ç»†è°ƒè¯•ä¿¡æ¯
        self._safe_print(f"[DEBUG] ========== èšåˆç»“æœç»Ÿè®¡ ==========")
        self._safe_print(f"[DEBUG] æ€»å…±èšåˆäº† {len(aggregated_results)} ä¸ªå”¯ä¸€çš„chunk")
        
        # ç»Ÿè®¡å‘½ä¸­æ¬¡æ•°åˆ†å¸ƒ
        hit_count_distribution = {}
        high_hit_chunks = []  # å‘½ä¸­æ¬¡æ•°>=3çš„chunks
        
        for key, info in aggregated_results.items():
            count = info["count"]
            hit_count_distribution[count] = hit_count_distribution.get(count, 0) + 1
            if count >= 3:
                high_hit_chunks.append({
                    "key": key,
                    "page": info["page"],
                    "count": count,
                    "similarities": info["similarities"],
                    "query_sources": info.get("query_sources", [])
                })
        
        self._safe_print(f"[DEBUG] å‘½ä¸­æ¬¡æ•°åˆ†å¸ƒ:")
        for count in sorted(hit_count_distribution.keys(), reverse=True):
            self._safe_print(f"[DEBUG]   - å‘½ä¸­{count}æ¬¡: {hit_count_distribution[count]}ä¸ªchunks")
        
        if high_hit_chunks:
            self._safe_print(f"[DEBUG] é«˜å‘½ä¸­chunks (>=3æ¬¡):")
            for chunk in high_hit_chunks[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                self._safe_print(f"[DEBUG]   - Page {chunk['page']}: å‘½ä¸­{chunk['count']}æ¬¡")
                self._safe_print(f"[DEBUG]      å¾—åˆ†: {chunk['similarities']}")
                if chunk['query_sources']:
                    self._safe_print(f"[DEBUG]      æŸ¥è¯¢æ¥æºæ•°: {len(chunk['query_sources'])}")
                    for i, q in enumerate(chunk['query_sources'][:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ªæŸ¥è¯¢
                        self._safe_print(f"[DEBUG]        {i}. {q[:80]}...")
        self._safe_print(f"[DEBUG] ==================================")
    
        # æ–°çš„èšåˆç­–ç•¥ï¼šä¿ç•™æœ€å¤§ç›¸ä¼¼åº¦ + æŸ¥è¯¢å‘½ä¸­æ•°å¥–åŠ± + æ–¹æ³•å¤šæ ·æ€§å¥–åŠ±
        def calculate_final_similarity(info):
            # 1. ä¿ç•™æœ€å¤§ç›¸ä¼¼åº¦ï¼ˆä¼˜å…ˆä¿¡ä»»ç›´æ¥ç›¸å…³çš„æ–¹æ³•ï¼‰
            base_similarity = max(info["similarities"])
            
            # 2. æŸ¥è¯¢å‘½ä¸­æ•°å¥–åŠ±ï¼ˆä¿æŒå½“å‰é€»è¾‘ï¼‰
            query_bonus = 1.0 + 0.2 * (info["count"] - 1)
            
            # 3. æ–¹æ³•å¤šæ ·æ€§å¥–åŠ±
            # æ³¨æ„ï¼šå¦‚æœretrieval_sourcesä¸ºç©ºï¼Œè¯´æ˜è¿™ä¸ªchunkå¯èƒ½æœ‰é—®é¢˜ï¼Œä½†æˆ‘ä»¬ä¸åº”è¯¥é»˜è®¤å‡è®¾å®ƒæ˜¯"basic"
            raw_sources = info.get("retrieval_sources", [])
            if not raw_sources:
                self._safe_print(f"[WARNING] Chunk page={info['page']} æ²¡æœ‰retrieval_sourcesï¼Œè¿™å¯èƒ½æ˜¯ä¸ªbug")
                raw_sources = ["basic"]  # é™çº§å¤„ç†
            unique_methods = set(raw_sources)
            method_diversity_bonus = 1.0
            if len(unique_methods) >= 2:
                # è¢«2ç§ä»¥ä¸Šæ–¹æ³•å‘½ä¸­ï¼Œç»™äºˆå¥–åŠ±
                method_diversity_bonus = 1.0 + 0.1 * (len(unique_methods) - 1)
                # 2ç§æ–¹æ³•ï¼š1.1ï¼Œ3ç§æ–¹æ³•ï¼š1.2
            
            # 4. æœ€ç»ˆå¾—åˆ†
            final_similarity = base_similarity * query_bonus * method_diversity_bonus
            return final_similarity, base_similarity, unique_methods
    
        final_results = []
        # è®°å½•æ‰€æœ‰æ–°å‘ç°çš„pageï¼ˆä»…ç”±SSGæˆ–Triangulationå¬å›ï¼‰
        new_pages_only_ssg = []
        new_pages_only_tri = []
        for idx, (key, info) in enumerate(aggregated_results.items()):
            final_similarity, base_similarity, unique_methods = calculate_final_similarity(info)
            
            # è®°å½•æ–°å‘ç°çš„pageï¼ˆä»…ç”±SSGæˆ–Triangulationå¬å›ï¼Œä¸åœ¨Basic Top-50ä¸­ï¼‰
            if len(unique_methods) == 1:
                if "ssg" in unique_methods:
                    new_pages_only_ssg.append((info["page"], info["source_sha1"], final_similarity))
                elif "triangulation" in unique_methods:
                    new_pages_only_tri.append((info["page"], info["source_sha1"], final_similarity))
            
            # è°ƒè¯•ï¼šæ£€æŸ¥retrieval_sourcesï¼ˆä»…åœ¨å‰20ä¸ªchunkä¸­è¾“å‡ºï¼Œé¿å…æ—¥å¿—è¿‡å¤šï¼‰
            if idx < 20:
                raw_sources = info.get("retrieval_sources", ["basic"])
                methods_str = ", ".join(sorted(unique_methods))
                method_count = len(unique_methods)
                
                # æ ¹æ®æ–¹æ³•æ•°é‡åˆ†ç±»æ˜¾ç¤º
                if method_count == 1:
                    method_type = "å•ç‹¬å¬å›"
                    if "ssg" in unique_methods:
                        method_type += " (ä»…SSG)"
                    elif "triangulation" in unique_methods:
                        method_type += " (ä»…Triangulation)"
                    elif "basic" in unique_methods:
                        method_type += " (ä»…Basic)"
                elif method_count == 2:
                    method_type = "ä¸¤ç§æ–¹æ³•ç»„åˆ"
                    if "basic" in unique_methods and "ssg" in unique_methods:
                        method_type += " (Basic+SSG)"
                    elif "basic" in unique_methods and "triangulation" in unique_methods:
                        method_type += " (Basic+Triangulation)"
                    elif "ssg" in unique_methods and "triangulation" in unique_methods:
                        method_type += " (SSG+Triangulation)"
                elif method_count == 3:
                    method_type = "ä¸‰ç§æ–¹æ³•è”åˆ (Basic+SSG+Triangulation)"
                else:
                    method_type = f"{method_count}ç§æ–¹æ³•"
                
                self._safe_print(f"[DEBUG] Chunk #{idx+1} page={info['page']}, æ–¹æ³•ç±»å‹={method_type}, æ–¹æ³•åˆ—è¡¨=[{methods_str}], raw_sources={raw_sources}")
            
            final_results.append({
                "vector_similarity": round(final_similarity, 4),  # æœ€ç»ˆå‘é‡ç›¸ä¼¼åº¦å¾—åˆ†ï¼ˆç”¨äºæ’åºï¼‰
                "max_original_similarity": round(base_similarity, 4),  # åŸå§‹å‘é‡ç›¸ä¼¼åº¦æœ€é«˜åˆ†
                "page": info["page"],
                "text": info["text"],
                "hit_count": info["count"],  # å‘½ä¸­æ¬¡æ•°
                "retrieval_sources": list(unique_methods),  # æ–°å¢ï¼šæ–¹æ³•æ¥æºåˆ—è¡¨ï¼ˆå·²å»é‡ï¼‰
                "source_sha1": info["source_sha1"],  # Include source document
                "query_sources": info.get("query_sources", [])  # æŸ¥è¯¢æ¥æºï¼ˆç”¨äºè°ƒè¯•ï¼‰
            })
    
        # èšåˆï¼šæŒ‰åŠ æƒåçš„ç›¸ä¼¼åº¦é™åºï¼Œå–å‰ top_nï¼ˆvector_similarityè¶Šå¤§è¶Šç›¸å…³ï¼‰
        final_results = sorted(final_results, key=lambda x: x["vector_similarity"], reverse=True)
        
        # è°ƒè¯•ï¼šæ˜¾ç¤ºæ–°å‘ç°çš„pageç»Ÿè®¡
        if new_pages_only_ssg or new_pages_only_tri:
            self._safe_print(f"[DEBUG] æ–°å‘ç°çš„Pageç»Ÿè®¡: SSG={len(new_pages_only_ssg)}, Triangulation={len(new_pages_only_tri)}")
            if new_pages_only_tri:
                self._safe_print(f"[DEBUG] Triangulationæ–°å‘ç°çš„Page (å‰10ä¸ª): {new_pages_only_tri[:10]}")
                # æ£€æŸ¥æ‰€æœ‰æ–°å‘ç°çš„pageæ˜¯å¦åœ¨final_resultsä¸­
                found_count = 0
                not_found_count = 0
                for page, sha1, sim in new_pages_only_tri:
                    rank_in_results = next((idx for idx, r in enumerate(final_results) if r.get("page") == page and r.get("source_sha1") == sha1), None)
                    if rank_in_results is not None:
                        found_count += 1
                        # åªæ‰“å°å‰20ä¸ªçš„è¯¦ç»†ä¿¡æ¯ï¼Œé¿å…æ—¥å¿—è¿‡å¤š
                        if found_count <= 20:
                            self._safe_print(f"[DEBUG] Page {page} (SHA1={sha1}, sim={sim:.4f}) åœ¨final_resultsä¸­: âœ… æ’å #{rank_in_results+1}")
                    else:
                        not_found_count += 1
                        # åªæ‰“å°å‰10ä¸ªæœªæ‰¾åˆ°çš„è¯¦ç»†ä¿¡æ¯
                        if not_found_count <= 10:
                            self._safe_print(f"[DEBUG] Page {page} (SHA1={sha1}, sim={sim:.4f}) åœ¨final_resultsä¸­: âŒ æœªæ‰¾åˆ°")
                self._safe_print(f"[DEBUG] Triangulationæ–°å‘ç°çš„Pageç»Ÿè®¡: åœ¨final_resultsä¸­={found_count}, æœªæ‰¾åˆ°={not_found_count}, æ€»è®¡={len(new_pages_only_tri)}")
        
        # ä¿å­˜æˆªæ–­å‰çš„å®Œæ•´ç»“æœï¼ˆç”¨äºæ˜¾ç¤º"åˆå§‹å¬å›ç»“æœ"ï¼‰
        all_initial_results = final_results.copy()  # æ‰©å±•åçš„å…¨éƒ¨ç»“æœï¼ˆæˆªæ–­å‰ï¼‰
        
        # Debug: æ˜¾ç¤ºèšåˆåçš„æ–‡æ¡£åˆ†å¸ƒ
        source_distribution = {}
        for res in final_results[:top_n]:
            source = res.get("source_sha1", "Unknown")
            source_distribution[source] = source_distribution.get(source, 0) + 1
        print(f"[DEBUG] Top {top_n} results distribution: {source_distribution}")
        print(f"[DEBUG] æ‰©å±•åçš„å…¨éƒ¨ç»“æœæ•°é‡: {len(all_initial_results)}, æˆªæ–­åæ•°é‡: {top_n}")
        
        final_results = final_results[:top_n]  # æˆªæ–­ï¼šåªå–å‰top_nè¿›å…¥reranker


        # æ£€ç´¢å®Œæˆ
        if progress_callback:
            progress_callback("âœ… æ£€ç´¢å®Œæˆï¼Œå‡†å¤‡é‡æ’åº...", 55)
        
        # Debug: ç»Ÿè®¡æ¥æºåˆ†å¸ƒ
        source_counts = {}
        for result in final_results[:top_n]:
            sha1 = result["source_sha1"]
            source_counts[sha1] = source_counts.get(sha1, 0) + 1
        print(f"[DEBUG] Top-{top_n} results distribution by source:")
        for sha1, count in sorted(source_counts.items(), key=lambda x: -x[1]):
            print(f"  {sha1}: {count} results")
        
        # å°†æ—¶é—´ä¿¡æ¯ã€æ‰©å±•æ–‡æœ¬ä¸ç»“æœä¸€èµ·è¿”å›
        # æ”¶é›†éå†è¯¦æƒ…ï¼ˆå¦‚æœæœ‰ï¼‰
        retrieval_details = None
        if all_traversal_details:
            retrieval_details = {
                "method": retrieval_method,
                "traversal_info": all_traversal_details[0] if len(all_traversal_details) == 1 else all_traversal_details,
                "max_hops": max_hops,
                "neighbor_k": neighbor_k
            }
        
        # å¦‚æœæ˜¯hybrid_expansionï¼Œæ·»åŠ ç®—æ³•ç‰¹å®šçš„å¬å›ä¿¡æ¯
        algorithm_contribution = None
        if retrieval_method == "hybrid_expansion":
            # å³ä½¿ basic_count ä¸º 0ï¼Œä¹Ÿè¿”å›ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¯èƒ½æ˜¯å¤šæ–‡æ¡£æŸ¥è¯¢å¯¼è‡´ï¼‰
            basic_count = all_algorithm_specific_results.get("basic_count", 0)
            ssg_new_chunks = all_algorithm_specific_results.get("ssg_new_chunks", [])
            tri_new_chunks = all_algorithm_specific_results.get("triangulation_new_chunks", [])
            
            algorithm_contribution = {
                "basic_retrieval_count": basic_count,
                "ssg_new_chunks_count": len(ssg_new_chunks),
                "triangulation_new_chunks_count": len(tri_new_chunks),
                "ssg_new_chunks": ssg_new_chunks,
                "triangulation_new_chunks": tri_new_chunks,
                "ssg_stats": all_algorithm_specific_results.get("ssg_stats", {
                    "total_expanded": 0,
                    "new_only": 0,
                    "in_basic_top50": 0
                }),
                "triangulation_stats": all_algorithm_specific_results.get("triangulation_stats", {
                    "total_expanded": 0,
                    "new_only": 0,
                    "in_basic_top50": 0
                })
            }
            self._safe_print(f"[DEBUG] Hybrid Expansionç»Ÿè®¡: Basic={basic_count}, SSGæ–°å‘ç°={len(ssg_new_chunks)}, Triangulationæ–°å‘ç°={len(tri_new_chunks)}")
            final_ssg_stats = all_algorithm_specific_results.get("ssg_stats", {})
            final_tri_stats = all_algorithm_specific_results.get("triangulation_stats", {})
            self._safe_print(f"[DEBUG] æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯: ssg_stats={final_ssg_stats}, tri_stats={final_tri_stats}")
            self._safe_print(f"[DEBUG] algorithm_contribution å·²ç”Ÿæˆ: {algorithm_contribution is not None}, keys={list(algorithm_contribution.keys()) if algorithm_contribution else []}")
        
        return {
            'results': final_results,  # æˆªæ–­åçš„ç»“æœï¼ˆè¿›å…¥rerankerï¼‰
            'timing': timing_info,
            'expansion_texts': expansion_texts,
            'retrieval_details': retrieval_details,
            'initial_retrieval_results': all_initial_results,  # åˆå§‹å¬å›ç»“æœï¼šæ‰©å±•åçš„å…¨éƒ¨ç»“æœï¼ˆæˆªæ–­å‰ï¼‰
            'algorithm_contribution': algorithm_contribution  # ç®—æ³•è´¡çŒ®ç»Ÿè®¡ï¼ˆä»…hybrid_expansionï¼‰
        }

    def retrieve_all(self, company_name: str) -> List[Dict]:
        """Retrieve all pages from all reports matching the company name."""
        #print("\n retrieve_all be used")
        
        # Collect all matching reports
        matching_reports = []
        for report in self.all_dbs:
            document = report.get("document", {})
            metainfo = document.get("metainfo")
            if not metainfo:
                continue
            if metainfo.get("company_name") == company_name:
                matching_reports.append(report)
        
        if not matching_reports:
            _log.error(f"No report found with '{company_name}' company name.")
            raise ValueError(f"No report found with '{company_name}' company name.")
        
        if len(matching_reports) > 1:
            print(f"[INFO] retrieve_all: Found {len(matching_reports)} reports for '{company_name}', retrieving all pages from all reports")
        
        # Collect pages from all matching reports
        all_pages = []
        for report in matching_reports:
            document = report["document"]
            pages = document["content"]["pages"]
            sha1 = document["metainfo"]["sha1_name"]
            
            for page in sorted(pages, key=lambda p: p["page"]):
                result = {
                    "vector_similarity": 0.5,
                    "page": page["page"],
                    "text": page["text"],
                    "source_sha1": sha1  # Track which report this page comes from
                }
                all_pages.append(result)
            
        return all_pages