import json
import logging
from typing import List, Tuple, Dict, Union, Optional
from rank_bm25 import BM25Okapi
import pickle
from pathlib import Path
import faiss
from src.api_requests import BaseQwenProcessor,BaseGeminiProcessor
from dotenv import load_dotenv
import os
import re
import numpy as np
from src.reranking import LLMReranker
from src.financial_glossary import (
    find_financial_concepts,
    format_concepts_for_prompt,
)
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock

_log = logging.getLogger(__name__)


def extract_years_from_question(question: str, expand_window: bool = True) -> List[int]:
    """
    ä»é—®é¢˜ä¸­æå–å¹´ä»½ä¿¡æ¯ï¼Œå¹¶å¯é€‰åœ°æ‰©å±•æ—¶é—´çª—å£
    æ”¯æŒæ ¼å¼ï¼š
    - æ˜ç¡®å¹´ä»½: "2025å¹´", "2023å¹´ç¬¬ä¸€å­£åº¦"
    - æ—¥æœŸæ ¼å¼: "2025å¹´9æœˆ30æ—¥"
    - å¤šå¹´ä»½æ¯”è¾ƒ: "2024å¹´ç›¸æ¯”2023å¹´" â†’ [2023, 2024]
    
    Args:
        question: ç”¨æˆ·é—®é¢˜
        expand_window: æ˜¯å¦æ‰©å±•æ—¶é—´çª—å£ï¼ˆåœ¨å¹´ä»½èŒƒå›´å‰åå„åŠ 1å¹´ï¼‰
    
    Returns:
        List[int]: æå–åˆ°çš„å¹´ä»½åˆ—è¡¨ï¼ˆå»é‡å¹¶æ’åºï¼‰
        
    ç¤ºä¾‹ï¼š
        é—®"2024å¹´xxx" + expand_window=True â†’ [2023, 2024, 2025]
        é—®"2024å¹´ç›¸æ¯”2023å¹´" + expand_window=True â†’ [2022, 2023, 2024, 2025] ï¼ˆèŒƒå›´æ‰©å±•è€Œéé€ä¸ªæ‰©å±•ï¼‰
        é—®"2024å¹´xxx" + expand_window=False â†’ [2024]
    """
    # æ­£åˆ™åŒ¹é… 20XXå¹´ æ ¼å¼
    year_pattern = r'(20\d{2})å¹´'
    matches = re.findall(year_pattern, question)
    extracted_years = [int(y) for y in matches]
    
    if not extracted_years:
        return []
    
    if expand_window:
        # æ‰¾åˆ°å¹´ä»½èŒƒå›´çš„æœ€å°å’Œæœ€å¤§å€¼
        min_year = min(extracted_years)
        max_year = max(extracted_years)
        
        # åœ¨èŒƒå›´å‰åå„æ‰©å±•1å¹´ï¼Œç”Ÿæˆè¿ç»­å¹´ä»½åˆ—è¡¨
        years = list(range(min_year - 1, max_year + 2))  # +2 å› ä¸º range ä¸åŒ…å«ç»“æŸå€¼
        
        print(f"[DEBUG] ğŸ“… æå–å¹´ä»½: {sorted(set(extracted_years))} â†’ æ‰©å±•èŒƒå›´: [{min_year-1}, {max_year+1}]")
    else:
        years = extracted_years
    
    return sorted(list(set(years)))  # å»é‡å¹¶æ’åº


def route_reports_by_time(
    company_name: str, 
    question: str, 
    all_reports: List[Dict],
    fallback_strategy: str = "all",  # "all" æˆ– "latest"
    selected_years: List[int] = None  # å¯é€‰ï¼šå‰ç«¯æŒ‡å®šçš„å¹´ä»½åˆ—è¡¨
) -> List[Dict]:
    """
    åŸºäºå…¬å¸åå’Œå¯é€‰å¹´ä»½ä¿¡æ¯è·¯ç”±åˆ°åˆé€‚çš„æ–‡æ¡£
    
    Args:
        company_name: å…¬å¸åç§°
        question: ç”¨æˆ·é—®é¢˜ï¼ˆä¸å†ç”¨äºæå–å¹´ä»½ï¼‰
        all_reports: æ‰€æœ‰å¯ç”¨çš„æŠ¥å‘Š
        fallback_strategy: å½“æ²¡æœ‰æŒ‡å®šå¹´ä»½æ—¶çš„å›é€€ç­–ç•¥
            - "all": è¿”å›è¯¥å…¬å¸æ‰€æœ‰æ–‡æ¡£ï¼ˆé»˜è®¤ï¼‰
            - "latest": åªè¿”å›æœ€æ–°å¹´ä»½çš„æ–‡æ¡£
        selected_years: å¯é€‰ï¼Œå‰ç«¯æŒ‡å®šçš„å¹´ä»½åˆ—è¡¨ã€‚å¦‚æœæä¾›ï¼Œåªè¿”å›è¿™äº›å¹´ä»½çš„æ–‡æ¡£
    
    Returns:
        List[Dict]: åŒ¹é…çš„æŠ¥å‘Šåˆ—è¡¨
    """
    # 1. å…ˆæŒ‰å…¬å¸åè¿‡æ»¤
    company_reports = []
    for report in all_reports:
        document = report.get("document", {})
        metainfo = document.get("metainfo", {})
        if metainfo.get("company_name") == company_name:
            company_reports.append(report)
    
    if not company_reports:
        return []
    
    # 2. å¦‚æœæŒ‡å®šäº†å¹´ä»½ï¼ŒæŒ‰å¹´ä»½è¿‡æ»¤
    if selected_years and len(selected_years) > 0:
        filtered_reports = []
        for report in company_reports:
            document = report.get("document", {})
            metainfo = document.get("metainfo", {})
            
            # ä¼˜å…ˆä» year å­—æ®µè·å–ï¼Œå¦åˆ™ä» sha1_name ä¸­æå–ï¼ˆå¦‚ "J2025" â†’ 2025ï¼‰
            report_year = metainfo.get("year")
            if report_year is None:
                sha1_name = metainfo.get("sha1_name", "")
                # ä» sha1_name ä¸­æå–å¹´ä»½ï¼šåŒ¹é… J20XX æˆ– 20XX æ ¼å¼
                year_match = re.search(r'[J]?(20\d{2})', sha1_name)
                if year_match:
                    report_year = int(year_match.group(1))
            
            # æ”¯æŒå­—ç¬¦ä¸²æˆ–æ•´æ•°æ ¼å¼çš„ year
            if report_year is not None:
                try:
                    report_year = int(report_year)
                    if report_year in selected_years:
                        filtered_reports.append(report)
                except (ValueError, TypeError):
                    pass
        
        if filtered_reports:
            print(f"[INFO] ğŸ¯ å¹´ä»½è¿‡æ»¤: é€‰æ‹©å¹´ä»½ {selected_years}ï¼ŒåŒ¹é…åˆ° {len(filtered_reports)} ä¸ªæ–‡æ¡£")
            return filtered_reports
        else:
            print(f"[WARNING] âš ï¸ æŒ‡å®šå¹´ä»½ {selected_years}ï¼Œä½†æœªæ‰¾åˆ°å¯¹åº”æ–‡æ¡£ï¼Œå›é€€åˆ°å…¨éƒ¨æ–‡æ¡£")
    
    # 3. æ²¡æœ‰æŒ‡å®šå¹´ä»½æ—¶çš„å›é€€ç­–ç•¥
    if fallback_strategy == "latest":
        # è¿”å›æœ€æ–°å¹´ä»½çš„æ–‡æ¡£
        latest_year = None
        latest_reports = []
        for report in company_reports:
            document = report.get("document", {})
            metainfo = document.get("metainfo", {})
            
            # ä¼˜å…ˆä» year å­—æ®µè·å–ï¼Œå¦åˆ™ä» sha1_name ä¸­æå–
            report_year = metainfo.get("year")
            if report_year is None:
                sha1_name = metainfo.get("sha1_name", "")
                year_match = re.search(r'[J]?(20\d{2})', sha1_name)
                if year_match:
                    report_year = int(year_match.group(1))
            
            if report_year is not None:
                try:
                    report_year = int(report_year)
                    if latest_year is None or report_year > latest_year:
                        latest_year = report_year
                        latest_reports = [report]
                    elif report_year == latest_year:
                        latest_reports.append(report)
                except (ValueError, TypeError):
                    pass
        
        if latest_reports:
            print(f"[INFO] ğŸ“… æ— æŒ‡å®šå¹´ä»½ï¼Œä½¿ç”¨æœ€æ–°å¹´ä»½ {latest_year} çš„æ–‡æ¡£")
            return latest_reports
    
    # 4. é»˜è®¤è¿”å›æ‰€æœ‰è¯¥å…¬å¸çš„æ–‡æ¡£ï¼ˆä¸å†æ ¹æ®é—®é¢˜ä¸­çš„å¹´ä»½è¿‡æ»¤ï¼‰
    print(f"[INFO] ğŸ“š ä½¿ç”¨è¯¥å…¬å¸æ‰€æœ‰ {len(company_reports)} ä¸ªæ–‡æ¡£ï¼ˆæ‰€æœ‰å¹´ä»½ï¼‰")
    return company_reports

class BM25Retriever:
    def __init__(self, bm25_db_dir: Path, documents_dir: Path, subset_path: Path = None):
        self.bm25_db_dir = bm25_db_dir
        self.documents_dir = documents_dir
        self.subset_path = subset_path
        self.year_lookup = self._load_year_lookup() if subset_path else {}
    
    def _load_year_lookup(self) -> dict:
        """ä» subset.csv åŠ è½½ sha1 -> year çš„æ˜ å°„"""
        import csv
        year_lookup = {}
        try:
            with open(self.subset_path, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    sha1 = row.get('sha1', '').strip()
                    year = row.get('year', '').strip()
                    if sha1 and year:
                        try:
                            year_lookup[sha1] = int(year)
                        except ValueError:
                            pass
            print(f"[INFO] ğŸ“… BM25: ä» subset.csv åŠ è½½äº† {len(year_lookup)} ä¸ªæ–‡æ¡£çš„å¹´ä»½ä¿¡æ¯")
        except Exception as e:
            print(f"[WARNING] âš ï¸ BM25: æ— æ³•åŠ è½½ subset.csv å¹´ä»½ä¿¡æ¯: {e}")
        return year_lookup
        
    def retrieve_by_company_name(self, company_name: str, query: str, top_n: int = 3, return_parent_pages: bool = False, selected_years: List[int] = None) -> List[Dict]:
        print("BM25Retriever retrieve_by_company_name is called")
        
        # ğŸ¯ å…ˆæ”¶é›†æ‰€æœ‰æ–‡æ¡£ï¼Œç„¶åä½¿ç”¨è·¯ç”±å‡½æ•°
        all_documents = []
        for path in self.documents_dir.glob("*.json"):
            with open(path, 'r', encoding='utf-8') as f:
                doc = json.load(f)
                all_documents.append({
                    "path": path,
                    "document": doc,
                    "sha1": doc["metainfo"]["sha1_name"]
                })
        
        # è½¬æ¢ä¸º route_reports_by_time éœ€è¦çš„æ ¼å¼
        all_reports = []
        for doc_info in all_documents:
            all_reports.append({
                "document": doc_info["document"],
                "name": doc_info["sha1"]
            })
        
        # ä½¿ç”¨è·¯ç”±å‡½æ•°è¿‡æ»¤æ–‡æ¡£ï¼ˆé»˜è®¤åœ¨æ‰€æœ‰å¹´ä»½ä¸­æ£€ç´¢ï¼Œé™¤éæŒ‡å®šäº† selected_yearsï¼‰
        matching_reports = route_reports_by_time(
            company_name=company_name,
            question=query,
            all_reports=all_reports,
            fallback_strategy="all",
            selected_years=selected_years
        )
        
        # è½¬æ¢å›åŸæ¥çš„æ ¼å¼
        matching_documents = []
        matching_sha1s = {rep["name"] for rep in matching_reports}
        for doc_info in all_documents:
            if doc_info["sha1"] in matching_sha1s:
                matching_documents.append(doc_info)
        
        if not matching_documents:
            raise ValueError(f"No report found with '{company_name}' company name.")
        
        if selected_years and len(selected_years) > 0:
            print(f"[INFO] ğŸ¯ BM25å¹´ä»½è¿‡æ»¤: é€‰æ‹©å¹´ä»½ {selected_years}ï¼ŒåŒ¹é…åˆ° {len(matching_documents)} ä¸ªæ–‡æ¡£")
        elif len(matching_documents) > 1:
            print(f"[INFO] Found {len(matching_documents)} reports for '{company_name}', retrieving from all")
            
        # Retrieve from all matching documents and aggregate results
        all_retrieval_results = []
        
        for doc_info in matching_documents:
            document = doc_info["document"]
            sha1 = doc_info["sha1"]
            
            # Load corresponding BM25 index
            bm25_path = self.bm25_db_dir / f"{sha1}.pkl"
            if not bm25_path.exists():
                print(f"[WARNING] BM25 index not found for {sha1}, skipping")
                continue
                
            with open(bm25_path, 'rb') as f:
                bm25_index = pickle.load(f)
                
            # Get the document content and BM25 index
            chunks = document["content"]["chunks"]
            pages = document["content"]["pages"]
            
            # Get BM25 scores for the query
            tokenized_query = query.split()
            scores = bm25_index.get_scores(tokenized_query)
            
            actual_top_n = min(top_n, len(scores))
            top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:actual_top_n]
            
            seen_pages = set()
            
            for index in top_indices:
                score = round(float(scores[index]), 4)
                chunk = chunks[index]
                parent_page = next(page for page in pages if page["page"] == chunk["page"])
                
                if return_parent_pages:
                    if parent_page["page"] not in seen_pages:
                        seen_pages.add(parent_page["page"])
                        result = {
                            "distance": score,
                            "page": parent_page["page"],
                            "text": parent_page["text"],
                            "source_sha1": sha1  # Add source document identifier
                        }
                        all_retrieval_results.append(result)
                else:
                    result = {
                        "distance": score,
                        "page": chunk["page"],
                        "text": chunk["text"],
                        "source_sha1": sha1  # Add source document identifier
                    }
                    all_retrieval_results.append(result)
        
        # Sort by score and return top_n results across all documents
        all_retrieval_results.sort(key=lambda x: x["distance"], reverse=True)
        return all_retrieval_results[:top_n]

class HybridRetriever:
    def __init__(
        self,
        vector_db_dir: Path,
        documents_dir: Path,
        use_hyde: bool = True,
        use_multi_query: bool = True,
        subset_path: Path = None,
        parallel_workers: int = 4,
        multi_query_methods: Optional[Dict[str, bool]] = None,
    ):
        self.vector_retriever = VectorRetriever(
            vector_db_dir,
            documents_dir,
            use_hyde=use_hyde,
            use_multi_query=use_multi_query,
            subset_path=subset_path,
            parallel_workers=parallel_workers,
            multi_query_methods=multi_query_methods,
        )
        self.reranker = LLMReranker()
        
    def retrieve_by_company_name(
        self, 
        company_name: str, 
        query: str, 
        llm_reranking_sample_size: int = 28,
        documents_batch_size: int = 2,
        top_n: int = 6,
        llm_weight: float = 0.7,
        return_parent_pages: bool = False,
        use_hyde: bool = None,
        use_multi_query: bool = None,
        progress_callback=None,
        selected_years: List[int] = None,
        multi_query_config: Optional[Dict[str, bool]] = None
    ) -> List[Dict]:
        """
        Retrieve and rerank documents using hybrid approach.
        
        Args:
            company_name: Name of the company to search documents for
            query: Search query
            llm_reranking_sample_size: Number of initial results to retrieve from vector DB
            documents_batch_size: Number of documents to analyze in one LLM prompt
            top_n: Number of final results to return after reranking
            llm_weight: Weight given to LLM scores (0-1)
            return_parent_pages: Whether to return full pages instead of chunks
            selected_years: Optional list of years to filter documents
            
        Returns:
            List of reranked document dictionaries with scores
        """
        import time
        
        timing_info = {
            'hyde_expansion': 0.0,
            'multi_query_expansion': 0.0,
            'vector_search': 0.0,
            'llm_reranking': 0.0
        }
        
        # Get initial results from vector retriever
        vector_retrieval_result = self.vector_retriever.retrieve_by_company_name(
            company_name=company_name,
            query=query,
            top_n=llm_reranking_sample_size,
            return_parent_pages=return_parent_pages,
            use_hyde=use_hyde,
            use_multi_query=use_multi_query,
            progress_callback=progress_callback,
            selected_years=selected_years,
            multi_query_config=multi_query_config
        )
        
        # å¤„ç†è¿”å›ç»“æœï¼ˆå¯èƒ½æ˜¯dictæˆ–listï¼‰
        expansion_texts = {}
        if isinstance(vector_retrieval_result, dict) and 'timing' in vector_retrieval_result:
            timing_info.update(vector_retrieval_result['timing'])
            vector_results = vector_retrieval_result['results']
            # æå–æ‰©å±•æ–‡æœ¬ä¿¡æ¯
            if 'expansion_texts' in vector_retrieval_result:
                expansion_texts = vector_retrieval_result['expansion_texts']
        else:
            vector_results = vector_retrieval_result
        
        print(f"[DEBUG] Initial vector results count: {len(vector_results)}")

        # é‡æ’åºé˜¶æ®µï¼ˆè¿™æ˜¯æœ€è€—æ—¶çš„éƒ¨åˆ†ï¼‰
        if progress_callback:
            progress_callback("ğŸ¯ LLM é‡æ’åºä¸­ï¼ˆè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰...", 58)
        
        # Rerank results using LLM
        rerank_start = time.time()
        reranked_results = self.reranker.rerank_documents(
            query=query,
            documents=vector_results,
            documents_batch_size=documents_batch_size,
            llm_weight=llm_weight
        )
        timing_info['llm_reranking'] = time.time() - rerank_start

        print(f"[DEBUG] Reranked results count: {len(reranked_results)}")
        #print("[DEBUG] HybridRetriever retrieve_by_company_name is called")
        print(f"[DEBUG] Final top_n: {top_n}")
        
        final_results = reranked_results[:top_n]
        
        # è¿”å›ç»“æœã€æ—¶é—´ä¿¡æ¯å’Œæ‰©å±•æ–‡æœ¬
        return {
            'results': final_results,
            'timing': timing_info,
            'expansion_texts': expansion_texts,
            'reranker_stats': self.reranker.get_stats()
        }

class VectorRetriever:
    def __init__(
        self,
        vector_db_dir: Path,
        documents_dir: Path,
        use_hyde: bool = True,
        use_multi_query: bool = True,
        subset_path: Path = None,
        parallel_workers: int = 4,
        multi_query_methods: Optional[Dict[str, bool]] = None,
    ):
        self.vector_db_dir = vector_db_dir
        self.documents_dir = documents_dir
        self.subset_path = subset_path
        self.year_lookup = self._load_year_lookup() if subset_path else {}
        self.all_dbs = self._load_dbs()
        self.qwen = BaseQwenProcessor()
        self.use_hyde = use_hyde
        self.use_multi_query = use_multi_query
        self.parallel_workers = max(1, parallel_workers)
        self.multi_query_methods = multi_query_methods or {
            'synonym': True,
            'subquestion': True,
            'variant': True
        }
        #print(f"[DEBUG][VectorRetriever.__init__] use_hyde={self.use_hyde}, use_multi_query={self.use_multi_query}")
    
    def _load_year_lookup(self) -> dict:
        """ä» subset.csv åŠ è½½ sha1 -> year çš„æ˜ å°„"""
        import csv
        year_lookup = {}
        try:
            with open(self.subset_path, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    sha1 = row.get('sha1', '').strip()
                    year = row.get('year', '').strip()
                    if sha1 and year:
                        try:
                            year_lookup[sha1] = int(year)
                        except ValueError:
                            pass
            print(f"[INFO] ğŸ“… ä» subset.csv åŠ è½½äº† {len(year_lookup)} ä¸ªæ–‡æ¡£çš„å¹´ä»½ä¿¡æ¯")
        except Exception as e:
            print(f"[WARNING] âš ï¸ æ— æ³•åŠ è½½ subset.csv å¹´ä»½ä¿¡æ¯: {e}")
        return year_lookup

    # Qwen embedding ä¸éœ€è¦ set_up_llm
    
    # Qwen embedding ä¸éœ€è¦ set_up_llm

    def _load_dbs(self):
        all_dbs = []
        company_names = []  # ç”¨äºæ”¶é›†company_name
        # Get list of JSON document paths
        all_documents_paths = list(self.documents_dir.glob('*.json'))
        vector_db_files = {db_path.stem: db_path for db_path in self.vector_db_dir.glob('*.faiss')}

        for document_path in all_documents_paths:
            #print(f"[DEBUG] Loading document: {document_path.name}")
            stem = document_path.stem
            if stem not in vector_db_files:
                _log.warning(f"No matching vector DB found for document {document_path.name}")
                continue
            try:
                with open(document_path, 'r', encoding='utf-8') as f:
                    document = json.load(f)
            except Exception as e:
                _log.error(f"Error loading JSON from {document_path.name}: {e}")
                continue

            # Validate that the document meets the expected schema
            if not (isinstance(document, dict) and "metainfo" in document and "content" in document):
                _log.warning(f"Skipping {document_path.name}: does not match the expected schema.")
                continue

            # æ”¶é›†company_name
            company_name = document.get("metainfo", {}).get("company_name", None)
            if company_name:
                company_names.append(company_name)
            
            # ğŸ†• ä» year_lookup æ³¨å…¥ year ä¿¡æ¯åˆ° metainfo
            sha1_name = document.get("metainfo", {}).get("sha1_name", stem)
            if sha1_name in self.year_lookup:
                document["metainfo"]["year"] = self.year_lookup[sha1_name]

            try:
                vector_db = faiss.read_index(str(vector_db_files[stem]))
            except Exception as e:
                _log.error(f"Error reading vector DB for {document_path.name}: {e}")
                continue

            report = {
                "name": stem,
                "vector_db": vector_db,
                "document": document
            }
            all_dbs.append(report)

        # print("[DEBUG] å½“å‰å¯ç”¨çš„company_nameæœ‰:")
        # for name in company_names:
        #     print(f"  - {name}")
        # print("[DEBUG] å½“å‰å¯ç”¨çš„company_nameä»¥ä¸Š")

        return all_dbs

    @staticmethod
    def get_strings_cosine_similarity(str1, str2):
        qwen = BaseQwenProcessor()
        emb1 = qwen.get_embeddings([str1])["embeddings"][0]
        emb2 = qwen.get_embeddings([str2])["embeddings"][0]
        similarity_score = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
        similarity_score = round(similarity_score, 4)
        return similarity_score

    
   
    def _safe_flush(self):
        """å®‰å…¨åœ°åˆ·æ–°æ ‡å‡†è¾“å‡ºï¼Œå¿½ç•¥ BrokenPipeError"""
        import sys
        try:
            sys.stdout.flush()
        except (BrokenPipeError, OSError):
            pass  # å¿½ç•¥ BrokenPipeErrorï¼Œåœ¨ Streamlit ç¯å¢ƒä¸­å¯èƒ½å‘ç”Ÿ
    
    def _safe_print(self, *args, **kwargs):
        """å®‰å…¨åœ°æ‰“å°ï¼Œå¿½ç•¥ BrokenPipeError"""
        try:
            print(*args, **kwargs)
            self._safe_flush()
        except (BrokenPipeError, OSError):
            pass  # å¿½ç•¥ BrokenPipeErrorï¼Œåœ¨ Streamlit ç¯å¢ƒä¸­å¯èƒ½å‘ç”Ÿ
    
    def retrieve_by_company_name(self, company_name: str, query: str, llm_reranking_sample_size: int = None, top_n: int = 3, return_parent_pages: bool = False, use_hyde: bool = None, use_multi_query: bool = None, progress_callback=None, selected_years: List[int] = None, multi_query_config: Optional[Dict[str, bool]] = None) -> List[Tuple[str, float]]:
        import sys
        import time
        
        # åˆå§‹åŒ–æ—¶é—´ç»Ÿè®¡å’Œæ‰©å±•æ–‡æœ¬ä¿¡æ¯
        timing_info = {
            'hyde_expansion': 0.0,
            'multi_query_expansion': 0.0,
            'embedding_generation': 0.0,
            'vector_search': 0.0
        }
        
        # ä¿å­˜æ‰©å±•ç”Ÿæˆçš„æ–‡æœ¬
        expansion_texts = {
            'hyde_text': None,
            'multi_query_texts': [],
            'glossary_context': None,
            'multi_query_methods': {}
        }
        
        self._safe_print("[DEBUG] VectorRetriever retrieve_by_company_name is called")

        # ğŸ¯ ä½¿ç”¨è·¯ç”±å‡½æ•°å®šä½æ–‡æ¡£ï¼ˆé»˜è®¤åœ¨æ‰€æœ‰å¹´ä»½ä¸­æ£€ç´¢ï¼Œé™¤éæŒ‡å®šäº† selected_yearsï¼‰
        if progress_callback:
            progress_callback("ğŸ“š å®šä½ç›¸å…³æ–‡æ¡£ä¸­...", 28)
        
        matching_reports = route_reports_by_time(
            company_name=company_name,
            question=query,
            all_reports=self.all_dbs,
            fallback_strategy="all",  # æ— æŒ‡å®šå¹´ä»½æ—¶ä½¿ç”¨æ‰€æœ‰æ–‡æ¡£
            selected_years=selected_years  # å‰ç«¯æŒ‡å®šçš„å¹´ä»½åˆ—è¡¨
        )
        
        if not matching_reports:
            _log.error(f"No report found with '{company_name}' company name.")
            raise ValueError(f"No report found with '{company_name}' company name.")
        
        if len(matching_reports) > 1:
            self._safe_print(f"[INFO] Found {len(matching_reports)} reports for '{company_name}', retrieving from all")
            for rep in matching_reports:
                doc = rep.get("document", {})
                metainfo = doc.get("metainfo", {})
                year = metainfo.get("year", "unknown")
                self._safe_print(f"  - Report: {rep['name']} (Year: {year})")
        
        # Priority parameters
        use_hyde = self.use_hyde if use_hyde is None else use_hyde
        use_multi_query = self.use_multi_query if use_multi_query is None else use_multi_query
        multi_query_config = multi_query_config or self.multi_query_methods or {}
        expansion_texts['multi_query_methods'] = multi_query_config
        self._safe_print(f"[DEBUG] multi_query_config = {multi_query_config}")
        if use_multi_query and not any(multi_query_config.values()):
            self._safe_print("[INFO] Multi-Query enabled but no methods selected; skipping expansion.")
            use_multi_query = False
        self._safe_print(f"[DEBUG][retrieve_by_company_name] use_hyde={use_hyde}, use_multi_query={use_multi_query}")
        
        qwen = BaseQwenProcessor()
        # æ§åˆ¶multi_queryå’Œhydeæ‰©å……
        queries = [query]

        if use_hyde:
            if progress_callback:
                progress_callback("ğŸ”® HYDE æŸ¥è¯¢æ‰©å±•ä¸­...", 32)
            self._safe_print(f"[DEBUG] å¼€å§‹ HYDE æ‰©å±•...")
            hyde_start = time.time()
            try:
                self._safe_print(f"[DEBUG] è°ƒç”¨ Qwen API ç”Ÿæˆå‡è®¾ç­”æ¡ˆ...")
                fake_answer = qwen.send_message(
                    model="qwen-turbo",
                    system_content=(
                        "You are a financial report analyst. Your task is to generate a hypothetical markdown table "
                        "that could plausibly appear in a company's annual report or financial statement to answer the given query. "
                        "\n\n"
                        "**Requirements:**\n"
                        "1. Generate a markdown-format table (using | and - for formatting)\n"
                        "2. The table should be relevant to the question and contain typical fields/columns that would appear in such a table\n"
                        "3. Include appropriate table headers (such as: ç±»å‹, é¡¹ç›®, é‡‘é¢, å•ä½, å¤‡æ³¨, å¹´ä»½, å­£åº¦, æ¯”ä¾‹, etc.)\n"
                        "4. Add a unit specification if applicable (e.g., 'å•ä½ï¼šä¸‡å…ƒ' or 'å•ä½ï¼šå…ƒ')\n"
                        "5. Include sample data rows that demonstrate the table structure\n"
                        "6. The table structure should match what would typically appear in Chinese financial reports\n"
                        "\n"
                        "**Table Format Example:**\n"
                        "```\n"
                        "å•ä½ï¼šä¸‡å…ƒ\n\n"
                        "| ç±»å‹ | é¡¹ç›® | é‡‘é¢ | å¤‡æ³¨ |\n"
                        "|------|------|------|------|\n"
                        "| ...  | ...  | ...  | ...  |\n"
                        "```\n"
                        "\n"
                        "**Important:**\n"
                        "- Focus on creating a realistic table structure, not accurate data\n"
                        "- The table should help retrieve similar tables from financial reports\n"
                        "- Use Chinese column names appropriate for financial statements\n"
                        "- Include calculation formulas or notes if relevant (e.g., 'â‘  â‘¡ â‘¢ = +' or 'â‘¥ â‘  â‘£ â‘¤ = - -')"
                    ),
                    human_content=f"Generate a markdown-format table that could appear in a company's financial report to answer this question: {query}\n\n"
                                 f"The table should include:\n"
                                 f"- Appropriate unit specification (if applicable)\n"
                                 f"- Relevant column headers based on the question\n"
                                 f"- Sample data rows showing the table structure\n"
                                 f"- Any relevant notes or calculation formulas",
                    is_structured=False
                )
                if isinstance(fake_answer, list):
                    fake_answer_str = ''.join(fake_answer)
                else:
                    fake_answer_str = str(fake_answer)
                queries.append(fake_answer_str)
                expansion_texts['hyde_text'] = fake_answer_str  # ä¿å­˜HYDEç”Ÿæˆçš„æ–‡æœ¬
                self._safe_print(f"[DEBUG] HYDE æ‰©å±•æˆåŠŸï¼Œç”Ÿæˆå‡è®¾ç­”æ¡ˆé•¿åº¦: {len(fake_answer_str)}")
            except Exception as e:
                self._safe_print(f"[ERROR] HYDE expansion failed: {e}")
            timing_info['hyde_expansion'] = time.time() - hyde_start

        if use_multi_query:
            if progress_callback:
                progress_callback("ğŸ”„ Multi-Query æŸ¥è¯¢æ‰©å±•ä¸­...", 38)
            self._safe_print(f"[DEBUG] å¼€å§‹ Multi-Query æ‰©å±•...")
            multi_query_start = time.time()
            matched_concepts = find_financial_concepts(query, limit=5)
            concept_terms = [concept["term"] for concept in matched_concepts]
            concept_context_text = format_concepts_for_prompt(matched_concepts)
            glossary_instruction = (
                "Financial glossary context.\n"
                "Foræ¯ä¸ªå‘½ä¸­çš„æœ¯è¯­ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼é€æ¡è¿½åŠ è§£é‡Šï¼š\n"
                "1) Termå + ä¸»è¦åˆ«å/è¿‘ä¹‰è¯\n"
                "2) å®šä¹‰ï¼ˆè‡³å°‘ä¸€å¥ï¼‰\n"
                "3) è®¡ç®—æ–¹æ³•/å…¸å‹å•ä½/æ•°æ®æ¥æºï¼ˆè‹¥é€‚ç”¨ï¼‰\n"
                "æ ¼å¼ç¤ºä¾‹ï¼š\n"
                "\"æ¯›åˆ©ç‡\n"
                "- åˆ«åï¼šç»¼åˆæ¯›åˆ©ç‡\n"
                "- å®šä¹‰ï¼šä½“ç°äº§å“ç›ˆåˆ©ç©ºé—´çš„æ¯”ä¾‹â€¦â€¦\n"
                "- è®¡ç®—æ–¹å¼ï¼šæ¯›åˆ©ç‡ = (è¥ä¸šæ”¶å…¥ - è¥ä¸šæˆæœ¬) Ã· è¥ä¸šæ”¶å…¥\"\n"
                "åœ¨ç”Ÿæˆæ–°çš„æŸ¥è¯¢æ—¶ï¼Œå°†ä¸Šè¿°è§£é‡Šé™„åŠ åœ¨åŸé—®é¢˜åæ–¹çš„ç‹¬ç«‹æ®µè½ä¸­ï¼Œè€Œä¸æ˜¯å†™åœ¨æ‹¬å·é‡Œã€‚\n"
                f"{concept_context_text}"
            )
            expansion_texts['glossary_context'] = concept_context_text
            expansion_texts['multi_query_methods'] = multi_query_config
            method_definitions = [
                (
                    1,
                    'synonym',
                    "ä½ çš„ä»»åŠ¡æ˜¯ä¸ºé—®é¢˜ä¸­çš„è´¢åŠ¡ä¸“ä¸šåè¯è¡¥å……è¯¦ç»†è§£é‡Šã€‚"
                    "ä¸Šé¢å·²æä¾›äº†è´¢åŠ¡æœ¯è¯­è¯æ±‡è¡¨(Financial glossary)ï¼ŒåŒ…å«æ¯ä¸ªæœ¯è¯­çš„åˆ«åã€å®šä¹‰å’Œè®¡ç®—æ–¹å¼ã€‚"
                    "ä»»åŠ¡è¦æ±‚ï¼šè¯†åˆ«é—®é¢˜ä¸­åŒ…å«çš„è´¢åŠ¡æœ¯è¯­ï¼Œå‚è€ƒ glossary ä¸­çš„ä¿¡æ¯ï¼Œåœ¨åŸé—®é¢˜ä¹‹åå•ç‹¬åˆ—å‡ºæ¯ä¸ªæœ¯è¯­çš„å®šä¹‰ã€è¿‘ä¹‰è¯ã€è®¡ç®—æ–¹æ³•ã€‚"
                    "æ ¼å¼ï¼š<åŸé—®é¢˜ åè¯è§£é‡Šï¼šæœ¯è¯­åç§° å®šä¹‰...è¿‘ä¹‰è¯...è®¡ç®—æ–¹æ³•...>"
                    "ç¤ºä¾‹ï¼šé‡‘ç›˜ç§‘æŠ€2024å¹´çš„æ¯›åˆ©ç‡æ˜¯å¤šå°‘ -> "
                    "<é‡‘ç›˜ç§‘æŠ€2024å¹´çš„æ¯›åˆ©ç‡æ˜¯å¤šå°‘ åè¯è§£é‡Šï¼šæ¯›åˆ©ç‡ å®šä¹‰ï¼šæ¯›åˆ©ä¸è¥ä¸šæ”¶å…¥ä¹‹æ¯”ï¼Œåæ˜ äº§å“æˆ–ä¸šåŠ¡çš„ç›ˆåˆ©ç©ºé—´ è¿‘ä¹‰è¯ï¼šç»¼åˆæ¯›åˆ©ç‡ è®¡ç®—æ–¹æ³•ï¼šæ¯›åˆ©ç‡=(è¥ä¸šæ”¶å…¥-è¥ä¸šæˆæœ¬)/è¥ä¸šæ”¶å…¥>"
                    "ä¼˜å…ˆä½¿ç”¨ glossary ä¸­æä¾›çš„å®šä¹‰ã€è¿‘ä¹‰è¯å’Œè®¡ç®—æ–¹å¼ã€‚å¦‚æœé—®é¢˜æ¶‰åŠè´¢åŠ¡æœ¯è¯­ä½† glossary ä¸­æ²¡æœ‰ï¼Œå¯ä»¥ç”¨ä½ è‡ªå·±çš„çŸ¥è¯†è¡¥å……ã€‚"
                    "åªæœ‰åœ¨é—®é¢˜å®Œå…¨ä¸æ¶‰åŠä»»ä½•è´¢åŠ¡æœ¯è¯­æ—¶ï¼Œæ‰è¿”å› <SKIP>ã€‚å¯ç”Ÿæˆ1-2ä¸ªå¸¦åè¯è§£é‡Šçš„æŸ¥è¯¢ï¼Œæ¯ä¸ªç”¨å°–æ‹¬å·åŒ…è£¹ã€‚"
                ),
                (
                    2,
                    'subquestion',
                    "æ ¹æ®è´¢åŠ¡æŒ‡æ ‡å°†é—®é¢˜æ‹†åˆ†ä¸º0-Nä¸ªç²’åº¦æ›´ç»†çš„å­é—®é¢˜ã€‚"
                    "æ¯ä¸ªå­é—®é¢˜ä¸“æ³¨äºå•ä¸€æŒ‡æ ‡/æ—¶é—´æ®µ/ä¸šåŠ¡æ¿å—ï¼Œå¹¶ç»“åˆ glossary é‡Œçš„æœ¯è¯­æˆ–å•ä½ã€‚"
                    "è‹¥æ²¡æœ‰åˆé€‚çš„æ‹†åˆ†åˆ™è¿”å› <SKIP>ï¼›å¦åˆ™æ¯ä¸ªå­é—®é¢˜å•ç‹¬ç”¨ <> åŒ…è£¹ã€‚"
                ),
                (
                    3,
                    'variant',
                    "ä»…å½“åŸé—®é¢˜åå¼€æ”¾æˆ–ä¿¡æ¯ä¸è¶³æ—¶ï¼Œç”Ÿæˆæƒ…æ™¯åŒ–/å˜ä½“æé—®ï¼Œæ¢ç´¢ä¸åŒè§’åº¦ï¼ˆå¦‚ç›ˆåˆ©è´¨é‡ã€ç°é‡‘å®‰å…¨å«ã€æµ·å¤–æ‰©å¼ ã€è¡¥è´´æŒç»­æ€§ç­‰ï¼‰ã€‚"
                    "è‹¥é—®é¢˜æœ¬å°±æ˜ç¡®ï¼Œåˆ™è¾“å‡º <SKIP>ï¼›è‹¥éœ€è¦æ”¹å†™ï¼Œå¯ç”Ÿæˆ1-2ä¸ªæŸ¥è¯¢ï¼Œæ¯ä¸ªç”¨ <> åŒ…è£¹ï¼Œå¹¶ä¿æŒä¸»ä½“ä¸ºé‡‘ç›˜ç§‘æŠ€ã€‚"
                )
            ]
            import re
            for method_id, method_key, prompt in method_definitions:
                if not multi_query_config.get(method_key, False):
                    continue
                self._safe_print(f"[DEBUG] Multi-Query æ–¹æ³• {method_id}...")
                try:
                    self._safe_print(f"[DEBUG] è°ƒç”¨ Qwen API æ‰©å±•æŸ¥è¯¢...")
                    response = qwen.send_message(
                        model="qwen-turbo",
                        system_content=(
                            "You are assisting in an Enterprise RAG Challenge focused on annual reports. "
                            "Always maintain financial rigor and keep the company name unchanged."
                        ),
                        human_content=(
                            f"{prompt}\n\n"
                            f"{glossary_instruction}\n\n"
                            f"Original question: {query}"
                        ),
                        is_structured=False
                    )
                    extracted_queries = re.findall(r"<(.*?)>", response, flags=re.DOTALL)
                    self._safe_print(f"[DEBUG] åŸå§‹å“åº”: {response[:200]}...")
                    self._safe_print(f"[DEBUG] æå–çš„æŸ¥è¯¢: {extracted_queries}")
                    for q in extracted_queries:
                        q_stripped = q.strip()
                        self._safe_print(f"[DEBUG] å¤„ç†æŸ¥è¯¢: '{q_stripped[:50]}...' (SKIP={q_stripped.upper() == 'SKIP'})")
                        if not q_stripped or q_stripped.upper() == "SKIP":
                            continue
                        queries.append(q_stripped)
                        expansion_texts['multi_query_texts'].append({
                            'method_id': method_id,
                            'query': q_stripped,
                            'concepts': concept_terms
                        })
                    self._safe_print(f"[DEBUG] Multi-Query æ–¹æ³• {method_id} å®Œæˆï¼Œæå–äº† {len(extracted_queries)} ä¸ªæŸ¥è¯¢ï¼Œå®é™…æ·»åŠ äº† {len([q for q in extracted_queries if q.strip() and q.strip().upper() != 'SKIP'])} ä¸ª")
                except Exception as e:
                    self._safe_print(f"Expansion method {method_id} failed: {e}")
            timing_info['multi_query_expansion'] = time.time() - multi_query_start
        
        # å»é‡å¹¶æ¸…æ´—æŸ¥è¯¢ï¼Œé¿å…é‡å¤ embedding è®¡ç®—
        deduped_queries = []
        seen_queries = set()
        for q in queries:
            normalized_q = q.strip()
            if not normalized_q or normalized_q in seen_queries:
                continue
            deduped_queries.append(normalized_q)
            seen_queries.add(normalized_q)
        queries = deduped_queries

        inner_factor = 1.0
        self._safe_print("[DEBUG] queries is", queries)
        self._safe_print("[DEBUG] queries's length is", len(queries))

        # é¢„å…ˆç”Ÿæˆ embeddingsï¼Œé¿å…åœ¨ä¸åŒæ–‡æ¡£ä¹‹é—´é‡å¤è¯·æ±‚
        query_embeddings = {}
        embedding_start = time.time()
        for q in queries:
            try:
                emb_result = self.qwen.get_embeddings([q])
                if (
                    not emb_result
                    or not isinstance(emb_result, list)
                    or not emb_result[0]
                    or 'embedding' not in emb_result[0]
                ):
                    self._safe_print(f"[ERROR] embedding result is empty or invalid for query: {q[:80]}")
                    continue
                embedding = emb_result[0]['embedding']
                query_embeddings[q] = np.array(embedding, dtype=np.float32).reshape(1, -1)
            except Exception as e:
                self._safe_print(f"[ERROR] Failed to get embedding for query snippet '{q[:50]}': {e}")
        timing_info['embedding_generation'] = time.time() - embedding_start

        if not query_embeddings:
            raise ValueError("Failed to generate embeddings for all queries.")

        # å‘½ä¸­ç»“æœå­˜å‚¨ï¼ˆç”¨å­—å…¸èšåˆï¼‰
        # key = (sha1, page_id or chunk_id), value = dict with distances, count, text
        aggregated_results = {}
        aggregation_lock = Lock()

        # ğŸ¯ æ–°æ£€ç´¢ç­–ç•¥ï¼šæ¯ä¸ªæ–‡æ¡£å‡å¬å› top_n ä¸ªchunksï¼Œç„¶åç»Ÿä¸€æŒ‰å‘é‡ç›¸ä¼¼åº¦æ’åº
        # æ”¶é›†æ‰€æœ‰æ–‡æ¡£çš„æ£€ç´¢ç»“æœï¼ˆæ€»å…± num_reports * top_n ä¸ªç»“æœï¼‰ï¼Œ
        # ç»Ÿä¸€æŒ‰å‘é‡ç›¸ä¼¼åº¦ï¼ˆåŠ æƒåçš„distanceï¼‰æ’åºï¼Œæˆªæ–­å¼é€‰å–å‰ top_n ä¸ªç»“æœ
        num_reports = len(matching_reports)
        
        self._safe_print(f"[INFO] ğŸ“Š æ£€ç´¢ç­–ç•¥: {num_reports}ä¸ªæ–‡æ¡£, æ¯ä¸ªæ–‡æ¡£æ£€ç´¢ {top_n} ä¸ªchunks (æ€»è®¡æœ€å¤š {num_reports * top_n} ä¸ªç»“æœ)")

        # å‘é‡æ£€ç´¢é˜¶æ®µ
        if progress_callback:
            progress_callback("ğŸ” å‘é‡æ£€ç´¢ä¸­...", 45)

        def process_query_for_document(report, query_text, embedding_array):
            local_hits = []
            try:
                document = report["document"]
                vector_db = report["vector_db"]
                chunks = document["content"]["chunks"]
                pages = document["content"]["pages"]
                sha1 = document["metainfo"]["sha1_name"]
                actual_top_n = min(top_n, len(chunks))
                if actual_top_n == 0:
                    return []
                distances, indices = vector_db.search(x=embedding_array, k=actual_top_n)
            
                for distance, index in zip(distances[0], indices[0]):
                    distance = round(float(distance)*inner_factor, 4)
                    chunk = chunks[index]
                    parent_page = next(page for page in pages if page["page"] == chunk["page"])
                    
                    if return_parent_pages:
                        # Include sha1 in key to differentiate same page numbers across different reports
                        key = (sha1, "page", parent_page["page"])
                        text = parent_page["text"]
                        page_id = parent_page["page"]
                    else:
                        key = (sha1, "chunk", index)
                        text = chunk["text"]
                        page_id = chunk["page"]
                    
                    local_hits.append((key, page_id, text, distance, sha1))
            except Exception as e:
                self._safe_print(f"[ERROR] Vector search failed for query '{query_text[:60]}' in report {report.get('name')}: {e}")
            return local_hits

        total_tasks = len(query_embeddings) * num_reports
        max_workers = min(self.parallel_workers, total_tasks) if total_tasks > 0 else 1
        vector_search_start = time.time()

        with ThreadPoolExecutor(max_workers=max(1, max_workers)) as executor:
            futures = []
            for report in matching_reports:
                for q_text, embedding_array in query_embeddings.items():
                    futures.append(executor.submit(process_query_for_document, report, q_text, embedding_array))

            for future in as_completed(futures):
                doc_hits = future.result()
                if not doc_hits:
                    continue
                with aggregation_lock:
                    for key, page_id, text, distance, sha1 in doc_hits:
                        if key not in aggregated_results:
                            aggregated_results[key] = {
                                "page": page_id,
                                "text": text,
                                "distances": [distance],
                                "count": 1,
                                "source_sha1": sha1  # Track source document
                            }
                        else:
                            aggregated_results[key]["distances"].append(distance)
                            aggregated_results[key]["count"] += 1

        timing_info['vector_search'] = time.time() - vector_search_start
    
        # åŠ æƒè§„åˆ™: 1æ¬¡=Ã—1.0, 2æ¬¡=Ã—1.2, 3æ¬¡=Ã—1.4ã€‚. ä»¥æ­¤ç±»æ¨ã€‚ æ³¨æ„ï¼šå½“å‰ faiss ç”¨çš„æ˜¯ IndexFlatIPï¼ˆå†…ç§¯ï¼‰ï¼Œdistance è¶Šå¤§è¡¨ç¤ºç›¸å…³æ€§è¶Šé«˜ã€‚å› æ­¤ï¼Œå‘½ä¸­å¤šæ¬¡æ—¶ï¼Œåº”è¯¥è®© distance å¢å¤§ã€‚
        def weight_factor(count: int) -> float:
            return 1.0 + 0.2 * (count - 1)
    
        final_results = []
        for key, info in aggregated_results.items():
            base_distance = max(info["distances"])  # å–æœ€å¤§è·ç¦»ä½œä¸ºåŸºå‡†
            factor = weight_factor(info["count"])
            weighted_distance = round(base_distance * factor, 4)
        
            final_results.append({
                "distance": weighted_distance,
                "page": info["page"],
                "text": info["text"],
                "hit_count": info["count"],  # æ–¹ä¾¿è°ƒè¯•çœ‹åˆ°è¢«å‘½ä¸­æ¬¡æ•°
                "source_sha1": info["source_sha1"]  # Include source document
            })
    
        # èšåˆï¼šæŒ‰åŠ æƒåçš„è·ç¦»é™åºï¼Œå–å‰ top_nï¼ˆdistanceè¶Šå¤§è¶Šç›¸å…³ï¼‰
        final_results = sorted(final_results, key=lambda x: x["distance"], reverse=True)
        
        # Debug: æ˜¾ç¤ºèšåˆåçš„æ–‡æ¡£åˆ†å¸ƒ
        source_distribution = {}
        for res in final_results[:top_n]:
            source = res.get("source_sha1", "Unknown")
            source_distribution[source] = source_distribution.get(source, 0) + 1
        print(f"[DEBUG] Top {top_n} results distribution: {source_distribution}")
        
        final_results = final_results[:top_n]


        # æ£€ç´¢å®Œæˆ
        if progress_callback:
            progress_callback("âœ… æ£€ç´¢å®Œæˆï¼Œå‡†å¤‡é‡æ’åº...", 55)
        
        # Debug: ç»Ÿè®¡æ¥æºåˆ†å¸ƒ
        source_counts = {}
        for result in final_results[:top_n]:
            sha1 = result["source_sha1"]
            source_counts[sha1] = source_counts.get(sha1, 0) + 1
        print(f"[DEBUG] Top-{top_n} results distribution by source:")
        for sha1, count in sorted(source_counts.items(), key=lambda x: -x[1]):
            print(f"  {sha1}: {count} results")
        
        # å°†æ—¶é—´ä¿¡æ¯ã€æ‰©å±•æ–‡æœ¬ä¸ç»“æœä¸€èµ·è¿”å›
        return {
            'results': final_results,
            'timing': timing_info,
            'expansion_texts': expansion_texts
        }

    def retrieve_all(self, company_name: str) -> List[Dict]:
        """Retrieve all pages from all reports matching the company name."""
        #print("\n retrieve_all be used")
        
        # Collect all matching reports
        matching_reports = []
        for report in self.all_dbs:
            document = report.get("document", {})
            metainfo = document.get("metainfo")
            if not metainfo:
                continue
            if metainfo.get("company_name") == company_name:
                matching_reports.append(report)
        
        if not matching_reports:
            _log.error(f"No report found with '{company_name}' company name.")
            raise ValueError(f"No report found with '{company_name}' company name.")
        
        if len(matching_reports) > 1:
            print(f"[INFO] retrieve_all: Found {len(matching_reports)} reports for '{company_name}', retrieving all pages from all reports")
        
        # Collect pages from all matching reports
        all_pages = []
        for report in matching_reports:
            document = report["document"]
            pages = document["content"]["pages"]
            sha1 = document["metainfo"]["sha1_name"]
            
            for page in sorted(pages, key=lambda p: p["page"]):
                result = {
                    "distance": 0.5,
                    "page": page["page"],
                    "text": page["text"],
                    "source_sha1": sha1  # Track which report this page comes from
                }
                all_pages.append(result)
            
        return all_pages