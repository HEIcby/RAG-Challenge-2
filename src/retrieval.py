import json
import logging
from typing import List, Tuple, Dict, Union
from rank_bm25 import BM25Okapi
import pickle
from pathlib import Path
import faiss
from src.api_requests import BaseQwenProcessor,BaseGeminiProcessor
from dotenv import load_dotenv
import os
import re
import numpy as np
from src.reranking import LLMReranker

_log = logging.getLogger(__name__)


def extract_years_from_question(question: str, expand_window: bool = True) -> List[int]:
    """
    ä»é—®é¢˜ä¸­æå–å¹´ä»½ä¿¡æ¯ï¼Œå¹¶å¯é€‰åœ°æ‰©å±•æ—¶é—´çª—å£
    æ”¯æŒæ ¼å¼ï¼š
    - æ˜ç¡®å¹´ä»½: "2025å¹´", "2023å¹´ç¬¬ä¸€å­£åº¦"
    - æ—¥æœŸæ ¼å¼: "2025å¹´9æœˆ30æ—¥"
    - å¤šå¹´ä»½æ¯”è¾ƒ: "2024å¹´ç›¸æ¯”2023å¹´" â†’ [2023, 2024]
    
    Args:
        question: ç”¨æˆ·é—®é¢˜
        expand_window: æ˜¯å¦æ‰©å±•æ—¶é—´çª—å£ï¼ˆåœ¨å¹´ä»½èŒƒå›´å‰åå„åŠ 1å¹´ï¼‰
    
    Returns:
        List[int]: æå–åˆ°çš„å¹´ä»½åˆ—è¡¨ï¼ˆå»é‡å¹¶æ’åºï¼‰
        
    ç¤ºä¾‹ï¼š
        é—®"2024å¹´xxx" + expand_window=True â†’ [2023, 2024, 2025]
        é—®"2024å¹´ç›¸æ¯”2023å¹´" + expand_window=True â†’ [2022, 2023, 2024, 2025] ï¼ˆèŒƒå›´æ‰©å±•è€Œéé€ä¸ªæ‰©å±•ï¼‰
        é—®"2024å¹´xxx" + expand_window=False â†’ [2024]
    """
    # æ­£åˆ™åŒ¹é… 20XXå¹´ æ ¼å¼
    year_pattern = r'(20\d{2})å¹´'
    matches = re.findall(year_pattern, question)
    extracted_years = [int(y) for y in matches]
    
    if not extracted_years:
        return []
    
    if expand_window:
        # æ‰¾åˆ°å¹´ä»½èŒƒå›´çš„æœ€å°å’Œæœ€å¤§å€¼
        min_year = min(extracted_years)
        max_year = max(extracted_years)
        
        # åœ¨èŒƒå›´å‰åå„æ‰©å±•1å¹´ï¼Œç”Ÿæˆè¿ç»­å¹´ä»½åˆ—è¡¨
        years = list(range(min_year - 1, max_year + 2))  # +2 å› ä¸º range ä¸åŒ…å«ç»“æŸå€¼
        
        print(f"[DEBUG] ğŸ“… æå–å¹´ä»½: {sorted(set(extracted_years))} â†’ æ‰©å±•èŒƒå›´: [{min_year-1}, {max_year+1}]")
    else:
        years = extracted_years
    
    return sorted(list(set(years)))  # å»é‡å¹¶æ’åº


def route_reports_by_time(
    company_name: str, 
    question: str, 
    all_reports: List[Dict],
    fallback_strategy: str = "all"  # "all" æˆ– "latest"
) -> List[Dict]:
    """
    åŸºäºæ—¶é—´ä¿¡æ¯å’Œå…¬å¸åè·¯ç”±åˆ°åˆé€‚çš„æ–‡æ¡£
    
    Args:
        company_name: å…¬å¸åç§°
        question: ç”¨æˆ·é—®é¢˜
        all_reports: æ‰€æœ‰å¯ç”¨çš„æŠ¥å‘Š
        fallback_strategy: å½“æ²¡æœ‰æ—¶é—´ä¿¡æ¯æ—¶çš„å›é€€ç­–ç•¥
            - "all": è¿”å›è¯¥å…¬å¸æ‰€æœ‰æ–‡æ¡£
            - "latest": åªè¿”å›æœ€æ–°å¹´ä»½çš„æ–‡æ¡£
    
    Returns:
        List[Dict]: åŒ¹é…çš„æŠ¥å‘Šåˆ—è¡¨
    """
    # 1. å…ˆæŒ‰å…¬å¸åè¿‡æ»¤
    company_reports = []
    for report in all_reports:
        document = report.get("document", {})
        metainfo = document.get("metainfo", {})
        if metainfo.get("company_name") == company_name:
            company_reports.append(report)
    
    if not company_reports:
        return []
    
    # 2. æå–é—®é¢˜ä¸­çš„å¹´ä»½ä¿¡æ¯ï¼ˆå¸¦æ—¶é—´çª—å£æ‰©å±•ï¼‰
    years = extract_years_from_question(question, expand_window=True)
    
    # 3. å¦‚æœæœ‰æ˜ç¡®å¹´ä»½ï¼Œåªè¿”å›å¯¹åº”å¹´ä»½çš„æ–‡æ¡£
    if years:
        filtered_reports = []
        for report in company_reports:
            document = report.get("document", {})
            metainfo = document.get("metainfo", {})
            
            # ä¼˜å…ˆä» year å­—æ®µè·å–ï¼Œå¦åˆ™ä» sha1_name ä¸­æå–ï¼ˆå¦‚ "J2025" â†’ 2025ï¼‰
            report_year = metainfo.get("year")
            if report_year is None:
                sha1_name = metainfo.get("sha1_name", "")
                # ä» sha1_name ä¸­æå–å¹´ä»½ï¼šåŒ¹é… J20XX æˆ– 20XX æ ¼å¼
                year_match = re.search(r'[J]?(20\d{2})', sha1_name)
                if year_match:
                    report_year = int(year_match.group(1))
            
            # æ”¯æŒå­—ç¬¦ä¸²æˆ–æ•´æ•°æ ¼å¼çš„ year
            if report_year is not None:
                try:
                    report_year = int(report_year)
                    if report_year in years:
                        filtered_reports.append(report)
                except (ValueError, TypeError):
                    pass
        
        if filtered_reports:
            print(f"[INFO] ğŸ¯ æ—¶é—´è·¯ç”±ï¼ˆå«å‰åå¹´çª—å£ï¼‰: å¹´ä»½ {years}ï¼ŒåŒ¹é…åˆ° {len(filtered_reports)} ä¸ªæ–‡æ¡£")
            return filtered_reports
        else:
            print(f"[WARNING] âš ï¸ è¯†åˆ«åˆ°å¹´ä»½ {years}ï¼Œä½†æœªæ‰¾åˆ°å¯¹åº”æ–‡æ¡£ï¼Œå›é€€åˆ°å…¨éƒ¨æ–‡æ¡£")
    
    # 4. æ²¡æœ‰æ—¶é—´ä¿¡æ¯æ—¶çš„å›é€€ç­–ç•¥
    if fallback_strategy == "latest":
        # è¿”å›æœ€æ–°å¹´ä»½çš„æ–‡æ¡£
        latest_year = None
        latest_reports = []
        for report in company_reports:
            document = report.get("document", {})
            metainfo = document.get("metainfo", {})
            
            # ä¼˜å…ˆä» year å­—æ®µè·å–ï¼Œå¦åˆ™ä» sha1_name ä¸­æå–
            report_year = metainfo.get("year")
            if report_year is None:
                sha1_name = metainfo.get("sha1_name", "")
                year_match = re.search(r'[J]?(20\d{2})', sha1_name)
                if year_match:
                    report_year = int(year_match.group(1))
            
            if report_year is not None:
                try:
                    report_year = int(report_year)
                    if latest_year is None or report_year > latest_year:
                        latest_year = report_year
                        latest_reports = [report]
                    elif report_year == latest_year:
                        latest_reports.append(report)
                except (ValueError, TypeError):
                    pass
                    pass
        
        if latest_reports:
            print(f"[INFO] ğŸ“… æ— æ˜ç¡®æ—¶é—´ä¿¡æ¯ï¼Œä½¿ç”¨æœ€æ–°å¹´ä»½ {latest_year} çš„æ–‡æ¡£")
            return latest_reports
    
    # é»˜è®¤è¿”å›æ‰€æœ‰è¯¥å…¬å¸çš„æ–‡æ¡£
    print(f"[INFO] ğŸ“š æ— æ˜ç¡®æ—¶é—´ä¿¡æ¯ï¼Œä½¿ç”¨è¯¥å…¬å¸æ‰€æœ‰ {len(company_reports)} ä¸ªæ–‡æ¡£")
    return company_reports

class BM25Retriever:
    def __init__(self, bm25_db_dir: Path, documents_dir: Path, subset_path: Path = None):
        self.bm25_db_dir = bm25_db_dir
        self.documents_dir = documents_dir
        self.subset_path = subset_path
        self.year_lookup = self._load_year_lookup() if subset_path else {}
    
    def _load_year_lookup(self) -> dict:
        """ä» subset.csv åŠ è½½ sha1 -> year çš„æ˜ å°„"""
        import csv
        year_lookup = {}
        try:
            with open(self.subset_path, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    sha1 = row.get('sha1', '').strip()
                    year = row.get('year', '').strip()
                    if sha1 and year:
                        try:
                            year_lookup[sha1] = int(year)
                        except ValueError:
                            pass
            print(f"[INFO] ğŸ“… BM25: ä» subset.csv åŠ è½½äº† {len(year_lookup)} ä¸ªæ–‡æ¡£çš„å¹´ä»½ä¿¡æ¯")
        except Exception as e:
            print(f"[WARNING] âš ï¸ BM25: æ— æ³•åŠ è½½ subset.csv å¹´ä»½ä¿¡æ¯: {e}")
        return year_lookup
        
    def retrieve_by_company_name(self, company_name: str, query: str, top_n: int = 3, return_parent_pages: bool = False) -> List[Dict]:
        print("BM25Retriever retrieve_by_company_name is called")
        
        # ğŸ¯ å…ˆæ”¶é›†æ‰€æœ‰æ–‡æ¡£ï¼Œç„¶åä½¿ç”¨æ—¶é—´è·¯ç”±
        all_documents = []
        for path in self.documents_dir.glob("*.json"):
            with open(path, 'r', encoding='utf-8') as f:
                doc = json.load(f)
                all_documents.append({
                    "path": path,
                    "document": doc,
                    "sha1": doc["metainfo"]["sha1_name"]
                })
        
        # ä½¿ç”¨æ—¶é—´è·¯ç”±è¿‡æ»¤æ–‡æ¡£
        years = extract_years_from_question(query)
        matching_documents = []
        
        for doc_info in all_documents:
            doc = doc_info["document"]
            metainfo = doc.get("metainfo", {})
            sha1 = doc_info["sha1"]
            
            # æ£€æŸ¥å…¬å¸å
            if metainfo.get("company_name") != company_name:
                continue
            
            # å¦‚æœæœ‰å¹´ä»½ä¿¡æ¯ï¼Œè¿›ä¸€æ­¥è¿‡æ»¤
            if years:
                # ä¼˜å…ˆä» metainfo è¯»å–ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä» year_lookup è¯»å–
                doc_year = metainfo.get("year")
                if doc_year is None and sha1 in self.year_lookup:
                    doc_year = self.year_lookup[sha1]
                
                if doc_year is not None:
                    try:
                        doc_year = int(doc_year)
                        if doc_year not in years:
                            continue
                    except (ValueError, TypeError):
                        pass
            
            matching_documents.append(doc_info)
        
        if not matching_documents:
            raise ValueError(f"No report found with '{company_name}' company name.")
        
        if years:
            print(f"[INFO] ğŸ¯ BM25æ—¶é—´è·¯ç”±: è¯†åˆ«å¹´ä»½ {years}ï¼ŒåŒ¹é…åˆ° {len(matching_documents)} ä¸ªæ–‡æ¡£")
        elif len(matching_documents) > 1:
            print(f"[INFO] Found {len(matching_documents)} reports for '{company_name}', retrieving from all")
            
        # Retrieve from all matching documents and aggregate results
        all_retrieval_results = []
        
        for doc_info in matching_documents:
            document = doc_info["document"]
            sha1 = doc_info["sha1"]
            
            # Load corresponding BM25 index
            bm25_path = self.bm25_db_dir / f"{sha1}.pkl"
            if not bm25_path.exists():
                print(f"[WARNING] BM25 index not found for {sha1}, skipping")
                continue
                
            with open(bm25_path, 'rb') as f:
                bm25_index = pickle.load(f)
                
            # Get the document content and BM25 index
            chunks = document["content"]["chunks"]
            pages = document["content"]["pages"]
            
            # Get BM25 scores for the query
            tokenized_query = query.split()
            scores = bm25_index.get_scores(tokenized_query)
            
            actual_top_n = min(top_n, len(scores))
            top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:actual_top_n]
            
            seen_pages = set()
            
            for index in top_indices:
                score = round(float(scores[index]), 4)
                chunk = chunks[index]
                parent_page = next(page for page in pages if page["page"] == chunk["page"])
                
                if return_parent_pages:
                    if parent_page["page"] not in seen_pages:
                        seen_pages.add(parent_page["page"])
                        result = {
                            "distance": score,
                            "page": parent_page["page"],
                            "text": parent_page["text"],
                            "source_sha1": sha1  # Add source document identifier
                        }
                        all_retrieval_results.append(result)
                else:
                    result = {
                        "distance": score,
                        "page": chunk["page"],
                        "text": chunk["text"],
                        "source_sha1": sha1  # Add source document identifier
                    }
                    all_retrieval_results.append(result)
        
        # Sort by score and return top_n results across all documents
        all_retrieval_results.sort(key=lambda x: x["distance"], reverse=True)
        return all_retrieval_results[:top_n]

class HybridRetriever:
    def __init__(self, vector_db_dir: Path, documents_dir: Path, use_hyde: bool = True, use_multi_query: bool = True, subset_path: Path = None):
        self.vector_retriever = VectorRetriever(vector_db_dir, documents_dir, use_hyde=use_hyde, use_multi_query=use_multi_query, subset_path=subset_path)
        self.reranker = LLMReranker()
        
    def retrieve_by_company_name(
        self, 
        company_name: str, 
        query: str, 
        llm_reranking_sample_size: int = 28,
        documents_batch_size: int = 2,
        top_n: int = 6,
        llm_weight: float = 0.7,
        return_parent_pages: bool = False,
        use_hyde: bool = None,
        use_multi_query: bool = None,
        progress_callback=None
    ) -> List[Dict]:
        """
        Retrieve and rerank documents using hybrid approach.
        
        Args:
            company_name: Name of the company to search documents for
            query: Search query
            llm_reranking_sample_size: Number of initial results to retrieve from vector DB
            documents_batch_size: Number of documents to analyze in one LLM prompt
            top_n: Number of final results to return after reranking
            llm_weight: Weight given to LLM scores (0-1)
            return_parent_pages: Whether to return full pages instead of chunks
            
        Returns:
            List of reranked document dictionaries with scores
        """
        # Get initial results from vector retriever
        vector_results = self.vector_retriever.retrieve_by_company_name(
            company_name=company_name,
            query=query,
            top_n=llm_reranking_sample_size,
            return_parent_pages=return_parent_pages,
            use_hyde=use_hyde,
            use_multi_query=use_multi_query,
            progress_callback=progress_callback
        )
        
        print(f"[DEBUG] Initial vector results count: {len(vector_results)}")

        # é‡æ’åºé˜¶æ®µï¼ˆè¿™æ˜¯æœ€è€—æ—¶çš„éƒ¨åˆ†ï¼‰
        if progress_callback:
            progress_callback("ğŸ¯ LLM é‡æ’åºä¸­ï¼ˆè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰...", 58)
        
        # Rerank results using LLM
        reranked_results = self.reranker.rerank_documents(
            query=query,
            documents=vector_results,
            documents_batch_size=documents_batch_size,
            llm_weight=llm_weight
        )

        print(f"[DEBUG] Reranked results count: {len(reranked_results)}")
        #print("[DEBUG] HybridRetriever retrieve_by_company_name is called")
        print(f"[DEBUG] Final top_n: {top_n}")
        return reranked_results[:top_n]


class VectorRetriever:
    def __init__(self, vector_db_dir: Path, documents_dir: Path, use_hyde: bool = True, use_multi_query: bool = True, subset_path: Path = None):
        self.vector_db_dir = vector_db_dir
        self.documents_dir = documents_dir
        self.subset_path = subset_path
        self.year_lookup = self._load_year_lookup() if subset_path else {}
        self.all_dbs = self._load_dbs()
        self.qwen = BaseQwenProcessor()
        self.use_hyde = use_hyde
        self.use_multi_query = use_multi_query
        #print(f"[DEBUG][VectorRetriever.__init__] use_hyde={self.use_hyde}, use_multi_query={self.use_multi_query}")
    
    def _load_year_lookup(self) -> dict:
        """ä» subset.csv åŠ è½½ sha1 -> year çš„æ˜ å°„"""
        import csv
        year_lookup = {}
        try:
            with open(self.subset_path, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    sha1 = row.get('sha1', '').strip()
                    year = row.get('year', '').strip()
                    if sha1 and year:
                        try:
                            year_lookup[sha1] = int(year)
                        except ValueError:
                            pass
            print(f"[INFO] ğŸ“… ä» subset.csv åŠ è½½äº† {len(year_lookup)} ä¸ªæ–‡æ¡£çš„å¹´ä»½ä¿¡æ¯")
        except Exception as e:
            print(f"[WARNING] âš ï¸ æ— æ³•åŠ è½½ subset.csv å¹´ä»½ä¿¡æ¯: {e}")
        return year_lookup

    # Qwen embedding ä¸éœ€è¦ set_up_llm
    
    # Qwen embedding ä¸éœ€è¦ set_up_llm

    def _load_dbs(self):
        all_dbs = []
        company_names = []  # ç”¨äºæ”¶é›†company_name
        # Get list of JSON document paths
        all_documents_paths = list(self.documents_dir.glob('*.json'))
        vector_db_files = {db_path.stem: db_path for db_path in self.vector_db_dir.glob('*.faiss')}

        for document_path in all_documents_paths:
            #print(f"[DEBUG] Loading document: {document_path.name}")
            stem = document_path.stem
            if stem not in vector_db_files:
                _log.warning(f"No matching vector DB found for document {document_path.name}")
                continue
            try:
                with open(document_path, 'r', encoding='utf-8') as f:
                    document = json.load(f)
            except Exception as e:
                _log.error(f"Error loading JSON from {document_path.name}: {e}")
                continue

            # Validate that the document meets the expected schema
            if not (isinstance(document, dict) and "metainfo" in document and "content" in document):
                _log.warning(f"Skipping {document_path.name}: does not match the expected schema.")
                continue

            # æ”¶é›†company_name
            company_name = document.get("metainfo", {}).get("company_name", None)
            if company_name:
                company_names.append(company_name)
            
            # ğŸ†• ä» year_lookup æ³¨å…¥ year ä¿¡æ¯åˆ° metainfo
            sha1_name = document.get("metainfo", {}).get("sha1_name", stem)
            if sha1_name in self.year_lookup:
                document["metainfo"]["year"] = self.year_lookup[sha1_name]

            try:
                vector_db = faiss.read_index(str(vector_db_files[stem]))
            except Exception as e:
                _log.error(f"Error reading vector DB for {document_path.name}: {e}")
                continue

            report = {
                "name": stem,
                "vector_db": vector_db,
                "document": document
            }
            all_dbs.append(report)

        # print("[DEBUG] å½“å‰å¯ç”¨çš„company_nameæœ‰:")
        # for name in company_names:
        #     print(f"  - {name}")
        # print("[DEBUG] å½“å‰å¯ç”¨çš„company_nameä»¥ä¸Š")

        return all_dbs

    @staticmethod
    def get_strings_cosine_similarity(str1, str2):
        qwen = BaseQwenProcessor()
        emb1 = qwen.get_embeddings([str1])["embeddings"][0]
        emb2 = qwen.get_embeddings([str2])["embeddings"][0]
        similarity_score = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
        similarity_score = round(similarity_score, 4)
        return similarity_score

    
   
    def retrieve_by_company_name(self, company_name: str, query: str, llm_reranking_sample_size: int = None, top_n: int = 3, return_parent_pages: bool = False, use_hyde: bool = None, use_multi_query: bool = None, progress_callback=None) -> List[Tuple[str, float]]:
        import sys
        print("[DEBUG] VectorRetriever retrieve_by_company_name is called")
        sys.stdout.flush()

        # ğŸ¯ ä½¿ç”¨æ—¶é—´æ™ºèƒ½è·¯ç”±æ›¿ä»£åŸæœ‰çš„ç®€å•å…¬å¸åè¿‡æ»¤
        # è¿™æ ·å¯ä»¥æ ¹æ®é—®é¢˜ä¸­çš„æ—¶é—´ä¿¡æ¯è‡ªåŠ¨å®šä½åˆ°å¯¹åº”å¹´ä»½çš„æ–‡æ¡£
        if progress_callback:
            progress_callback("ğŸ“š å®šä½ç›¸å…³æ–‡æ¡£ä¸­...", 28)
        
        matching_reports = route_reports_by_time(
            company_name=company_name,
            question=query,
            all_reports=self.all_dbs,
            fallback_strategy="all"  # æ— æ—¶é—´ä¿¡æ¯æ—¶ä½¿ç”¨æ‰€æœ‰æ–‡æ¡£
        )
        
        if not matching_reports:
            _log.error(f"No report found with '{company_name}' company name.")
            raise ValueError(f"No report found with '{company_name}' company name.")
        
        if len(matching_reports) > 1:
            print(f"[INFO] Found {len(matching_reports)} reports for '{company_name}', retrieving from all")
            sys.stdout.flush()
            for rep in matching_reports:
                doc = rep.get("document", {})
                metainfo = doc.get("metainfo", {})
                year = metainfo.get("year", "unknown")
                print(f"  - Report: {rep['name']} (Year: {year})")
                sys.stdout.flush()
        
        # Priority parameters
        use_hyde = self.use_hyde if use_hyde is None else use_hyde
        use_multi_query = self.use_multi_query if use_multi_query is None else use_multi_query
        print(f"[DEBUG][retrieve_by_company_name] use_hyde={use_hyde}, use_multi_query={use_multi_query}")
        sys.stdout.flush()
        
        qwen = BaseQwenProcessor()
        # æ§åˆ¶multi_queryå’Œhydeæ‰©å……
        queries = [query]

        if use_hyde:
            if progress_callback:
                progress_callback("ğŸ”® HYDE æŸ¥è¯¢æ‰©å±•ä¸­...", 32)
            print(f"[DEBUG] å¼€å§‹ HYDE æ‰©å±•...")
            sys.stdout.flush()
            try:
                print(f"[DEBUG] è°ƒç”¨ Qwen API ç”Ÿæˆå‡è®¾ç­”æ¡ˆ...")
                sys.stdout.flush()
                fake_answer = qwen.send_message(
                    model="qwen-turbo",
                    system_content=(
                        "You are a creative report writer. "
                        "When asked a question, your task is NOT to retrieve real-time or factual financial data, "
                        "but instead to **invent, compile, or simulate** a helpful passage, article, or news-style report "
                        "that could plausibly assist in answering the query. "
                        "Even if the query asks about real numbers or unavailable information, "
                        "you should respond by **fabricating a coherent, contextually relevant narrative** "
                        "rather than disclaiming lack of data. "
                        "Your goal is to produce a well-written piece (report, analysis, or article) "
                        "that reads like it could come from a newspaper, magazine, or research commentary."
                    ),
                    human_content=f"Write a full passage to address this query in an informative and narrative way: {query}",
                    is_structured=False
                )
                if isinstance(fake_answer, list):
                    fake_answer_str = ''.join(fake_answer)
                else:
                    fake_answer_str = str(fake_answer)
                queries.append(fake_answer_str)
                print(f"[DEBUG] HYDE æ‰©å±•æˆåŠŸï¼Œç”Ÿæˆå‡è®¾ç­”æ¡ˆé•¿åº¦: {len(fake_answer_str)}")
                sys.stdout.flush()
            except Exception as e:
                print(f"[ERROR] HYDE expansion failed: {e}")
                sys.stdout.flush()

        if use_multi_query:
            if progress_callback:
                progress_callback("ğŸ”„ Multi-Query æŸ¥è¯¢æ‰©å±•ä¸­...", 38)
            print(f"[DEBUG] å¼€å§‹ Multi-Query æ‰©å±•...")
            sys.stdout.flush()
            # expansion_methods = {
            #     1: "Expand the question by replacing key terms with synonyms or related terms while keeping the meaning in the context of annual reports and financial statements. Generate three queries, each wrapped in <>.",
            #     2: "Expand the question by including broader or narrower related terms (hypernyms or hyponyms) relevant to annual reports and financial statements. Generate three queries, each wrapped in <>.",
            #     3: "Rewrite the question into three paraphrased variations that keep the same intent in the context of annual reports and financial statements. Generate three queries, each wrapped in <>."
            # }
            expansion_methods = {
                1: "Expand the question by replacing key terms with synonyms or related terms while keeping the meaning in the context of annual reports and financial statements. Generate one query, wrapped in <>.",
                2: "Expand the question by including broader or narrower related terms (hypernyms or hyponyms) relevant to annual reports and financial statements. Generate one query, wrapped in <>.",
                3: "Rewrite the question into one paraphrased variation that keeps the same intent in the context of annual reports and financial statements. Generate one query, wrapped in <>."
            }
            import re
            for method_id, prompt in expansion_methods.items():
                print(f"[DEBUG] Multi-Query æ–¹æ³• {method_id}...")
                sys.stdout.flush()
                try:
                    print(f"[DEBUG] è°ƒç”¨ Qwen API æ‰©å±•æŸ¥è¯¢...")
                    sys.stdout.flush()
                    response = qwen.send_message(
                        model="qwen-turbo",
                        system_content="You are assisting in an Enterprise RAG Challenge focused on annual reports.",
                        human_content=f"{prompt}\nOriginal question: {query}",
                        is_structured=False
                    )
                    extracted_queries = re.findall(r"<(.*?)>", response, flags=re.DOTALL)
                    for q in extracted_queries:
                        queries.append(q.strip())
                    print(f"[DEBUG] Multi-Query æ–¹æ³• {method_id} å®Œæˆï¼Œæå–äº† {len(extracted_queries)} ä¸ªæŸ¥è¯¢")
                    sys.stdout.flush()
                except Exception as e:
                    print(f"Expansion method {method_id} failed: {e}")
        
        # å‘½ä¸­ç»“æœå­˜å‚¨ï¼ˆç”¨å­—å…¸èšåˆï¼‰
        # key = (sha1, page_id or chunk_id), value = dict with distances, count, text
        aggregated_results = {}

        inner_factor = 1.0
        print("[DEBUG] queries is", queries)
        print("[DEBUG] queries's length is", len(queries))

        # ğŸ¯ æ™ºèƒ½åˆ†é…ç­–ç•¥ï¼šå°† top_n å¹³å‡åˆ†é…åˆ°æ¯ä¸ªåŒ¹é…çš„æ–‡æ¡£
        # è¿™æ ·å¯ä»¥ç¡®ä¿æ¯ä¸ªæ–‡æ¡£éƒ½æœ‰å…¬å¹³çš„æœºä¼šè¢«æ£€ç´¢åˆ°
        # é¿å…å•ä¸ªæ–‡æ¡£dominateæ‰€æœ‰ç»“æœ
        num_reports = len(matching_reports)
        top_n_per_report = max(1, top_n // num_reports)  # ç¡®ä¿è‡³å°‘ä¸º1
        remaining = top_n % num_reports  # ä½™æ•°åˆ†é…ç»™å‰å‡ ä¸ªæ–‡æ¡£
        
        print(f"[INFO] ğŸ“Š æ£€ç´¢ç­–ç•¥: {num_reports}ä¸ªæ–‡æ¡£, æ¯ä¸ªåˆ†é…çº¦{top_n_per_report}ä¸ªchunks (æ€»é¢„ç®—{top_n})")
        if remaining > 0:
            print(f"[INFO] ğŸ’¡ å‰{remaining}ä¸ªæ–‡æ¡£é¢å¤–è·å¾—1ä¸ªchunké…é¢")

        # å‘é‡æ£€ç´¢é˜¶æ®µ
        if progress_callback:
            progress_callback("ğŸ” å‘é‡æ£€ç´¢ä¸­...", 45)

        # Process each matching report
        for idx, report in enumerate(matching_reports):
            document = report["document"]
            vector_db = report["vector_db"]
            chunks = document["content"]["chunks"]
            pages = document["content"]["pages"]
            sha1 = document["metainfo"]["sha1_name"]
            
            # ä¸ºæ¯ä¸ªæ–‡æ¡£åˆ†é…åˆé€‚çš„ top_n
            doc_top_n = top_n_per_report + (1 if idx < remaining else 0)
            actual_top_n = min(doc_top_n, len(chunks))
            
            print(f"[DEBUG] ä» {sha1} æ£€ç´¢ {actual_top_n} ä¸ªchunks (å…±{len(chunks)}ä¸ª)")
            
            # Retrieve for each query
            for q in queries:
                if not q.strip():
                    print(f"[ERROR] query is empty, skip embedding: '{q}'")
                    continue
                emb_result = self.qwen.get_embeddings([q])
                if not emb_result or not isinstance(emb_result, list) or not emb_result[0] or 'embedding' not in emb_result[0]:
                    print(f"[ERROR] embedding result is empty or invalid for query: {q}, emb_result: {emb_result}")
                    continue
                print("[DEBUG] emb_result[0] =", emb_result[0])
                embedding = emb_result[0]['embedding']
                embedding_array = np.array(embedding, dtype=np.float32).reshape(1, -1)
                distances, indices = vector_db.search(x=embedding_array, k=actual_top_n)
            
                for distance, index in zip(distances[0], indices[0]):
                    distance = round(float(distance)*inner_factor, 4)
                    chunk = chunks[index]
                    parent_page = next(page for page in pages if page["page"] == chunk["page"])
                    
                    # Debug: æ‰“å°æ¯ä¸ªæ–‡æ¡£çš„æ£€ç´¢ç»“æœ
                    print(f"[DEBUG] Retrieved from {sha1}: page={chunk['page']}, distance={distance}, text_preview={chunk['text'][:50]}...")
                    
                    if return_parent_pages:
                        # Include sha1 in key to differentiate same page numbers across different reports
                        key = (sha1, "page", parent_page["page"])
                        text = parent_page["text"]
                        page_id = parent_page["page"]
                    else:
                        key = (sha1, "chunk", index)
                        text = chunk["text"]
                        page_id = chunk["page"]
                    
                    if key not in aggregated_results:
                        aggregated_results[key] = {
                            "page": page_id,
                            "text": text,
                            "distances": [distance],
                            "count": 1,
                            "source_sha1": sha1  # Track source document
                        }
                    else:
                        aggregated_results[key]["distances"].append(distance)
                        aggregated_results[key]["count"] += 1
    
        # åŠ æƒè§„åˆ™: 1æ¬¡=Ã—1.0, 2æ¬¡=Ã—1.2, 3æ¬¡=Ã—1.4ã€‚. ä»¥æ­¤ç±»æ¨ã€‚ æ³¨æ„ï¼šå½“å‰ faiss ç”¨çš„æ˜¯ IndexFlatIPï¼ˆå†…ç§¯ï¼‰ï¼Œdistance è¶Šå¤§è¡¨ç¤ºç›¸å…³æ€§è¶Šé«˜ã€‚å› æ­¤ï¼Œå‘½ä¸­å¤šæ¬¡æ—¶ï¼Œåº”è¯¥è®© distance å¢å¤§ã€‚
        def weight_factor(count: int) -> float:
            return 1.0 + 0.2 * (count - 1)
    
        final_results = []
        for key, info in aggregated_results.items():
            base_distance = max(info["distances"])  # å–æœ€å¤§è·ç¦»ä½œä¸ºåŸºå‡†
            factor = weight_factor(info["count"])
            weighted_distance = round(base_distance * factor, 4)
        
            final_results.append({
                "distance": weighted_distance,
                "page": info["page"],
                "text": info["text"],
                "hit_count": info["count"],  # æ–¹ä¾¿è°ƒè¯•çœ‹åˆ°è¢«å‘½ä¸­æ¬¡æ•°
                "source_sha1": info["source_sha1"]  # Include source document
            })
    
        # èšåˆï¼šæŒ‰åŠ æƒåçš„è·ç¦»é™åºï¼Œå–å‰ top_nï¼ˆdistanceè¶Šå¤§è¶Šç›¸å…³ï¼‰
        final_results = sorted(final_results, key=lambda x: x["distance"], reverse=True)
        
        # Debug: æ˜¾ç¤ºèšåˆåçš„æ–‡æ¡£åˆ†å¸ƒ
        source_distribution = {}
        for res in final_results[:top_n]:
            source = res.get("source_sha1", "Unknown")
            source_distribution[source] = source_distribution.get(source, 0) + 1
        print(f"[DEBUG] Top {top_n} results distribution: {source_distribution}")
        
        final_results = final_results[:top_n]


        # æ£€ç´¢å®Œæˆ
        if progress_callback:
            progress_callback("âœ… æ£€ç´¢å®Œæˆï¼Œå‡†å¤‡é‡æ’åº...", 55)
        
        # Debug: ç»Ÿè®¡æ¥æºåˆ†å¸ƒ
        source_counts = {}
        for result in final_results[:top_n]:
            sha1 = result["source_sha1"]
            source_counts[sha1] = source_counts.get(sha1, 0) + 1
        print(f"[DEBUG] Top-{top_n} results distribution by source:")
        for sha1, count in sorted(source_counts.items(), key=lambda x: -x[1]):
            print(f"  {sha1}: {count} results")
        
    
        return final_results

    def retrieve_all(self, company_name: str) -> List[Dict]:
        """Retrieve all pages from all reports matching the company name."""
        #print("\n retrieve_all be used")
        
        # Collect all matching reports
        matching_reports = []
        for report in self.all_dbs:
            document = report.get("document", {})
            metainfo = document.get("metainfo")
            if not metainfo:
                continue
            if metainfo.get("company_name") == company_name:
                matching_reports.append(report)
        
        if not matching_reports:
            _log.error(f"No report found with '{company_name}' company name.")
            raise ValueError(f"No report found with '{company_name}' company name.")
        
        if len(matching_reports) > 1:
            print(f"[INFO] retrieve_all: Found {len(matching_reports)} reports for '{company_name}', retrieving all pages from all reports")
        
        # Collect pages from all matching reports
        all_pages = []
        for report in matching_reports:
            document = report["document"]
            pages = document["content"]["pages"]
            sha1 = document["metainfo"]["sha1_name"]
            
            for page in sorted(pages, key=lambda p: p["page"]):
                result = {
                    "distance": 0.5,
                    "page": page["page"],
                    "text": page["text"],
                    "source_sha1": sha1  # Track which report this page comes from
                }
                all_pages.append(result)
            
        return all_pages