#!/usr/bin/env python3
"""
Interactive Q&A script for val_set (é‡‘ç›˜ç§‘æŠ€)
Allows users to ask questions in real-time and get answers from the RAG system.
"""

import sys
from pathlib import Path
from src.questions_processing import QuestionsProcessor
from src.pipeline import RunConfig
import json
from datetime import datetime

class ValOnline:
    def __init__(self, 
                 root_path: Path,
                 use_hyde: bool = True,
                 use_multi_query: bool = True,
                 llm_reranking: bool = True,
                 top_n_retrieval: int = 10,
                 api_provider: str = "qwen",
                 answering_model: str = "qwen-max"):
        """
        Initialize the interactive Q&A system for val_set.
        
        Args:
            root_path: Path to val_set directory
            use_hyde: Enable HYDE hypothetical document expansion
            use_multi_query: Enable multi-query expansion
            llm_reranking: Enable LLM-based reranking
            top_n_retrieval: Number of chunks to retrieve
            api_provider: API provider ("qwen", "openai", "gemini")
            answering_model: Model name for answering
        """
        self.root_path = root_path
        self.company_name = "é‡‘ç›˜ç§‘æŠ€"
        
        # Initialize paths
        self.vector_db_dir = root_path / "databases" / "vector_dbs"
        self.documents_dir = root_path / "databases" / "chunked_reports"
        self.subset_path = root_path / "subset.csv"
        
        # Check if databases exist
        if not self.documents_dir.exists() or not self.vector_db_dir.exists():
            print("âŒ é”™è¯¯: æ•°æ®åº“ä¸å­˜åœ¨ï¼")
            print(f"è¯·å…ˆè¿è¡Œä»¥ä¸‹å‘½ä»¤å¤„ç† PDF æ–‡ä»¶:")
            print(f"  cd {root_path}")
            print(f"  python main.py parse-pdfs")
            print(f"  python main.py process-reports")
            sys.exit(1)
        
        # Initialize processor
        print("ğŸ”§ åˆå§‹åŒ–é—®ç­”ç³»ç»Ÿ...")
        print(f"ğŸ“ æ•°æ®ç›®å½•: {root_path}")
        print(f"ğŸ¢ å…¬å¸: {self.company_name}")
        print(f"ğŸ¤– APIæä¾›å•†: {api_provider}")
        print(f"ğŸ§  æ¨¡å‹: {answering_model}")
        print(f"ğŸ” æ£€ç´¢æ•°é‡: {top_n_retrieval}")
        print(f"ğŸ’¡ HYDE: {'å¯ç”¨' if use_hyde else 'ç¦ç”¨'}")
        print(f"ğŸ”„ Multi-Query: {'å¯ç”¨' if use_multi_query else 'ç¦ç”¨'}")
        print(f"ğŸ¯ LLMé‡æ’åº: {'å¯ç”¨' if llm_reranking else 'ç¦ç”¨'}")
        if llm_reranking:
            print(f"   â””â”€ åˆå§‹æ£€ç´¢: 50ä¸ªchunks (å¤šæ–‡æ¡£æ—¶è‡ªåŠ¨å¹³å‡åˆ†é…)")
        print()
        
        self.processor = QuestionsProcessor(
            vector_db_dir=self.vector_db_dir,
            documents_dir=self.documents_dir,
            questions_file_path=None,  # No file, we'll ask interactively
            new_challenge_pipeline=True,
            subset_path=self.subset_path,
            parent_document_retrieval=False,
            llm_reranking=llm_reranking,
            llm_reranking_sample_size=50 if llm_reranking else 10,  # å¢åŠ åˆ°50ä»¥ç¡®ä¿å¤šæ–‡æ¡£æ—¶æ¯ä¸ªéƒ½èƒ½è¢«å……åˆ†æ£€ç´¢
            top_n_retrieval=top_n_retrieval,
            parallel_requests=1,  # Sequential for interactive mode
            api_provider=api_provider,
            answering_model=answering_model,
            full_context=False,
            use_hyde=use_hyde,
            use_multi_query=use_multi_query
        )
        
        print("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼\n")
    
    def format_answer(self, answer_dict: dict) -> str:
        """Format the answer for display."""
        output = []
        output.append("=" * 80)
        output.append("ğŸ“Š ç­”æ¡ˆ")
        output.append("=" * 80)
        
        # Main answer - check both 'final_answer' and 'answer' fields
        answer = answer_dict.get("final_answer", answer_dict.get("answer", "N/A"))
        output.append(f"ğŸ’¡ ç­”æ¡ˆ: {answer}")
        output.append("")
        
        # Step by step analysis
        if "step_by_step_analysis" in answer_dict:
            output.append("ğŸ” åˆ†æè¿‡ç¨‹:")
            output.append("-" * 80)
            analysis = answer_dict["step_by_step_analysis"]
            if isinstance(analysis, list):
                for i, step in enumerate(analysis, 1):
                    output.append(f"{i}. {step}")
            else:
                output.append(str(analysis))
            output.append("")
        
        # Reasoning summary
        if "reasoning_summary" in answer_dict:
            output.append("ğŸ“ æ¨ç†æ€»ç»“:")
            output.append("-" * 80)
            output.append(answer_dict["reasoning_summary"])
            output.append("")
        
        # References
        if "references" in answer_dict:
            refs = answer_dict["references"]
            if refs:
                output.append("ğŸ“š å‚è€ƒæ¥æº:")
                output.append("-" * 80)
                for i, ref in enumerate(refs, 1):
                    sha1 = ref.get("pdf_sha1", "N/A")[:8]
                    page = ref.get("page_index", "N/A")
                    output.append(f"{i}. æ–‡æ¡£: {sha1}... | é¡µç : {page}")
                output.append("")
        
        # Source SHA1 (if available)
        if "source_sha1" in answer_dict:
            output.append(f"ğŸ“„ æ¥æºæ–‡æ¡£: {answer_dict['source_sha1']}")
            output.append("")
        
        output.append("=" * 80)
        return "\n".join(output)

    def ask_question(self, question: str, schema: str = "jingpan") -> dict:
        """
        Ask a question and get an answer.
        
        Args:
            question: The question to ask
            schema: Expected answer type ("jingpan", "number", "boolean", "name")
        
        Returns:
            Dictionary containing the answer and related information
        """
        # Ensure company name is in the question
        if self.company_name not in question:
            question = f"{self.company_name}{question}"
        
        print(f"â“ é—®é¢˜: {question}")
        print(f"ğŸ“ ç­”æ¡ˆç±»å‹: {schema}")
        print("â³ å¤„ç†ä¸­...\n")
        
        try:
            # Get answer
            answer_dict = self.processor.get_answer_for_company(
                company_name=self.company_name,
                question=question,
                schema=schema
            )
            
            return answer_dict
            
        except Exception as e:
            print(f"âŒ é”™è¯¯: {str(e)}")
            import traceback
            traceback.print_exc()
            return {"error": str(e)}
    
    def interactive_mode(self):
        """Run in interactive mode, continuously asking for questions."""
        print("ğŸ¯ äº¤äº’å¼é—®ç­”æ¨¡å¼")
        print("=" * 80)
        print(f"ğŸ“Œ å½“å‰å…¬å¸: {self.company_name}")
        print("ğŸ“Œ å¯ä»¥ç›´æ¥è¾“å…¥é—®é¢˜ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨æ·»åŠ å…¬å¸åç§°")
        print("ğŸ“Œ é»˜è®¤ä½¿ç”¨ 'jingpan' schemaï¼ˆä¸­æ–‡è´¢åŠ¡é—®ç­”ä¸“ç”¨ï¼‰")
        print("ğŸ“Œ è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
        print("ğŸ“Œ è¾“å…¥ 'save' ä¿å­˜å†å²è®°å½•")
        print("=" * 80)
        print()
        
        history = []
        
        while True:
            try:
                # Get question from user
                question = input("ğŸ’¬ è¯·è¾“å…¥é—®é¢˜ (æˆ–å‘½ä»¤): ").strip()
                
                if not question:
                    continue
                
                # Check for exit commands
                if question.lower() in ['quit', 'exit', 'q']:
                    print("\nğŸ‘‹ å†è§ï¼")
                    break
                
                # Check for save command
                if question.lower() == 'save':
                    self.save_history(history)
                    continue
                
                # Ask for schema (optional)
                print("ğŸ“ ç­”æ¡ˆç±»å‹ (ç›´æ¥å›è½¦ä½¿ç”¨ 'jingpan')ã€å¯é€‰é¡¹ç›®['jingpan', 'number', 'boolean', 'name']ã€‘: ", end="")
                schema_input = input().strip().lower()
                schema = schema_input if schema_input in ['jingpan', 'number', 'boolean', 'name'] else 'jingpan'
                
                print()
                
                # Get answer
                answer_dict = self.ask_question(question, schema)

                print("answer_dict:", answer_dict)
                
                # Display answer
                print(self.format_answer(answer_dict))
                print()
                
                # Save to history
                history.append({
                    "timestamp": datetime.now().isoformat(),
                    "question": question,
                    "schema": schema,
                    "answer": answer_dict
                })

                _ = input("è¾“å…¥ä»»æ„å€¼å¼€å¯ä¸‹ä¸€è½®å¯¹è¯âœ… ").strip()
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ å†è§ï¼")
                break
            except Exception as e:
                print(f"âŒ é”™è¯¯: {str(e)}")
                import traceback
                traceback.print_exc()
                print()
    
    def batch_mode(self, questions: list):
        """
        Process a batch of questions.
        
        Args:
            questions: List of dicts with 'question' and optionally 'schema'
        """
        print("ğŸ“¦ æ‰¹é‡å¤„ç†æ¨¡å¼")
        print(f"ğŸ“Š æ€»é—®é¢˜æ•°: {len(questions)}")
        print("=" * 80)
        print()
        
        results = []
        
        for i, q in enumerate(questions, 1):
            question = q.get("question", q.get("text", ""))
            schema = q.get("schema", q.get("kind", "jingpan"))  # Default to jingpan
            
            print(f"[{i}/{len(questions)}] å¤„ç†ä¸­...")
            answer_dict = self.ask_question(question, schema)
            print(self.format_answer(answer_dict))
            print()
            
            results.append({
                "question": question,
                "schema": schema,
                "answer": answer_dict
            })
        
        return results
    
    def save_history(self, history: list):
        """Save question history to a JSON file."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = self.root_path / f"qa_history_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… å†å²è®°å½•å·²ä¿å­˜åˆ°: {filename}\n")


def main():
    """Main entry point."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Interactive Q&A for é‡‘ç›˜ç§‘æŠ€ (val_set)")
    parser.add_argument("--root", type=str, default="data/val_set",
                       help="Root path to val_set directory (default: data/val_set)")
    parser.add_argument("--mode", type=str, default="interactive",
                       choices=["interactive", "batch"],
                       help="Run mode: interactive or batch (default: interactive)")
    parser.add_argument("--questions-file", type=str, default=None,
                       help="JSON file with questions for batch mode")
    parser.add_argument("--hyde", action="store_true", default=True,
                       help="Enable HYDE expansion (default: True)")
    parser.add_argument("--no-hyde", action="store_false", dest="hyde",
                       help="Disable HYDE expansion")
    parser.add_argument("--multi-query", action="store_true", default=True,
                       help="Enable multi-query expansion (default: True)")
    parser.add_argument("--no-multi-query", action="store_false", dest="multi_query",
                       help="Disable multi-query expansion")
    parser.add_argument("--rerank", action="store_true", default=True,
                       help="Enable LLM reranking (default: True)")
    parser.add_argument("--no-rerank", action="store_false", dest="rerank",
                       help="Disable LLM reranking")
    parser.add_argument("--top-n", type=int, default=10,
                       help="Number of chunks to retrieve (default: 10)")
    parser.add_argument("--api-provider", type=str, default="qwen",
                       choices=["qwen", "openai", "gemini"],
                       help="API provider (default: qwen)")
    parser.add_argument("--model", type=str, default="qwen-max",
                       help="Model name (default: qwen-max)")
    
    args = parser.parse_args()
    
    # Convert root path to Path object
    root_path = Path(args.root)
    
    if not root_path.exists():
        print(f"âŒ é”™è¯¯: ç›®å½•ä¸å­˜åœ¨: {root_path}")
        sys.exit(1)
    
    # Initialize system
    val_online = ValOnline(
        root_path=root_path,
        use_hyde=args.hyde,
        use_multi_query=args.multi_query,
        llm_reranking=args.rerank,
        top_n_retrieval=args.top_n,
        api_provider=args.api_provider,
        answering_model=args.model
    )
    
    # Run in selected mode
    if args.mode == "interactive":
        val_online.interactive_mode()
    elif args.mode == "batch":
        if not args.questions_file:
            print("âŒ é”™è¯¯: æ‰¹é‡æ¨¡å¼éœ€è¦ --questions-file å‚æ•°")
            sys.exit(1)
        
        with open(args.questions_file, 'r', encoding='utf-8') as f:
            questions = json.load(f)
        
        results = val_online.batch_mode(questions)
        
        # Save results
        output_file = root_path / f"answers_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_file}")


if __name__ == "__main__":
    main()
