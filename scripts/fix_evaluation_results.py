#!/usr/bin/env python3
"""
é‡æ–°è¯„ä¼°è„šæœ¬ - ä¿®å¤è¯„ä¼°ç»“æœä¸­reasoningä¸ºç©ºçš„é—®é¢˜

è¯¥è„šæœ¬ä¼šï¼š
1. æ‰«ææ‰€æœ‰è¯„ä¼°JSONæ–‡ä»¶
2. è¯†åˆ«reasoningä¸ºç©ºçš„é—®é¢˜
3. ä½¿ç”¨APIProcessoré‡æ–°è¯„ä¼°è¿™äº›é—®é¢˜
4. å¤‡ä»½åŸæ–‡ä»¶å¹¶ç”Ÿæˆä¿®å¤åçš„æ–°æ–‡ä»¶
"""

import json
import sys
import shutil
from pathlib import Path
from typing import List, Dict
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.api_requests import APIProcessor


def find_evaluation_files(val_result_dir: Path) -> List[Path]:
    """æŸ¥æ‰¾æ‰€æœ‰è¯„ä¼°JSONæ–‡ä»¶"""
    return sorted(val_result_dir.glob("evaluation_*.json"))


def backup_file(file_path: Path) -> Path:
    """å¤‡ä»½æ–‡ä»¶ï¼Œæ·»åŠ .backupåç¼€"""
    backup_path = file_path.with_suffix(file_path.suffix + ".backup")
    if backup_path.exists():
        print(f"  âš ï¸  å¤‡ä»½æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡å¤‡ä»½: {backup_path.name}")
    else:
        shutil.copy2(file_path, backup_path)
        print(f"  âœ… å·²å¤‡ä»½: {backup_path.name}")
    return backup_path


def identify_failed_evaluations(data: Dict) -> List[int]:
    """è¯†åˆ«æ‰€æœ‰reasoningä¸ºç©ºæˆ–åŒ…å«è¯„ä¼°å¤±è´¥çš„é—®é¢˜ç´¢å¼•"""
    failed_indices = []
    for i, result in enumerate(data.get("results", [])):
        reasoning = result.get("reasoning", "")
        score = result.get("score", 0.0)
        # æ£€æŸ¥reasoningä¸ºç©ºï¼Œæˆ–åŒ…å«"è¯„ä¼°å¤±è´¥"ï¼Œæˆ–score=0.0ä¸”reasoningåŒ…å«é”™è¯¯ä¿¡æ¯
        if (not reasoning or not reasoning.strip() or 
            "è¯„ä¼°å¤±è´¥" in reasoning or 
            (score == 0.0 and ("è¯„ä¼°è¿”å›çš„reasoningä¸ºç©º" in reasoning or "è¯„ä¼°å¤±è´¥" in reasoning))):
            failed_indices.append(i)
    return failed_indices


def recalculate_statistics(data: Dict) -> Dict:
    """é‡æ–°è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡"""
    results = data.get("results", [])
    total_questions = len(results)
    
    if total_questions == 0:
        return data
    
    correct_count = sum(1 for r in results if r.get("is_correct", False))
    total_score = sum(r.get("score", 0.0) for r in results)
    
    data["evaluated_count"] = total_questions
    data["correct_count"] = correct_count
    data["accuracy"] = correct_count / total_questions if total_questions > 0 else 0.0
    data["average_score"] = total_score / total_questions if total_questions > 0 else 0.0
    
    return data


def fix_evaluation_file(
    file_path: Path,
    api_processor: APIProcessor,
    model: str = "qwen-turbo",
    dry_run: bool = False
) -> Dict[str, any]:
    """ä¿®å¤å•ä¸ªè¯„ä¼°æ–‡ä»¶"""
    print(f"\n{'='*80}")
    print(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {file_path.name}")
    print(f"{'='*80}")
    
    # è¯»å–è¯„ä¼°ç»“æœ
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # è¯†åˆ«éœ€è¦ä¿®å¤çš„é—®é¢˜
    failed_indices = identify_failed_evaluations(data)
    
    if not failed_indices:
        print("  âœ… æ²¡æœ‰å‘ç°éœ€è¦ä¿®å¤çš„é—®é¢˜")
        return {
            "file": file_path.name,
            "total_questions": len(data.get("results", [])),
            "fixed_count": 0,
            "success_count": 0,
            "error_count": 0
        }
    
    print(f"  ğŸ“Š å‘ç° {len(failed_indices)} ä¸ªéœ€è¦ä¿®å¤çš„é—®é¢˜")
    
    # å¤‡ä»½æ–‡ä»¶
    if not dry_run:
        backup_file(file_path)
    
    # ä¿®å¤æ¯ä¸ªé—®é¢˜
    results = data.get("results", [])
    success_count = 0
    error_count = 0
    
    for idx, failed_idx in enumerate(failed_indices, 1):
        result = results[failed_idx]
        question = result.get("question", "")
        standard_answer = result.get("standard_answer", "")
        rag_answer = result.get("rag_answer", "")
        
        print(f"\n  [{idx}/{len(failed_indices)}] ä¿®å¤é—®é¢˜: {question[:50]}...")
        print(f"    RAGç­”æ¡ˆ: {rag_answer[:50]}...")
        
        if not rag_answer or rag_answer.strip() == "":
            print("    âš ï¸  RAGç­”æ¡ˆä¸ºç©ºï¼Œè·³è¿‡")
            error_count += 1
            continue
        
        try:
            if dry_run:
                print("    [DRY RUN] è·³è¿‡å®é™…è¯„ä¼°")
                success_count += 1
            else:
                # é‡æ–°è¯„ä¼°
                eval_result = api_processor.evaluate_answer(
                    question=question,
                    standard_answer=standard_answer,
                    rag_answer=rag_answer,
                    model=model
                )
                
                # éªŒè¯ç»“æœ
                if not eval_result or not isinstance(eval_result, dict):
                    raise ValueError("è¯„ä¼°ç»“æœä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯")
                
                score = eval_result.get("score", 0.0)
                reasoning = eval_result.get("reasoning", "")
                
                if not reasoning or not reasoning.strip():
                    raise ValueError("è¯„ä¼°è¿”å›çš„reasoningä»ä¸ºç©º")
                
                # æ›´æ–°ç»“æœ
                result["score"] = score
                result["reasoning"] = reasoning
                result["is_correct"] = score >= 0.8
                
                print(f"    âœ… ä¿®å¤æˆåŠŸ: score={score:.2f}, is_correct={result['is_correct']}")
                print(f"    ğŸ“ Reasoning: {reasoning[:80]}...")
                success_count += 1
                
        except Exception as e:
            print(f"    âŒ ä¿®å¤å¤±è´¥: {str(e)}")
            error_count += 1
    
    # é‡æ–°è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
    if not dry_run and success_count > 0:
        data = recalculate_statistics(data)
        
        # ä¿å­˜ä¿®å¤åçš„æ–‡ä»¶
        fixed_file_path = file_path.with_name(
            file_path.stem + "_fixed" + file_path.suffix
        )
        with open(fixed_file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"\n  ğŸ’¾ ä¿®å¤åçš„æ–‡ä»¶å·²ä¿å­˜: {fixed_file_path.name}")
        print(f"  ğŸ“ˆ æ›´æ–°åçš„ç»Ÿè®¡:")
        print(f"     - æ­£ç¡®æ•°: {data['correct_count']}/{data['total_questions']}")
        print(f"     - å‡†ç¡®ç‡: {data['accuracy']:.2%}")
        print(f"     - å¹³å‡å¾—åˆ†: {data['average_score']:.3f}")
    
    return {
        "file": file_path.name,
        "total_questions": len(results),
        "failed_count": len(failed_indices),
        "fixed_count": success_count,
        "error_count": error_count
    }


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ä¿®å¤è¯„ä¼°ç»“æœä¸­reasoningä¸ºç©ºçš„é—®é¢˜")
    parser.add_argument(
        "--val-result-dir",
        type=str,
        default="data/val_set/val_result",
        help="è¯„ä¼°ç»“æœç›®å½•è·¯å¾„ï¼ˆé»˜è®¤: data/val_set/val_resultï¼‰"
    )
    parser.add_argument(
        "--model",
        type=str,
        default="qwen-turbo",
        help="è¯„ä¼°ä½¿ç”¨çš„æ¨¡å‹ï¼ˆé»˜è®¤: qwen-turboï¼‰"
    )
    parser.add_argument(
        "--provider",
        type=str,
        default="qwen",
        choices=["qwen", "openai", "gemini", "ibm"],
        help="APIæä¾›å•†ï¼ˆé»˜è®¤: qwenï¼‰"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="è¯•è¿è¡Œæ¨¡å¼ï¼Œä¸å®é™…ä¿®å¤æ–‡ä»¶"
    )
    parser.add_argument(
        "--file",
        type=str,
        help="åªå¤„ç†æŒ‡å®šçš„æ–‡ä»¶ï¼ˆç›¸å¯¹äºval_result_dirï¼‰"
    )
    
    args = parser.parse_args()
    
    # ç¡®å®šè¯„ä¼°ç»“æœç›®å½•
    val_result_dir = Path(project_root) / args.val_result_dir
    if not val_result_dir.exists():
        print(f"âŒ é”™è¯¯: è¯„ä¼°ç»“æœç›®å½•ä¸å­˜åœ¨: {val_result_dir}")
        sys.exit(1)
    
    print(f"ğŸ“‚ è¯„ä¼°ç»“æœç›®å½•: {val_result_dir}")
    print(f"ğŸ¤– è¯„ä¼°æ¨¡å‹: {args.model} ({args.provider})")
    if args.dry_run:
        print("ğŸ” è¯•è¿è¡Œæ¨¡å¼: ä¸ä¼šå®é™…ä¿®æ”¹æ–‡ä»¶")
    print()
    
    # æŸ¥æ‰¾è¯„ä¼°æ–‡ä»¶
    if args.file:
        eval_files = [val_result_dir / args.file]
        if not eval_files[0].exists():
            print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨: {eval_files[0]}")
            sys.exit(1)
    else:
        eval_files = find_evaluation_files(val_result_dir)
    
    if not eval_files:
        print("âŒ æœªæ‰¾åˆ°è¯„ä¼°æ–‡ä»¶")
        sys.exit(1)
    
    print(f"ğŸ“‹ æ‰¾åˆ° {len(eval_files)} ä¸ªè¯„ä¼°æ–‡ä»¶")
    
    # åˆå§‹åŒ–APIå¤„ç†å™¨
    print("\nğŸ”§ åˆå§‹åŒ–APIå¤„ç†å™¨...")
    api_processor = APIProcessor(provider=args.provider)
    print("âœ… APIå¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ\n")
    
    # å¤„ç†æ¯ä¸ªæ–‡ä»¶
    summary = []
    for eval_file in eval_files:
        try:
            result = fix_evaluation_file(
                eval_file,
                api_processor,
                model=args.model,
                dry_run=args.dry_run
            )
            summary.append(result)
        except Exception as e:
            print(f"\nâŒ å¤„ç†æ–‡ä»¶ {eval_file.name} æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            summary.append({
                "file": eval_file.name,
                "error": str(e)
            })
    
    # æ‰“å°æ€»ç»“
    print(f"\n{'='*80}")
    print("ğŸ“Š ä¿®å¤æ€»ç»“")
    print(f"{'='*80}")
    
    total_failed = 0
    total_fixed = 0
    total_errors = 0
    
    for result in summary:
        if "error" in result:
            print(f"âŒ {result['file']}: å¤„ç†å‡ºé”™ - {result['error']}")
        else:
            print(f"ğŸ“„ {result['file']}:")
            print(f"   æ€»é—®é¢˜æ•°: {result['total_questions']}")
            print(f"   éœ€è¦ä¿®å¤: {result.get('failed_count', 0)}")
            print(f"   æˆåŠŸä¿®å¤: {result.get('fixed_count', 0)}")
            print(f"   ä¿®å¤å¤±è´¥: {result.get('error_count', 0)}")
            
            total_failed += result.get('failed_count', 0)
            total_fixed += result.get('fixed_count', 0)
            total_errors += result.get('error_count', 0)
    
    print(f"\næ€»è®¡:")
    print(f"  - éœ€è¦ä¿®å¤çš„é—®é¢˜: {total_failed}")
    print(f"  - æˆåŠŸä¿®å¤: {total_fixed}")
    print(f"  - ä¿®å¤å¤±è´¥: {total_errors}")
    
    if not args.dry_run and total_fixed > 0:
        print(f"\nâœ… ä¿®å¤å®Œæˆï¼ä¿®å¤åçš„æ–‡ä»¶å·²ä¿å­˜ï¼ˆ_fixedåç¼€ï¼‰")
        print(f"   åŸæ–‡ä»¶å·²å¤‡ä»½ï¼ˆ.backupåç¼€ï¼‰")


if __name__ == "__main__":
    main()

