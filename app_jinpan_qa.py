#!/usr/bin/env python3
"""
é‡‘ç›˜ç§‘æŠ€ RAG é—®ç­”ç³»ç»Ÿ - Streamlit å‰ç«¯
åŸºäº val_jinpan_colab.ipynb çš„äº¤äº’å¼æœ¬åœ°å‰ç«¯
"""

import streamlit as st
import sys
import os
from pathlib import Path
import json
from datetime import datetime
import traceback
import pandas as pd
try:
    import fitz  # PyMuPDF
except ImportError:
    fitz = None
from PIL import Image
import io

# é¢„é…ç½® API Keys
os.environ["DASHSCOPE_API_KEY"] = "sk-6a44d15e56dd4007945ccc41b97b499c"
os.environ["GOOGLE_API_KEY"] = "AIzaSyA4pIV3SB-OWYfGZoZjDM_8dbU6Zycpaz8"

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
root_dir = Path(__file__).parent
sys.path.insert(0, str(root_dir))

from src.questions_processing import QuestionsProcessor

# åŠ è½½ subset.csv æ˜ å°„ï¼ˆSHA1 -> æ–‡æ¡£ä¿¡æ¯ï¼‰
@st.cache_data
def load_document_mapping(subset_path: str) -> dict:
    """
    åŠ è½½ subset.csvï¼Œå»ºç«‹ SHA1 -> {company_name, year} çš„æ˜ å°„
    """
    import csv
    mapping = {}
    try:
        with open(subset_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                sha1 = row.get('sha1', '')
                company_name = row.get('company_name', '')
                year = row.get('year', '')
                if sha1:
                    mapping[sha1] = {
                        'company_name': company_name,
                        'year': year,
                        'display_name': f"{company_name} {year}å¹´æŠ¥" if year else company_name
                    }
    except Exception as e:
        st.error(f"åŠ è½½ subset.csv å¤±è´¥: {e}")
    return mapping

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="é‡‘ç›˜ç§‘æŠ€ RAG é—®ç­”ç³»ç»Ÿ",
    page_icon="ğŸ¢",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰CSS
st.markdown("""
    <style>
    /* å¢åŠ ä¾§è¾¹æ å®½åº¦ */
    [data-testid="stSidebar"] {
        min-width: 400px;
        max-width: 450px;
    }
    .main {
        padding: 0rem 1rem;
    }
    .stAlert {
        margin-top: 1rem;
    }
    .answer-box {
        background-color: #f8f9fa;
        padding: 1.5rem;
        border-radius: 0.5rem;
        margin: 1rem 0;
        border: 1px solid #dee2e6;
        color: #212529;
    }
    .question-box {
        background-color: #cfe2ff;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
        border-left: 4px solid #0d6efd;
        color: #052c65;
    }
    .reference-box {
        background-color: #fff3cd;
        padding: 0.8rem;
        border-radius: 0.3rem;
        margin: 0.3rem 0;
        font-size: 0.9rem;
        border: 1px solid #ffecb5;
        color: #664d03;
    }
    /* æ”¹å–„æŒ‰é’®å¯¹æ¯”åº¦ */
    .stButton > button {
        border: 1px solid #dee2e6;
    }
    /* æ”¹å–„æ–‡æœ¬æ¡†å¯¹æ¯”åº¦ */
    .stTextInput > div > div > input {
        background-color: #ffffff;
        border: 2px solid #ced4da;
        color: #212529;
    }
    /* Tab æ ‡ç­¾æ ·å¼ä¼˜åŒ– */
    .stTabs [data-baseweb="tab-list"] button {
        color: #495057;
        font-weight: 500;
    }
    .stTabs [data-baseweb="tab-list"] button[aria-selected="true"] {
        color: #0d6efd;
    }
    /* æˆåŠŸ/è­¦å‘Š/ä¿¡æ¯æ¡†å¯¹æ¯”åº¦å¢å¼º */
    .stSuccess {
        background-color: #d1e7dd;
        color: #0a3622;
    }
    .stWarning {
        background-color: #fff3cd;
        color: #664d03;
    }
    .stInfo {
        background-color: #cfe2ff;
        color: #052c65;
    }
    </style>
""", unsafe_allow_html=True)

# åˆå§‹åŒ–session state
if 'processor' not in st.session_state:
    st.session_state.processor = None
if 'history' not in st.session_state:
    st.session_state.history = []
if 'initialized' not in st.session_state:
    st.session_state.initialized = False
if 'current_question' not in st.session_state:
    st.session_state.current_question = ""
if 'current_schema' not in st.session_state:
    st.session_state.current_schema = "jingpan"
if 'example_clicked' not in st.session_state:
    st.session_state.example_clicked = False
if 'widget_key_counter' not in st.session_state:
    st.session_state.widget_key_counter = 0
if 'enable_multi_turn' not in st.session_state:
    st.session_state.enable_multi_turn = True  # é»˜è®¤å¯ç”¨å¤šè½®å¯¹è¯
if 'context_turns' not in st.session_state:
    st.session_state.context_turns = 3  # é»˜è®¤ä¿ç•™3è½®å†å²

def initialize_system():
    """åˆå§‹åŒ–RAGé—®ç­”ç³»ç»Ÿ"""
    try:
        root_path = Path("data/val_set")
        company_name = "é‡‘ç›˜ç§‘æŠ€"
        
        # æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å­˜åœ¨
        vector_db_dir = root_path / "databases" / "vector_dbs"
        documents_dir = root_path / "databases" / "chunked_reports"
        subset_path = root_path / "subset.csv"
        
        if not documents_dir.exists() or not vector_db_dir.exists():
            st.error("âŒ æ•°æ®åº“ä¸å­˜åœ¨ï¼è¯·å…ˆè¿è¡Œ main.py å¤„ç† PDF æ–‡ä»¶")
            return False
        
        # è·å–é…ç½®
        config = st.session_state.config
        
        with st.spinner("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–é—®ç­”ç³»ç»Ÿ..."):
            processor = QuestionsProcessor(
                vector_db_dir=vector_db_dir,
                documents_dir=documents_dir,
                questions_file_path=None,
                new_challenge_pipeline=True,
                subset_path=subset_path,
                parent_document_retrieval=False,
                llm_reranking=config['llm_reranking'],
                llm_reranking_sample_size=config.get('rerank_sample_size', 50),
                top_n_retrieval=config['top_n_retrieval'],
                parallel_requests=1,
                api_provider=config['api_provider'],
                answering_model=config['answering_model'],
                full_context=False,
                use_hyde=config['use_hyde'],
                use_multi_query=config['use_multi_query'],
                expand_upstream=config.get('expand_upstream', False),
                expand_top_k=config.get('expand_top_k', 5),
                expand_context_size=config.get('expand_context_size', 2)
            )
            
            st.session_state.processor = processor
            st.session_state.company_name = company_name
            st.session_state.initialized = True
            
        return True
    except Exception as e:
        st.error(f"âŒ åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        with st.expander("æŸ¥çœ‹è¯¦ç»†é”™è¯¯"):
            st.code(traceback.format_exc())
        return False

def get_pdf_page_image(pdf_path: str, page_num: int, dpi: int = 150):
    """
    ä»PDFæå–æŒ‡å®šé¡µç çš„å›¾ç‰‡ï¼ˆä½¿ç”¨PyMuPDFï¼‰
    
    Args:
        pdf_path: PDFæ–‡ä»¶è·¯å¾„
        page_num: é¡µç ç´¢å¼•ï¼ˆ**0-based**ï¼Œç¬¬1é¡µ=0ï¼Œç¬¬2é¡µ=1ï¼Œä»¥æ­¤ç±»æ¨ï¼‰
        dpi: å›¾ç‰‡åˆ†è¾¨ç‡ï¼ˆå®é™…ä½¿ç”¨zoomå‚æ•°ï¼‰
    
    Returns:
        PIL Imageå¯¹è±¡æˆ–None
    """
    if fitz is None:
        st.warning("âš ï¸ PyMuPDFæœªå®‰è£…ï¼Œæ— æ³•æ˜¾ç¤ºPDFé¡µé¢å›¾ç‰‡")
        return None
    
    try:
        # æ‰“å¼€PDFæ–‡æ¡£
        doc = fitz.open(pdf_path)
        
        # æ£€æŸ¥é¡µç æ˜¯å¦æœ‰æ•ˆ
        if page_num < 0 or page_num >= len(doc):
            st.warning(f"âš ï¸ é¡µç  {page_num} è¶…å‡ºèŒƒå›´ (æ€»é¡µæ•°: {len(doc)})")
            return None
        
        # è·å–æŒ‡å®šé¡µé¢
        page = doc[page_num]
        
        # è®¾ç½®ç¼©æ”¾æ¯”ä¾‹ (dpi/72ï¼Œå› ä¸ºPDFé»˜è®¤æ˜¯72dpi)
        zoom = dpi / 72
        mat = fitz.Matrix(zoom, zoom)
        
        # æ¸²æŸ“é¡µé¢ä¸ºå›¾ç‰‡
        pix = page.get_pixmap(matrix=mat)
        
        # è½¬æ¢ä¸ºPIL Image
        img_data = pix.tobytes("png")
        img = Image.open(io.BytesIO(img_data))
        
        doc.close()
        return img
        
    except Exception as e:
        st.warning(f"âš ï¸ æ— æ³•æå–PDFé¡µé¢å›¾ç‰‡: {str(e)}")
        return None

def format_answer_display(answer_dict: dict):
    """æ ¼å¼åŒ–å¹¶æ˜¾ç¤ºç­”æ¡ˆ"""
    # è·å–ç­”æ¡ˆ
    answer = answer_dict.get("final_answer", answer_dict.get("answer", "N/A"))
    
    # ä¸»ç­”æ¡ˆ - ä½¿ç”¨æ›´æ˜æ˜¾çš„å¯¹æ¯”è‰²
    st.markdown("### ğŸ“Š ç­”æ¡ˆ")
    st.markdown(f'<div class="answer-box"><h2 style="color: #0d6efd; margin-top: 0; margin-bottom: 0;">ğŸ’¡ {answer}</h2></div>', 
                unsafe_allow_html=True)
    
    # åˆ›å»ºæ ‡ç­¾é¡µ
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ” åˆ†æè¿‡ç¨‹", "ğŸ“ æ¨ç†æ€»ç»“", "ğŸ“š LLMé€‰ç”¨çš„å‚è€ƒ", "ğŸ—‚ï¸ æ‰€æœ‰æ£€ç´¢ç»“æœ"])
    
    with tab1:
        if "step_by_step_analysis" in answer_dict:
            analysis = answer_dict["step_by_step_analysis"]
            if isinstance(analysis, list):
                for i, step in enumerate(analysis, 1):
                    st.markdown(f"**{i}.** {step}")
            else:
                st.write(analysis)
        else:
            st.info("æ— è¯¦ç»†åˆ†æ")
    
    with tab2:
        if "reasoning_summary" in answer_dict:
            st.write(answer_dict["reasoning_summary"])
        else:
            st.info("æ— æ¨ç†æ€»ç»“")
    
    with tab3:
        if "references" in answer_dict and answer_dict["references"]:
            refs = answer_dict["references"]
            
            # åŠ è½½æ–‡æ¡£æ˜ å°„
            doc_mapping = load_document_mapping("data/val_set/subset.csv")
            
            # ç»Ÿè®¡æ ¸å¿ƒé¡µé¢å’Œæ‰©å……é¡µé¢
            core_count = sum(1 for ref in refs if not ref.get('is_expanded', False))
            expanded_count = sum(1 for ref in refs if ref.get('is_expanded', False))
            
            st.markdown(f"### ğŸ“š LLMé€‰ç”¨çš„å‚è€ƒèµ„æ–™")
            
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ä¸Šæ¸¸æ‰©å……
            if "selected_groups" in answer_dict:
                # ä¸Šæ¸¸æ‰©å……æ¨¡å¼ï¼šæ˜¾ç¤ºç»„åˆä¿¡æ¯
                selected_groups = answer_dict["selected_groups"]
                st.caption(f"ğŸ”„ ä½¿ç”¨ä¸Šæ¸¸æ‰©å……æ¨¡å¼ | é€‰ç”¨ {len(selected_groups)} ä¸ªé¡µé¢ç»„åˆ | å…± {len(refs)} é¡µï¼ˆæ ¸å¿ƒé¡µ: {core_count}ï¼Œæ‰©å……é¡µ: {expanded_count}ï¼‰")
                
                # æ˜¾ç¤ºæ¯ä¸ªç»„åˆ
                for group_idx, group in enumerate(selected_groups, 1):
                    core_page = group['core_page']
                    core_score = group['core_score']
                    pages = group['pages']
                    
                    with st.expander(f"ğŸ“¦ ç»„åˆ {group_idx}: æ ¸å¿ƒé¡µ {core_page} (å¾—åˆ†: {core_score:.4f}) - åŒ…å« {len(pages)} é¡µ", expanded=(group_idx == 1)):
                        st.info(f"ğŸ“„ é¡µé¢èŒƒå›´: {pages[0]} - {pages[-1]} | æ ¸å¿ƒé¡µ: {core_page} | ç»„åˆå¾—åˆ†: {core_score:.4f}")
                        
                        # æ˜¾ç¤ºç»„åˆä¸­çš„é¡µé¢
                        group_refs = [r for r in refs if r['page_index'] in pages]
                        for ref in group_refs:
                            page_num = ref['page_index']
                            is_core = not ref.get('is_expanded', False)
                            doc_sha1 = ref.get('pdf_sha1', '')
                            
                            if is_core:
                                badge = 'â­ æ ¸å¿ƒé¡µ'
                                color = '#28a745'
                            else:
                                badge = 'ğŸ“ æ‰©å……é¡µ'
                                color = '#007bff'
                            
                            st.markdown(f'<span style="background-color: {color}; color: white; padding: 2px 6px; border-radius: 3px; font-size: 0.8em;">{badge}</span> ç¬¬ {page_num} é¡µ', unsafe_allow_html=True)
            else:
                # ä¸‹æ¸¸æ‰©å……æ¨¡å¼ï¼šåŸæœ‰æ˜¾ç¤º
                st.caption(f"âœ… æ ¸å¿ƒå¼•ç”¨: {core_count}ä¸ª | ğŸ“ æ‰©å……é¡µé¢: {expanded_count}ä¸ªï¼ˆè‡ªåŠ¨æ·»åŠ ç›¸é‚»é¡µé¢ï¼‰")
            
            # æŒ‰æ–‡æ¡£åˆ†ç»„å¹¶æŒ‰é¡µç æ’åº
            from collections import defaultdict
            doc_groups = defaultdict(list)
            for ref in refs:
                sha1 = ref.get("pdf_sha1", "")
                page = ref.get("page_index", "N/A")
                chunk_text = ref.get("chunk_text", "")
                is_expanded = ref.get("is_expanded", False)
                group_id = ref.get("group_id")
                core_page = ref.get("core_page")
                group_score = ref.get("group_score")
                if sha1 and page != "N/A":
                    doc_groups[sha1].append({
                        'page': page,
                        'text': chunk_text,
                        'is_expanded': is_expanded,
                        'group_id': group_id,
                        'core_page': core_page,
                        'group_score': group_score
                    })
            
            # æŒ‰æ–‡æ¡£æ˜¾ç¤ºï¼Œæ¯ä¸ªæ–‡æ¡£å†…éƒ¨æŒ‰é¡µç æ’åº
            for doc_sha1, pages_data in doc_groups.items():
                # è·å–æ–‡æ¡£æ˜¾ç¤ºåç§°
                doc_info = doc_mapping.get(doc_sha1, {})
                doc_display_name = doc_info.get('display_name', doc_sha1)
                
                # æŒ‰é¡µç æ’åº
                pages_data.sort(key=lambda x: x['page'])
                
                # ç»Ÿè®¡è¯¥æ–‡æ¡£çš„æ ¸å¿ƒå’Œæ‰©å……é¡µé¢æ•°
                doc_core = sum(1 for p in pages_data if not p['is_expanded'])
                doc_expanded = sum(1 for p in pages_data if p['is_expanded'])
                
                # æ˜¾ç¤ºæ–‡æ¡£æ ‡é¢˜
                st.markdown(f"### ğŸ“„ {doc_display_name}")
                st.caption(f"æ ¸å¿ƒå¼•ç”¨: {doc_core}ä¸ª | æ‰©å……é¡µé¢: {doc_expanded}ä¸ª | å…± {len(pages_data)} é¡µ")
                
                # ä¸ºæ¯ä¸ªé¡µç æ˜¾ç¤ºå›¾ç‰‡å’Œæ–‡æœ¬
                for idx, page_data in enumerate(pages_data, 1):
                    page_num = page_data['page']
                    chunk_text = page_data['text']
                    is_expanded = page_data['is_expanded']
                    group_id = page_data.get('group_id')
                    core_page = page_data.get('core_page')
                    group_score = page_data.get('group_score')
                    
                    # æ ¹æ®æ˜¯å¦æ‰©å……é¡µé¢ä½¿ç”¨ä¸åŒçš„å›¾æ ‡å’Œæ ‡ç­¾
                    if is_expanded:
                        icon = "ğŸ“"
                        badge = '<span style="background-color: #007bff; color: white; padding: 2px 8px; border-radius: 3px; font-size: 0.85em;">ğŸ“ ç›¸é‚»æ‰©å……</span>'
                        if group_id is not None and core_page is not None:
                            group_info = f" | ç»„åˆ {group_id + 1}ï¼ˆæ ¸å¿ƒé¡µ: {core_page}ï¼‰"
                        else:
                            group_info = ""
                    else:
                        icon = "âœ…"
                        badge = '<span style="background-color: #28a745; color: white; padding: 2px 8px; border-radius: 3px; font-size: 0.85em; font-weight: bold;">âœ… LLMæ ¸å¿ƒå¼•ç”¨</span>'
                        if group_score is not None:
                            group_info = f" | ç»„åˆå¾—åˆ†: {group_score:.4f}"
                        else:
                            group_info = ""
                    
                    with st.expander(f"{icon} å¼•ç”¨ {idx}: ç¬¬ {page_num} é¡µ{group_info}", expanded=(idx == 1 and not is_expanded)):
                        # æ˜¾ç¤ºé¡µé¢ç±»å‹æ ‡ç­¾
                        st.markdown(badge, unsafe_allow_html=True)
                        st.markdown("")  # ç©ºè¡Œ
                        
                        # æ„å»ºPDFè·¯å¾„
                        pdf_path = Path("data/val_set/pdf_reports") / f"{doc_sha1}.pdf"
                        
                        if pdf_path.exists():
                            # æ˜¾ç¤ºPDFé¡µé¢å›¾ç‰‡
                            st.markdown(f"**ğŸ“– æ–‡æ¡£ç¬¬ {page_num} é¡µ:**")
                            
                            # æå–å¹¶æ˜¾ç¤ºPDFé¡µé¢å›¾ç‰‡
                            # æ³¨æ„ï¼špage_num æ˜¯ 1-basedï¼ˆç¬¬1é¡µã€ç¬¬2é¡µ...ï¼‰ï¼Œä½† PyMuPDF ä½¿ç”¨ 0-based ç´¢å¼•
                            page_image = get_pdf_page_image(str(pdf_path), page_num - 1)
                            if page_image:
                                st.image(page_image, use_container_width=True, caption=f"{doc_sha1} - é¡µç  {page_num}")
                            else:
                                st.warning("æ— æ³•åŠ è½½é¡µé¢å›¾ç‰‡")
                        else:
                            st.warning(f"æœªæ‰¾åˆ°PDFæ–‡ä»¶: {doc_sha1}.pdf")
                        
                        # æ˜¾ç¤ºæ–‡æœ¬æ‘˜å½•
                        if chunk_text:
                            st.markdown("**ğŸ“ ç›¸å…³æ–‡æœ¬æ‘˜å½•:**")
                            st.caption(chunk_text[:300] + "..." if len(chunk_text) > 300 else chunk_text)
        else:
            st.info("æ— å¼•ç”¨ä¿¡æ¯")
        
        # æ˜¾ç¤ºæºæ–‡æ¡£SHA1
        if "source_sha1" in answer_dict:
            st.markdown(f"**ğŸ“„ ä¸»è¦æ¥æº:** `{answer_dict['source_sha1']}`")
    
    with tab4:
        # æ˜¾ç¤ºæ‰€æœ‰æ£€ç´¢åˆ°çš„chunks
        if "all_retrieved_chunks" in answer_dict and answer_dict["all_retrieved_chunks"]:
            all_chunks = answer_dict["all_retrieved_chunks"]
            
            # åŠ è½½æ–‡æ¡£æ˜ å°„
            doc_mapping = load_document_mapping("data/val_set/subset.csv")
            
            st.markdown(f"### ğŸ” æ£€ç´¢åˆ° {len(all_chunks)} ä¸ªç›¸å…³æ–‡æœ¬å—")
            st.caption("âœ¨ æ ‡è®°ä¸º **LLMé€‰ç”¨** çš„æ˜¯æ¨¡å‹æœ€ç»ˆå¼•ç”¨çš„æ–‡æœ¬å—")
            
            # ç»Ÿè®¡ä¿¡æ¯
            llm_selected_count = sum(1 for chunk in all_chunks if chunk.get('selected_by_llm', False))
            
            # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨äº†é‡æ’åºï¼ˆå¦‚æœæœ‰combined_scoreåˆ™ä½¿ç”¨äº†é‡æ’åºï¼‰
            has_reranking = any(chunk.get('combined_score') is not None for chunk in all_chunks)
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("æ€»æ£€ç´¢æ•°", len(all_chunks))
            with col2:
                st.metric("LLMé€‰ç”¨", llm_selected_count, delta=f"{llm_selected_count}/{len(all_chunks)}")
            with col3:
                if has_reranking:
                    # è¿‡æ»¤æ‰ None å€¼ï¼Œåªè®¡ç®—æœ‰æ•ˆçš„ combined_score
                    valid_scores = [chunk.get('combined_score', 0) for chunk in all_chunks if chunk.get('combined_score') is not None]
                    avg_combined = sum(valid_scores) / len(valid_scores) if valid_scores else 0
                    st.metric("å¹³å‡ç»„åˆå¾—åˆ†", f"{avg_combined:.4f}")
                else:
                    # è¿‡æ»¤æ‰ None å€¼ï¼Œåªè®¡ç®—æœ‰æ•ˆçš„ vector_score
                    valid_scores = [chunk.get('vector_score', 0) for chunk in all_chunks if chunk.get('vector_score') is not None]
                    avg_vector = sum(valid_scores) / len(valid_scores) if valid_scores else 0
                    st.metric("å¹³å‡å‘é‡å¾—åˆ†", f"{avg_vector:.4f}")
            with col4:
                if has_reranking:
                    st.info("âœ… ä½¿ç”¨äº†LLMé‡æ’åº")
                else:
                    st.info("ğŸ“Š çº¯å‘é‡æ£€ç´¢")
            
            st.markdown("---")
            
            # æŒ‰å¾—åˆ†æ’åºæ˜¾ç¤º
            for chunk in all_chunks:
                rank = chunk.get('rank', 0)
                page = chunk.get('page', 'N/A')
                source_sha1 = chunk.get('source_sha1', '')
                text = chunk.get('text', '')
                vector_score = chunk.get('vector_score', 0.0)
                relevance_score = chunk.get('relevance_score', None)
                combined_score = chunk.get('combined_score', None)
                reasoning = chunk.get('reasoning', '')
                selected = chunk.get('selected_by_llm', False)
                is_expanded = chunk.get('is_expanded', False)  # æ˜¯å¦ä¸ºæ‰©å……çš„ç›¸é‚»é¡µé¢
                
                # è·å–æ–‡æ¡£æ˜¾ç¤ºåç§°
                doc_info = doc_mapping.get(source_sha1, {})
                doc_display_name = doc_info.get('display_name', source_sha1)
                
                # æ ¹æ®é¡µé¢çŠ¶æ€ï¼Œä½¿ç”¨ä¸åŒçš„æ ·å¼
                if selected:
                    icon = "â­"
                    badge = '<span style="background-color: #28a745; color: white; padding: 2px 8px; border-radius: 3px; font-size: 0.85em; font-weight: bold;">âœ… LLMæ ¸å¿ƒå¼•ç”¨</span>'
                    border_color = "#28a745"
                elif is_expanded:
                    icon = "ğŸ“"
                    badge = '<span style="background-color: #007bff; color: white; padding: 2px 8px; border-radius: 3px; font-size: 0.85em;">ğŸ“ ç›¸é‚»æ‰©å……</span>'
                    border_color = "#007bff"
                else:
                    icon = "ğŸ“„"
                    badge = '<span style="background-color: #6c757d; color: white; padding: 2px 8px; border-radius: 3px; font-size: 0.85em;">æœªé€‰ç”¨</span>'
                    border_color = "#dee2e6"
                
                # æ„å»ºæ˜¾ç¤ºçš„å¾—åˆ†ä¿¡æ¯
                if combined_score is not None:
                    score_display = f"ç»„åˆå¾—åˆ†: {combined_score:.4f}"
                else:
                    score_display = f"å‘é‡å¾—åˆ†: {vector_score:.4f}"
                
                # æ ‡è®°æ–‡æœ¬
                status_text = ""
                if selected:
                    status_text = "â­"
                elif is_expanded:
                    status_text = "ğŸ“"
                
                # æ˜¾ç¤ºæ¯ä¸ªchunk
                with st.expander(
                    f"{icon} æ’å #{rank} - {doc_display_name} ç¬¬{page}é¡µ - {score_display} {status_text}",
                    expanded=(rank == 1 and selected)
                ):
                    # é¡¶éƒ¨ä¿¡æ¯æ 
                    st.markdown(f"""
                    <div style="background-color: #f8f9fa; padding: 10px; border-radius: 5px; margin-bottom: 10px; border-left: 4px solid {border_color};">
                        <div style="display: flex; justify-content: space-between; align-items: center;">
                            <div>
                                <strong>ğŸ“ æ–‡æ¡£:</strong> {doc_display_name} | 
                                <strong>ğŸ“„ é¡µç :</strong> {page} |
                                <strong>ğŸ† æ’å:</strong> #{rank}
                            </div>
                            <div>
                                {badge}
                            </div>
                        </div>
                        <div style="margin-top: 8px; font-size: 0.9em;">
                            <strong>ğŸ”— SHA1:</strong> <code>{source_sha1}</code>
                        </div>
                    </div>
                    """, unsafe_allow_html=True)
                    
                    # è¯¦ç»†å¾—åˆ†æ„æˆ
                    st.markdown("**ğŸ“Š å¾—åˆ†è¯¦æƒ…:**")
                    score_cols = st.columns(3)
                    with score_cols[0]:
                        st.metric("å‘é‡ç›¸ä¼¼åº¦", f"{vector_score:.6f}", help="åŸºäºåµŒå…¥å‘é‡çš„è¯­ä¹‰ç›¸ä¼¼åº¦å¾—åˆ†ï¼ˆè¶Šé«˜è¶Šç›¸ä¼¼ï¼‰")
                    with score_cols[1]:
                        if relevance_score is not None:
                            st.metric("LLMç›¸å…³æ€§", f"{relevance_score:.6f}", help="LLMåˆ¤æ–­çš„ç›¸å…³æ€§å¾—åˆ†ï¼ˆ0-1ä¹‹é—´ï¼‰")
                        else:
                            st.metric("LLMç›¸å…³æ€§", "æœªä½¿ç”¨", help="æœªå¯ç”¨LLMé‡æ’åº")
                    with score_cols[2]:
                        if combined_score is not None:
                            st.metric("ç»„åˆå¾—åˆ†", f"{combined_score:.6f}", help="å‘é‡å¾—åˆ†ä¸LLMå¾—åˆ†çš„åŠ æƒç»„åˆ")
                        else:
                            st.metric("ç»„åˆå¾—åˆ†", "æœªä½¿ç”¨", help="æœªå¯ç”¨LLMé‡æ’åº")
                    
                    # LLMæ¨ç†è¿‡ç¨‹ï¼ˆå¦‚æœæœ‰ï¼‰
                    if reasoning and selected:
                        st.markdown("**ğŸ¤” LLMæ¨ç†è¿‡ç¨‹:**")
                        st.info(reasoning)
                    
                    # PDFé¢„è§ˆï¼ˆå¦‚æœæ˜¯LLMé€‰ç”¨çš„ï¼‰
                    if selected:
                        pdf_path = Path("data/val_set/pdf_reports") / f"{source_sha1}.pdf"
                        if pdf_path.exists():
                            st.markdown("**ğŸ“– PDFé¡µé¢é¢„è§ˆ:**")
                            page_image = get_pdf_page_image(str(pdf_path), page - 1)
                            if page_image:
                                st.image(page_image, use_container_width=True, caption=f"{doc_display_name} - ç¬¬{page}é¡µ")
                    
                    # æ–‡æœ¬å†…å®¹
                    st.markdown("**ğŸ“ æ–‡æœ¬å†…å®¹:**")
                    st.text_area(
                        "æ–‡æœ¬",
                        text,
                        height=150,
                        key=f"chunk_{rank}_{page}_{source_sha1}",
                        label_visibility="collapsed"
                    )
        else:
            st.info("æ— æ£€ç´¢ç»“æœä¿¡æ¯")

def save_history():
    """ä¿å­˜é—®ç­”å†å²"""
    if st.session_state.history:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"qa_history_{timestamp}.json"
        
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(st.session_state.history, f, ensure_ascii=False, indent=2)
        
        return filename
    return None

def prepare_conversation_history(max_turns: int) -> list:
    """
    å‡†å¤‡å¯¹è¯å†å²ç”¨äºå¤šè½®å¯¹è¯
    
    Args:
        max_turns: æœ€å¤šä¿ç•™çš„å†å²è½®æ•°
    
    Returns:
        æ ¼å¼åŒ–çš„å†å²å¯¹è¯åˆ—è¡¨ï¼ŒåŒ…å«é—®é¢˜å’Œç®€åŒ–çš„ç­”æ¡ˆ
    """
    if not st.session_state.history or not st.session_state.enable_multi_turn:
        return None
    
    # è·å–æœ€è¿‘çš„Nè½®å¯¹è¯
    recent_history = st.session_state.history[-max_turns:] if len(st.session_state.history) > max_turns else st.session_state.history
    
    # æ ¼å¼åŒ–å†å²è®°å½•ï¼ˆæå–å…³é”®ä¿¡æ¯ï¼‰
    formatted_history = []
    for record in recent_history:
        question = record.get('question', '')
        answer_dict = record.get('answer', {})
        
        # æå–å…³é”®ç­”æ¡ˆä¿¡æ¯ï¼ˆä¼˜å…ˆä½¿ç”¨æ¨ç†æ‘˜è¦ï¼Œå…¶æ¬¡ä½¿ç”¨æœ€ç»ˆç­”æ¡ˆï¼‰
        if 'reasoning_summary' in answer_dict and answer_dict['reasoning_summary']:
            answer = answer_dict['reasoning_summary']
        elif 'final_answer' in answer_dict:
            answer = str(answer_dict['final_answer'])
        else:
            answer = 'N/A'
        
        formatted_history.append({
            'question': question,
            'answer': answer
        })
    
    return formatted_history

# ==================== ä¾§è¾¹æ é…ç½® ====================
with st.sidebar:
    st.title("âš™ï¸ ç³»ç»Ÿé…ç½®")
    
    # æ˜¾ç¤ºæ¨èé…ç½®æç¤º
    with st.expander("âœ¨ å½“å‰é…ç½® - æ¨èé…ç½®", expanded=False):
        st.markdown("""
        **ğŸ¯ æ¨èè®¾ç½®ï¼ˆå·²åº”ç”¨ï¼‰**
        
        âœ… æ£€ç´¢æ•°é‡ï¼š10  
        âœ… HYDEï¼šå¼€å¯  
        âœ… Multi-Queryï¼šå¼€å¯  
        âœ… LLMé‡æ’åºï¼šå¼€å¯  
        âœ… åˆå§‹å¬å›ï¼š50  
        âœ… ä¸Šæ¸¸æ‰©å……ï¼šå¼€å¯  
        âœ… æ ¸å¿ƒé¡µé¢ï¼š5  
        âœ… æ‰©å……é¡µæ•°ï¼šä¸Šä¸‹å„2é¡µ  
        âœ… å¤šè½®å¯¹è¯ï¼šå…³é—­  
        
        ğŸ’¡ è¿™äº›é…ç½®åœ¨å¤§å¤šæ•°åœºæ™¯ä¸‹æ•ˆæœæœ€ä½³
        """)
    
    # åˆå§‹åŒ–é»˜è®¤é…ç½®
    if 'config' not in st.session_state:
        st.session_state.config = {
            'api_provider': 'qwen',
            'answering_model': 'qwen-max',
            'top_n_retrieval': 10,
            'use_hyde': True,  # âœ… å·²æ”¹ç”¨ Qwen API
            'use_multi_query': True,  # âœ… å·²æ”¹ç”¨ Qwen API
            'llm_reranking': True,
            'rerank_sample_size': 50
        }
    
    st.markdown("---")
    st.subheader("ğŸ¤– æ¨¡å‹é…ç½®")
    
    api_provider = st.selectbox(
        "API æä¾›å•†",
        options=['qwen', 'openai', 'gemini'],
        index=0,
        help="é€‰æ‹©å¤§è¯­è¨€æ¨¡å‹APIæä¾›å•†"
    )
    
    # æ ¹æ®APIæä¾›å•†æ˜¾ç¤ºä¸åŒçš„æ¨¡å‹é€‰é¡¹
    if api_provider == 'qwen':
        model_options = ['qwen-max', 'qwen-plus', 'qwen-turbo']
        default_model = 'qwen-max'
    elif api_provider == 'openai':
        model_options = ['gpt-4o', 'gpt-4o-mini', 'gpt-4-turbo']
        default_model = 'gpt-4o-mini'
    else:  # gemini
        model_options = ['gemini-1.5-pro', 'gemini-1.5-flash']
        default_model = 'gemini-1.5-pro'
    
    answering_model = st.selectbox(
        "å›ç­”æ¨¡å‹",
        options=model_options,
        index=0,
        help="ç”¨äºç”Ÿæˆç­”æ¡ˆçš„æ¨¡å‹"
    )
    
    st.markdown("---")
    
    st.markdown("### âš™ï¸ åŸºç¡€æ£€ç´¢")
    
    top_n_retrieval = st.slider(
        "ğŸ“Š æœ€ç»ˆæ£€ç´¢æ•°é‡",
        min_value=5,
        max_value=30,
        value=10,
        step=5,
        help="ç»è¿‡é‡æ’åºåæœ€ç»ˆè¿”å›ç»™LLMçš„æ–‡æ¡£å—æ•°é‡"
    )
    
    st.markdown("---")
    st.markdown("### ğŸš€ æ£€ç´¢å¢å¼º")
    
    use_hyde = st.checkbox(
        "âœ¨ HYDEï¼ˆå‡è®¾æ€§æ–‡æ¡£æ‰©å±•ï¼‰",
        value=True,
        help="ç”Ÿæˆå‡è®¾æ€§ç­”æ¡ˆè¾…åŠ©æ£€ç´¢ï¼Œæé«˜è¯­ä¹‰åŒ¹é…åº¦"
    )
    
    use_multi_query = st.checkbox(
        "ğŸ”„ Multi-Queryï¼ˆå¤šæŸ¥è¯¢æ‰©å±•ï¼‰",
        value=True,
        help="ç”Ÿæˆå¤šä¸ªç›¸å…³æŸ¥è¯¢å¹¶è¡Œæ£€ç´¢ï¼Œæé«˜å¬å›ç‡"
    )
    
    st.markdown("---")
    st.markdown("### ğŸ¯ LLMé‡æ’åº")
    
    llm_reranking = st.checkbox(
        "ğŸ§  å¯ç”¨ LLM é‡æ’åº",
        value=True,
        help="ä½¿ç”¨LLMæ™ºèƒ½è¯„ä¼°ç›¸å…³æ€§å¹¶é‡æ–°æ’åºï¼Œæ˜¾è‘—æé«˜ç²¾ç¡®åº¦"
    )
    
    if llm_reranking:
        rerank_sample_size = st.slider(
            "ğŸ” åˆå§‹å¬å›æ•°é‡",
            min_value=20,
            max_value=100,
            value=50,
            step=10,
            help="LLMé‡æ’åºå‰å…ˆå¬å›çš„å€™é€‰chunksæ•°é‡ï¼ˆæ›´å¤š=æ›´å…¨é¢ä½†æ›´æ…¢ï¼‰"
        )
        st.success(f"âœ… **æ¨èé…ç½®**\n\nğŸ¯ æ£€ç´¢æµç¨‹ï¼šå¬å› **{rerank_sample_size}** ä¸ªå€™é€‰ â†’ LLMé‡æ’åº â†’ è¿”å›å‰ **{top_n_retrieval}** ä¸ª")
        
        # ä¸Šæ¸¸æ‰©å……é…ç½®
        st.markdown("---")
        st.markdown("### ğŸ”„ ä¸Šæ¸¸æ‰©å……ï¼ˆæ¨èï¼‰")
        
        expand_upstream = st.checkbox(
            "ğŸ“ˆ å¯ç”¨ä¸Šæ¸¸æ‰©å……",
            value=True,
            help="âœ¨ æ¨èå¼€å¯ï¼åœ¨ç­”æ¡ˆç”Ÿæˆå‰æ‰©å……é¡µé¢ç»„åˆï¼Œè®©LLMåŸºäºæ›´å®Œæ•´çš„ä¸Šä¸‹æ–‡ç”Ÿæˆé«˜è´¨é‡ç­”æ¡ˆ"
        )
        
        if expand_upstream:
            col1, col2 = st.columns(2)
            with col1:
                expand_top_k = st.slider(
                    "æ ¸å¿ƒé¡µé¢æ•°",
                    min_value=3,
                    max_value=10,
                    value=5,
                    help="é€‰å–é‡æ’åºåçš„å‰Kä¸ªé¡µé¢ä½œä¸ºæ ¸å¿ƒ"
                )
            with col2:
                expand_context_size = st.slider(
                    "ä¸Šä¸‹æ‰©å……é¡µæ•°",
                    min_value=1,
                    max_value=3,
                    value=2,
                    help="æ¯ä¸ªæ ¸å¿ƒé¡µé¢ä¸Šä¸‹å„æ‰©å……Né¡µ"
                )
            
            estimated_pages = expand_top_k * (2 * expand_context_size + 1)
            st.info(f"ğŸ“Š **æ‰©å……é¢„è§ˆ**\n\n{expand_top_k} ä¸ªæ ¸å¿ƒé¡µ Ã— {2*expand_context_size+1} é¡µ/ç»„ â‰ˆ **{estimated_pages}** é¡µ â†’ å»é‡åçº¦ **20-40** é¡µ")
            
            # Tokenä¼°ç®—å’Œè­¦å‘Š
            estimated_tokens = estimated_pages * 800  # å‡è®¾æ¯é¡µå¹³å‡800 tokens
            if estimated_tokens > 25000:
                st.error(f"ğŸš¨ **Tokenè¶…é™è­¦å‘Š**\n\né¢„è®¡ **{estimated_tokens:,}** tokensï¼Œå¯èƒ½è¶…è¿‡APIé™åˆ¶ï¼\n\nğŸ’¡ **å»ºè®®**ï¼šexpand_top_k â‰¤ 5 æˆ– expand_context_size = 1")
            elif estimated_tokens > 15000:
                st.warning(f"âš ï¸ **Tokenæ¶ˆè€—è¾ƒé«˜**\n\né¢„è®¡ **{estimated_tokens:,}** tokensï¼Œå“åº”æ—¶é—´å¯èƒ½è¾ƒé•¿")
            else:
                st.success(f"âœ… **Tokenæ¶ˆè€—é€‚ä¸­**\n\né¢„è®¡ **{estimated_tokens:,}** tokens")
        else:
            expand_top_k = 5
            expand_context_size = 2
            st.info("ğŸ’¡ **ä¸‹æ¸¸æ‰©å……æ¨¡å¼**\n\nLLMç”Ÿæˆç­”æ¡ˆåæ‰©å……ï¼Œä»…ç”¨äºå±•ç¤ºå‚è€ƒèµ„æ–™ï¼ˆä¸å½±å“ç­”æ¡ˆè´¨é‡ï¼‰")
    else:
        rerank_sample_size = 10  # ä¸å¯ç”¨æ—¶é»˜è®¤å€¼
        expand_upstream = False
        expand_top_k = 5
        expand_context_size = 2
    
    # å¤šè½®å¯¹è¯è®¾ç½®
    st.markdown("---")
    st.markdown("### ğŸ’¬ å¤šè½®å¯¹è¯è®¾ç½®")
    
    enable_multi_turn = st.checkbox(
        "å¯ç”¨å¤šè½®å¯¹è¯",
        value=False,
        help="å¯ç”¨åï¼Œç³»ç»Ÿä¼šè®°ä½å†å²å¯¹è¯ï¼Œç†è§£ä¸Šä¸‹æ–‡å’ŒæŒ‡ä»£å…³ç³»ï¼ˆå¯èƒ½å¢åŠ tokenæ¶ˆè€—ï¼‰",
        key="multi_turn_checkbox"
    )
    st.session_state.enable_multi_turn = enable_multi_turn
    
    if enable_multi_turn:
        context_turns = st.slider(
            "ä¿ç•™å¯¹è¯è½®æ•°",
            min_value=1,
            max_value=10,
            value=st.session_state.context_turns,
            step=1,
            help="è®¾ç½®ä¿ç•™å¤šå°‘è½®å†å²å¯¹è¯ä½œä¸ºä¸Šä¸‹æ–‡ï¼ˆè½®æ•°è¶Šå¤šï¼Œtokenæ¶ˆè€—è¶Šå¤§ï¼‰",
            key="context_turns_slider"
        )
        st.session_state.context_turns = context_turns
        
        st.info(f"ğŸ’¡ å½“å‰å°†ä¿ç•™æœ€è¿‘ **{context_turns}** è½®å¯¹è¯ä½œä¸ºä¸Šä¸‹æ–‡")
    else:
        st.warning("âš ï¸ å¤šè½®å¯¹è¯å·²å…³é—­ï¼Œæ¯æ¬¡é—®ç­”ç›¸äº’ç‹¬ç«‹")
    
    # æ£€æµ‹é…ç½®å˜åŒ–
    new_config = {
        'api_provider': api_provider,
        'answering_model': answering_model,
        'top_n_retrieval': top_n_retrieval,
        'use_hyde': use_hyde,
        'use_multi_query': use_multi_query,
        'llm_reranking': llm_reranking,
        'rerank_sample_size': rerank_sample_size,
        'expand_upstream': expand_upstream,
        'expand_top_k': expand_top_k,
        'expand_context_size': expand_context_size
    }
    
    # å¦‚æœé…ç½®æ”¹å˜ä¸”ç³»ç»Ÿå·²åˆå§‹åŒ–ï¼Œéœ€è¦é‡æ–°åˆå§‹åŒ–
    if st.session_state.initialized and st.session_state.config != new_config:
        st.session_state.initialized = False
        st.session_state.processor = None
        st.warning("âš ï¸ æ£€æµ‹åˆ°é…ç½®å˜åŒ–ï¼Œç³»ç»Ÿå°†åœ¨ä¸‹æ¬¡æŸ¥è¯¢æ—¶é‡æ–°åˆå§‹åŒ–")
    
    # æ›´æ–°é…ç½®
    st.session_state.config = new_config
    
    st.markdown("---")
    st.subheader("ğŸ“Š ç³»ç»ŸçŠ¶æ€")
    
    if st.session_state.initialized:
        st.success("âœ… ç³»ç»Ÿå·²åˆå§‹åŒ–")
        st.info(f"ğŸ¢ å…¬å¸: {st.session_state.company_name}")
        st.info(f"ğŸ’¬ å†å²é—®ç­”: {len(st.session_state.history)} æ¡")
    else:
        st.warning("âš ï¸ ç³»ç»Ÿæœªåˆå§‹åŒ–")
    
    # åˆå§‹åŒ–æŒ‰é’®
    if st.button("ğŸ”„ é‡æ–°åˆå§‹åŒ–ç³»ç»Ÿ", use_container_width=True):
        st.session_state.initialized = False
        st.session_state.processor = None
        st.rerun()
    
    # ä¿å­˜å†å²æŒ‰é’®
    if st.button("ğŸ’¾ ä¿å­˜é—®ç­”å†å²", use_container_width=True):
        filename = save_history()
        if filename:
            st.success(f"âœ… å†å²å·²ä¿å­˜åˆ° {filename}")
        else:
            st.warning("âš ï¸ æ— å†å²è®°å½•å¯ä¿å­˜")
    
    # æ¸…ç©ºå†å²æŒ‰é’®
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯å†å²", use_container_width=True, type="secondary"):
        if st.session_state.history:
            st.session_state.history = []
            st.success("âœ… å¯¹è¯å†å²å·²æ¸…ç©º")
            st.rerun()
        else:
            st.info("â„¹ï¸ å½“å‰æ— å†å²è®°å½•")
    
    # æ¸…ç©ºå†å²æŒ‰é’®
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå†å²", use_container_width=True):
        st.session_state.history = []
        st.success("âœ… å†å²å·²æ¸…ç©º")
        st.rerun()
    
    st.markdown("---")
    st.markdown("### ğŸ“– ä½¿ç”¨è¯´æ˜")
    st.markdown("""
    1. **é…ç½®æ¨¡å‹**: é€‰æ‹©APIæä¾›å•†å’Œæ¨¡å‹
    2. **è°ƒæ•´å‚æ•°**: è®¾ç½®æ£€ç´¢æ•°é‡å’Œå¢å¼ºç­–ç•¥
    3. **è¾“å…¥é—®é¢˜**: åœ¨ä¸»ç•Œé¢è¾“å…¥é—®é¢˜
    4. **é€‰æ‹©ç±»å‹**: é€‰æ‹©æœŸæœ›çš„ç­”æ¡ˆç±»å‹
    5. **æŸ¥çœ‹ç»“æœ**: ç³»ç»Ÿè¿”å›ç­”æ¡ˆå’Œåˆ†æè¿‡ç¨‹
    
    **ç­”æ¡ˆç±»å‹è¯´æ˜**:
    - `jingpan`: é‡‘ç›˜ç§‘æŠ€ä¸“ç”¨ï¼ˆä¸­æ–‡è´¢åŠ¡ï¼‰
    - `number`: æ•°å­—ç±»ç­”æ¡ˆ
    - `boolean`: æ˜¯/å¦ç±»ç­”æ¡ˆ
    - `name`: å•ä¸ªåç§°
    - `names`: å¤šä¸ªåç§°åˆ—è¡¨
    """)

# ==================== ä¸»ç•Œé¢ ====================
st.title("ğŸ¢ é‡‘ç›˜ç§‘æŠ€ RAG é—®ç­”ç³»ç»Ÿ")
st.markdown("åŸºäº **FAISS + Qwen + æ—¶é—´è·¯ç”±** çš„æ™ºèƒ½è´¢åŠ¡é—®ç­”ç³»ç»Ÿ")

# åˆå§‹åŒ–ç³»ç»Ÿ
if not st.session_state.initialized:
    if initialize_system():
        st.success("âœ… ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸï¼")
        st.rerun()
    else:
        st.stop()

# é—®ç­”åŒºåŸŸ
st.markdown("---")

# å¦‚æœç‚¹å‡»äº†ç¤ºä¾‹é—®é¢˜ï¼Œæ˜¾ç¤ºæç¤º
if st.session_state.get('example_clicked', False):
    st.success(f"âœ… å·²é€‰æ‹©ç¤ºä¾‹é—®é¢˜ï¼š**{st.session_state.current_question}**")
    st.session_state.example_clicked = False

# å›ºå®šä½¿ç”¨ jingpan æ¨¡å¼ï¼ˆæ·±åº¦åˆ†ææ¨¡å¼ï¼‰
schema_type = "jingpan"

# æ·»åŠ è¯´æ˜ä¿¡æ¯
st.info("ğŸ’¡ **æ·±åº¦åˆ†ææ¨¡å¼**ï¼šç³»ç»Ÿå°†ä¸ºæ‚¨æä¾›è¯¦ç»†çš„ç­”æ¡ˆã€æ¨ç†è¿‡ç¨‹å’Œæ•°æ®æ¥æºï¼Œé€‚ç”¨äºæ‰€æœ‰ç±»å‹çš„é—®é¢˜ã€‚")

# ä½¿ç”¨åŠ¨æ€ key ä»¥ä¾¿æ¯æ¬¡ç‚¹å‡»ç¤ºä¾‹é—®é¢˜åé‡æ–°æ¸²æŸ“
question_key = f"question_input_{st.session_state.widget_key_counter}"
question_input = st.text_input(
    "ğŸ’¬ è¯·è¾“å…¥æ‚¨çš„é—®é¢˜",
    value=st.session_state.current_question,
    placeholder="ä¾‹å¦‚ï¼š2024å¹´ç¬¬ä¸€å­£åº¦çš„è¥ä¸šæ”¶å…¥æ˜¯å¤šå°‘ï¼Ÿ",
    help="ç³»ç»Ÿä¼šè‡ªåŠ¨æ·»åŠ å…¬å¸åç§°ï¼ˆé‡‘ç›˜ç§‘æŠ€ï¼‰åˆ°é—®é¢˜å‰",
    key=question_key
)

# é—®ç­”æŒ‰é’®
if st.button("ğŸš€ è·å–ç­”æ¡ˆ", type="primary", use_container_width=True):
    if not question_input.strip():
        st.warning("âš ï¸ è¯·è¾“å…¥é—®é¢˜")
    else:
        # ç¡®ä¿é—®é¢˜åŒ…å«å…¬å¸åç§°
        company_name = st.session_state.company_name
        if company_name not in question_input:
            full_question = f"{company_name}{question_input}"
        else:
            full_question = question_input
        
        # æ˜¾ç¤ºé—®é¢˜
        st.markdown(f'<div class="question-box"><b>â“ é—®é¢˜:</b> {full_question}<br><b>ğŸ“ åˆ†ææ¨¡å¼:</b> æ·±åº¦åˆ†æ (jingpan)</div>', 
                   unsafe_allow_html=True)
        
        try:
            # å‡†å¤‡å¯¹è¯å†å²ï¼ˆå¦‚æœå¯ç”¨å¤šè½®å¯¹è¯ï¼‰
            conversation_history = prepare_conversation_history(st.session_state.context_turns)
            
            # æ˜¾ç¤ºå¤šè½®å¯¹è¯çŠ¶æ€
            if conversation_history:
                st.info(f"ğŸ”„ å¤šè½®å¯¹è¯æ¨¡å¼ï¼šä½¿ç”¨æœ€è¿‘ {len(conversation_history)} è½®å¯¹è¯ä½œä¸ºä¸Šä¸‹æ–‡")
            
            # åˆ›å»ºè¿›åº¦æ¡å®¹å™¨
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # å®šä¹‰è¿›åº¦å›è°ƒå‡½æ•°
            def update_progress(stage: str, progress: int):
                """æ›´æ–°è¿›åº¦æ¡çš„å›è°ƒå‡½æ•°"""
                status_text.text(stage)
                progress_bar.progress(progress)
            
            # è°ƒç”¨é—®ç­”ç³»ç»Ÿï¼Œä¼ å…¥çœŸå®çš„è¿›åº¦å›è°ƒ
            answer_dict = st.session_state.processor.get_answer_for_company(
                company_name=company_name,
                question=full_question,
                schema=schema_type,
                conversation_history=conversation_history,
                progress_callback=update_progress
            )
            
            # å®Œæˆ
            import time
            status_text.text("âœ… å¤„ç†å®Œæˆï¼")
            progress_bar.progress(100)
            time.sleep(0.5)
            
            # æ¸…é™¤è¿›åº¦æ˜¾ç¤º
            progress_bar.empty()
            status_text.empty()
            
            # æ˜¾ç¤ºç­”æ¡ˆ
            format_answer_display(answer_dict)
            
            # ä¿å­˜åˆ°å†å²
            st.session_state.history.append({
                'timestamp': datetime.now().isoformat(),
                'question': full_question,
                'schema': schema_type,
                'answer': answer_dict
            })
            
            st.success("âœ… é—®ç­”å®Œæˆï¼")
            
        except Exception as e:
            error_msg = str(e)
            
            # ç‰¹æ®Šå¤„ç†400é”™è¯¯ï¼ˆé€šå¸¸æ˜¯Tokenè¶…é™ï¼‰
            if "400" in error_msg or "Bad Request" in error_msg:
                st.error("âŒ **APIè¯·æ±‚å¤±è´¥ï¼š400 Bad Request**")
                st.markdown("""
                **å¯èƒ½åŸå› **ï¼š
                - ğŸš¨ **Tokenè¶…é™**ï¼šä¸Šæ¸¸æ‰©å……å¯¼è‡´ä¸Šä¸‹æ–‡è¿‡é•¿ï¼ˆ46é¡µçº¦36,800 tokensï¼‰
                - âš ï¸ APIå‚æ•°é”™è¯¯æˆ–æ ¼å¼ä¸æ­£ç¡®
                
                **è§£å†³æ–¹æ³•**ï¼š
                1. **é™ä½ expand_top_k**ï¼šä» 10 é™è‡³ 3-5
                2. **é™ä½ expand_context_size**ï¼šä» 2 é™è‡³ 1
                3. **å…³é—­ä¸Šæ¸¸æ‰©å……**ï¼šä½¿ç”¨ä¼ ç»Ÿçš„ä¸‹æ¸¸æ‰©å……æ¨¡å¼
                4. æ£€æŸ¥ä¾§è¾¹æ çš„Tokené¢„ä¼°ï¼Œç¡®ä¿ä¸è¶…è¿‡ 25,000 tokens
                
                **æ¨èé…ç½®**ï¼ˆé€‚åˆå¤§éƒ¨åˆ†åœºæ™¯ï¼‰ï¼š
                - expand_top_k = 5
                - expand_context_size = 2
                - é¢„è®¡Token: ~20,000 âœ…
                """)
            else:
                st.error(f"âŒ å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {error_msg}")
            
            with st.expander("æŸ¥çœ‹è¯¦ç»†é”™è¯¯"):
                st.code(traceback.format_exc())

# ç¤ºä¾‹é—®é¢˜ï¼ˆä»é—®é¢˜åº“åŠ è½½ï¼‰
st.markdown("---")
st.markdown("### ğŸ’¡ æŠ•èµ„è€…å…³æ³¨é—®é¢˜")
st.markdown("ç‚¹å‡»ä¸‹æ–¹é—®é¢˜å¯è‡ªåŠ¨å¡«å…¥è¾“å…¥æ¡† | å…±127ä¸ªçœŸå®æŠ•èµ„è€…é—®é¢˜")

# åŠ è½½é—®é¢˜åº“
try:
    questions_df = pd.read_csv("data/val_set/questions.csv")
    
    # è·å–æ‰€æœ‰é—®é¢˜ç±»å‹
    question_types = questions_df['é—®é¢˜ç±»å‹'].unique().tolist()
    
    # åˆ›å»ºé—®é¢˜ç±»å‹é€‰æ‹©å™¨
    col_select, col_random = st.columns([3, 1])
    
    with col_select:
        selected_type = st.selectbox(
            "é€‰æ‹©é—®é¢˜ç±»å‹",
            options=["å…¨éƒ¨"] + sorted(question_types),
            index=0,
            key="question_type_selector"
        )
    
    with col_random:
        if st.button("ğŸ² éšæœºé—®é¢˜", use_container_width=True):
            random_q = questions_df.sample(1)['æé—®å†…å®¹'].values[0]
            st.session_state.current_question = random_q
            st.session_state.current_schema = "jingpan"
            st.session_state.example_clicked = True
            st.session_state.widget_key_counter += 1
            st.rerun()
    
    # ç­›é€‰é—®é¢˜
    if selected_type == "å…¨éƒ¨":
        filtered_questions = questions_df
    else:
        filtered_questions = questions_df[questions_df['é—®é¢˜ç±»å‹'] == selected_type]
    
    # æ˜¾ç¤ºé—®é¢˜ç»Ÿè®¡
    st.info(f"ğŸ“Š å½“å‰ç±»å‹å…± **{len(filtered_questions)}** ä¸ªé—®é¢˜")
    
    # åˆ†3åˆ—å±•ç¤ºé—®é¢˜
    col1, col2, col3 = st.columns(3)
    
    # å°†é—®é¢˜åˆ†é…åˆ°3åˆ—
    questions_list = filtered_questions['æé—®å†…å®¹'].tolist()
    
    # æœ€å¤šæ˜¾ç¤º15ä¸ªé—®é¢˜ï¼ˆé¿å…é¡µé¢è¿‡é•¿ï¼‰
    display_limit = 15
    if len(questions_list) > display_limit:
        questions_list = questions_list[:display_limit]
        st.warning(f"âš ï¸ ä»…æ˜¾ç¤ºå‰ {display_limit} ä¸ªé—®é¢˜ï¼Œå¯é€šè¿‡ç±»å‹ç­›é€‰æŸ¥çœ‹æ›´å¤š")
    
    # å¹³å‡åˆ†é…åˆ°3åˆ—
    questions_per_col = len(questions_list) // 3 + (1 if len(questions_list) % 3 > 0 else 0)
    
    for idx, col in enumerate([col1, col2, col3]):
        with col:
            start_idx = idx * questions_per_col
            end_idx = min((idx + 1) * questions_per_col, len(questions_list))
            
            for i in range(start_idx, end_idx):
                q = questions_list[i]
                # æˆªæ–­è¿‡é•¿çš„é—®é¢˜ç”¨äºæŒ‰é’®æ˜¾ç¤º
                button_text = q if len(q) <= 50 else q[:47] + "..."
                
                if st.button(button_text, key=f"ex_q_{i}_{hash(q) % 10000}", use_container_width=True):
                    # æ›´æ–°é—®é¢˜ï¼ˆå›ºå®šä½¿ç”¨ jingpanï¼‰
                    st.session_state.current_question = q
                    st.session_state.current_schema = "jingpan"
                    st.session_state.example_clicked = True
                    # å¢åŠ è®¡æ•°å™¨ï¼Œå¼ºåˆ¶é‡æ–°æ¸²æŸ“è¾“å…¥æ¡†
                    st.session_state.widget_key_counter += 1
                    st.rerun()

except FileNotFoundError:
    st.warning("âš ï¸ é—®é¢˜åº“æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œæ˜¾ç¤ºé»˜è®¤ç¤ºä¾‹é—®é¢˜")
    
    # é»˜è®¤ç¤ºä¾‹é—®é¢˜ï¼ˆä½œä¸ºåå¤‡ï¼‰
    example_col1, example_col2 = st.columns(2)
    
    with example_col1:
        st.markdown("**ğŸ“Š è´¢åŠ¡æ•°æ®ç±»**")
        examples_financial = [
            "2024å¹´ç¬¬ä¸€å­£åº¦çš„è¥ä¸šæ”¶å…¥æ˜¯å¤šå°‘ï¼Ÿ",
            "2023å¹´åˆ°2025å¹´çš„å‡€åˆ©æ¶¦å¯¹æ¯”",
            "æˆªè‡³2025å¹´9æœˆ30æ—¥çš„æ€»èµ„äº§",
        ]
        for q in examples_financial:
            if st.button(q, key=f"ex_fin_{q[:10]}", use_container_width=True):
                st.session_state.current_question = q
                st.session_state.current_schema = "jingpan"
                st.session_state.example_clicked = True
                st.session_state.widget_key_counter += 1
                st.rerun()
    
    with example_col2:
        st.markdown("**ğŸ“ ä¿¡æ¯æŸ¥è¯¢ç±»**")
        examples_info = [
            "å…¬å¸çš„æ³•å®šä»£è¡¨äººæ˜¯è°ï¼Ÿ",
            "2024å¹´æœ‰å“ªäº›ä¸»è¦äº§å“ï¼Ÿ",
            "å…¬å¸æ˜¯å¦æœ‰æµ·å¤–ä¸šåŠ¡ï¼Ÿ",
        ]
        for q in examples_info:
            if st.button(q, key=f"ex_info_{q[:10]}", use_container_width=True):
                st.session_state.current_question = q
                st.session_state.current_schema = "jingpan"
                st.session_state.example_clicked = True
                st.session_state.widget_key_counter += 1
                st.rerun()

except Exception as e:
    st.error(f"âŒ åŠ è½½é—®é¢˜åº“æ—¶å‡ºé”™: {str(e)}")

# å†å²è®°å½•å±•ç¤º
if st.session_state.history:
    st.markdown("---")
    st.markdown("### ğŸ“œ é—®ç­”å†å²")
    
    with st.expander(f"æŸ¥çœ‹å†å²è®°å½•ï¼ˆå…± {len(st.session_state.history)} æ¡ï¼‰", expanded=False):
        for i, record in enumerate(reversed(st.session_state.history), 1):
            # ä½¿ç”¨æ›´æ¸…æ™°çš„å®¹å™¨å±•ç¤ºæ¯æ¡è®°å½•
            with st.container():
                st.markdown(f"#### ğŸ“‹ è®°å½• {i}")
                st.markdown(f"ğŸ• **æ—¶é—´**: {record['timestamp']}")
                st.markdown(f"â“ **é—®é¢˜**: {record['question']}")
                st.markdown(f"ğŸ“ **ç±»å‹**: `{record['schema']}`")
                
                answer = record['answer'].get('final_answer', record['answer'].get('answer', 'N/A'))
                st.markdown(f"ğŸ’¡ **ç­”æ¡ˆ**: **{answer}**")
                st.markdown("---")

# é¡µè„š
st.markdown("---")
st.markdown("""
<div style="text-align: center; color: #666; font-size: 0.9rem;">
    <p>ğŸ”§ é‡‘ç›˜ç§‘æŠ€ RAG é—®ç­”ç³»ç»Ÿ | åŸºäº FAISS + Qwen-max + æ—¶é—´æ™ºèƒ½è·¯ç”±</p>
    <p>ğŸ’¡ æ”¯æŒå¤šå¹´ä»½å¯¹æ¯”ã€æ™ºèƒ½æ£€ç´¢å¢å¼ºï¼ˆHYDE + Multi-Query + LLMé‡æ’åºï¼‰</p>
</div>
""", unsafe_allow_html=True)
