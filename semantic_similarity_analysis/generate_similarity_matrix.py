#!/usr/bin/env python3
"""
è¯­ä¹‰ç›¸ä¼¼åº¦çŸ©é˜µç”Ÿæˆå™¨
ä¸ºè´¢æŠ¥æ–‡æ¡£æ„å»ºè¯­ä¹‰ç›¸ä¼¼åº¦çŸ©é˜µï¼Œå¹¶ç”Ÿæˆäº¤äº’å¼çƒ­åº¦å›¾å¯è§†åŒ–
"""

import faiss
import numpy as np
import json
from pathlib import Path
from tqdm import tqdm
import plotly.graph_objects as go
from typing import Dict, Tuple
import time
import warnings
warnings.filterwarnings('ignore')


class SimilarityMatrixGenerator:
    """è¯­ä¹‰ç›¸ä¼¼åº¦çŸ©é˜µç”Ÿæˆå™¨"""
    
    def __init__(self, vector_db_dir: Path, documents_dir: Path, output_dir: Path):
        """
        åˆå§‹åŒ–ç”Ÿæˆå™¨
        
        Args:
            vector_db_dir: FAISS å‘é‡æ•°æ®åº“ç›®å½•
            documents_dir: æ–‡æ¡£ JSON ç›®å½•
            output_dir: è¾“å‡ºç›®å½•
        """
        self.vector_db_dir = Path(vector_db_dir)
        self.documents_dir = Path(documents_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.stats = {}  # å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯
    
    def load_vectors_from_faiss(self, faiss_path: Path) -> np.ndarray:
        """
        ä» FAISS ç´¢å¼•ä¸­æå–æ‰€æœ‰å‘é‡ï¼ˆä¼˜åŒ–ç‰ˆï¼Œæ‰¹é‡æå–ï¼‰
        
        Args:
            faiss_path: FAISS æ–‡ä»¶è·¯å¾„
            
        Returns:
            å‘é‡æ•°ç»„ (n_vectors, dimension)
        """
        index = faiss.read_index(str(faiss_path))
        n_vectors = index.ntotal
        
        print(f"   - æå– {n_vectors:,} ä¸ªå‘é‡ï¼ˆç»´åº¦: {index.d}ï¼‰...")
        
        # ä¼˜åŒ–ï¼šæ‰¹é‡é‡æ„å‘é‡
        vectors = np.zeros((n_vectors, index.d), dtype=np.float32)
        batch_size = 1000  # æ¯æ¬¡æå–1000ä¸ªå‘é‡
        
        with tqdm(total=n_vectors, desc="   æå–å‘é‡", ncols=80, leave=False) as pbar:
            for start_idx in range(0, n_vectors, batch_size):
                end_idx = min(start_idx + batch_size, n_vectors)
                batch_ids = np.arange(start_idx, end_idx, dtype=np.int64)
                vectors[start_idx:end_idx] = index.reconstruct_batch(batch_ids)
                pbar.update(end_idx - start_idx)
        
        return vectors
    
    def compute_cosine_similarity_matrix(self, vectors: np.ndarray) -> np.ndarray:
        """
        è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µï¼ˆä¼˜åŒ–ç‰ˆï¼‰
        
        Args:
            vectors: å‘é‡æ•°ç»„ (n_vectors, dimension)
            
        Returns:
            ç›¸ä¼¼åº¦çŸ©é˜µ (n_vectors, n_vectors)
        """
        n = vectors.shape[0]
        
        # ä¼°è®¡å†…å­˜ä½¿ç”¨
        matrix_size_mb = (n * n * 4) / (1024 * 1024)  # float32 = 4 bytes
        print(f"   - çŸ©é˜µå¤§å°: {n} Ã— {n} (~{matrix_size_mb:.1f} MB)")
        
        # å½’ä¸€åŒ–å‘é‡
        print("   - å½’ä¸€åŒ–å‘é‡...")
        norms = np.linalg.norm(vectors, axis=1, keepdims=True)
        normalized_vectors = vectors / (norms + 1e-8)  # é¿å…é™¤é›¶
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ (ç‚¹ç§¯) - ä½¿ç”¨ @ è¿ç®—ç¬¦ï¼ˆæ›´å¿«ï¼‰
        print("   - è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ...")
        start_time = time.time()
        similarity_matrix = normalized_vectors @ normalized_vectors.T
        elapsed = time.time() - start_time
        print(f"   - çŸ©é˜µè®¡ç®—å®Œæˆï¼ˆè€—æ—¶: {elapsed:.2f}ç§’ï¼‰")
        
        # ç¡®ä¿å¯¹è§’çº¿ä¸º 1ï¼Œå¤„ç†æ•°å€¼è¯¯å·®
        np.fill_diagonal(similarity_matrix, 1.0)
        
        # é™åˆ¶èŒƒå›´åœ¨ [-1, 1]
        similarity_matrix = np.clip(similarity_matrix, -1.0, 1.0)
        
        return similarity_matrix
    
    def load_document_info(self, doc_path: Path) -> Dict:
        """
        åŠ è½½æ–‡æ¡£å…ƒä¿¡æ¯
        
        Args:
            doc_path: æ–‡æ¡£ JSON è·¯å¾„
            
        Returns:
            æ–‡æ¡£ä¿¡æ¯å­—å…¸
        """
        with open(doc_path, 'r', encoding='utf-8') as f:
            doc = json.load(f)
        
        return {
            'name': doc['metainfo']['sha1_name'],
            'company': doc['metainfo'].get('company_name', 'Unknown'),
            'chunks_count': len(doc['content']['chunks']),
            'pages': doc['metainfo'].get('pages_amount', 0),
            'text_blocks': doc['metainfo'].get('text_blocks_amount', 0),
            'tables': doc['metainfo'].get('tables_amount', 0),
        }
    
    def generate_heatmap(
        self, 
        similarity_matrix: np.ndarray, 
        doc_info: Dict,
        output_path: Path
    ):
        """
        ç”Ÿæˆäº¤äº’å¼çƒ­åº¦å›¾
        
        Args:
            similarity_matrix: ç›¸ä¼¼åº¦çŸ©é˜µ
            doc_info: æ–‡æ¡£ä¿¡æ¯
            output_path: è¾“å‡º HTML æ–‡ä»¶è·¯å¾„
        """
        n = similarity_matrix.shape[0]
        
        # åˆ›å»º Plotly çƒ­åº¦å›¾
        fig = go.Figure(data=go.Heatmap(
            z=similarity_matrix,
            x=list(range(n)),
            y=list(range(n)),
            colorscale=[
                [0.0, 'rgb(0, 0, 255)'],      # è“è‰² (ä½ç›¸ä¼¼åº¦)
                [0.5, 'rgb(255, 255, 255)'],  # ç™½è‰² (ä¸­ç­‰)
                [1.0, 'rgb(255, 0, 0)']       # çº¢è‰² (é«˜ç›¸ä¼¼åº¦)
            ],
            zmid=0.5,  # ä¸­é—´å€¼è®¾ç½®ä¸º 0.5
            zmin=0,
            zmax=1,
            colorbar=dict(
                title=dict(text='ç›¸ä¼¼åº¦', side='right'),
                tickmode='linear',
                tick0=0,
                dtick=0.1
            ),
            hovertemplate='Chunk %{x} â†” Chunk %{y}<br>ç›¸ä¼¼åº¦: %{z:.4f}<extra></extra>'
        ))
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        # æ’é™¤å¯¹è§’çº¿çš„ç»Ÿè®¡
        mask = ~np.eye(n, dtype=bool)
        off_diagonal = similarity_matrix[mask]
        
        avg_similarity = np.mean(off_diagonal)
        median_similarity = np.median(off_diagonal)
        std_similarity = np.std(off_diagonal)
        max_similarity = np.max(off_diagonal)
        min_similarity = np.min(off_diagonal)
        
        # å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯
        self.stats[doc_info['name']] = {
            'chunks_count': n,
            'avg_similarity': float(avg_similarity),
            'median_similarity': float(median_similarity),
            'std_similarity': float(std_similarity),
            'max_similarity': float(max_similarity),
            'min_similarity': float(min_similarity),
            'company': doc_info['company'],
            'pages': doc_info['pages'],
            'text_blocks': doc_info['text_blocks'],
            'tables': doc_info['tables']
        }
        
        # è®¾ç½®å¸ƒå±€
        title_text = (
            f"<b>{doc_info['name']} - {doc_info['company']}</b><br>"
            f"<sub>Chunks: {n:,} | å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.4f} | "
            f"ä¸­ä½æ•°: {median_similarity:.4f} | æ ‡å‡†å·®: {std_similarity:.4f}</sub>"
        )
        
        fig.update_layout(
            title={
                'text': title_text,
                'x': 0.5,
                'xanchor': 'center',
                'font': {'size': 16}
            },
            xaxis_title='Chunk ç´¢å¼•',
            yaxis_title='Chunk ç´¢å¼•',
            width=1200,
            height=1200,
            xaxis=dict(
                showgrid=False,
                zeroline=False,
                showticklabels=True,
                tickfont=dict(size=10)
            ),
            yaxis=dict(
                showgrid=False,
                zeroline=False,
                showticklabels=True,
                tickfont=dict(size=10),
                autorange='reversed'  # Y è½´ä»ä¸Šåˆ°ä¸‹
            ),
            plot_bgcolor='white',
            paper_bgcolor='white'
        )
        
        # ä¿å­˜ä¸º HTML
        print("   - æ­£åœ¨ä¿å­˜ HTML æ–‡ä»¶...")
        save_start = time.time()
        fig.write_html(
            str(output_path),
            config={
                'displayModeBar': True,
                'displaylogo': False,
                'toImageButtonOptions': {
                    'format': 'png',
                    'filename': f'{doc_info["name"]}_similarity_matrix',
                    'height': 1200,
                    'width': 1200,
                    'scale': 2
                }
            }
        )
        save_elapsed = time.time() - save_start
        
        # è·å–æ–‡ä»¶å¤§å°
        file_size_mb = output_path.stat().st_size / (1024 * 1024)
        
        print(f"âœ… ç”Ÿæˆçƒ­åº¦å›¾: {output_path.name}")
        print(f"   - Chunks æ•°é‡: {n:,}")
        print(f"   - å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.4f}")
        print(f"   - ç›¸ä¼¼åº¦èŒƒå›´: [{min_similarity:.4f}, {max_similarity:.4f}]")
        print(f"   - HTML æ–‡ä»¶å¤§å°: {file_size_mb:.1f} MB")
        print(f"   - ä¿å­˜è€—æ—¶: {save_elapsed:.2f}ç§’")
    
    def process_document(self, doc_name: str):
        """
        å¤„ç†å•ä¸ªæ–‡æ¡£
        
        Args:
            doc_name: æ–‡æ¡£åç§°ï¼ˆä¸å«æ‰©å±•åï¼‰
        """
        doc_start_time = time.time()
        
        print(f"\n{'='*60}")
        print(f"ğŸ“„ å¤„ç†æ–‡æ¡£: {doc_name}")
        print(f"{'='*60}")
        
        # æ–‡ä»¶è·¯å¾„
        faiss_path = self.vector_db_dir / f"{doc_name}.faiss"
        doc_path = self.documents_dir / f"{doc_name}.json"
        output_path = self.output_dir / f"{doc_name}_similarity_matrix.html"
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not faiss_path.exists():
            print(f"âŒ æœªæ‰¾åˆ° FAISS æ–‡ä»¶: {faiss_path}")
            return
        
        if not doc_path.exists():
            print(f"âŒ æœªæ‰¾åˆ°æ–‡æ¡£æ–‡ä»¶: {doc_path}")
            return
        
        # åŠ è½½æ–‡æ¡£ä¿¡æ¯
        print("\nğŸ“– [1/3] åŠ è½½æ–‡æ¡£ä¿¡æ¯...")
        doc_info = self.load_document_info(doc_path)
        
        # ä» FAISS æå–å‘é‡
        print("\nğŸ” [2/3] ä» FAISS æå–å‘é‡...")
        vectors = self.load_vectors_from_faiss(faiss_path)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        print("\nğŸ§® [3/3] è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ...")
        similarity_matrix = self.compute_cosine_similarity_matrix(vectors)
        
        # ç”Ÿæˆçƒ­åº¦å›¾
        print("\nğŸ¨ ç”Ÿæˆäº¤äº’å¼çƒ­åº¦å›¾...")
        self.generate_heatmap(similarity_matrix, doc_info, output_path)
        
        # æ€»è€—æ—¶
        total_time = time.time() - doc_start_time
        print(f"\nâ±ï¸  æ–‡æ¡£å¤„ç†æ€»è€—æ—¶: {total_time:.2f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
    
    def process_all_documents(self):
        """å¤„ç†æ‰€æœ‰æ–‡æ¡£"""
        overall_start_time = time.time()
        
        # è·å–æ‰€æœ‰ FAISS æ–‡ä»¶
        faiss_files = sorted(self.vector_db_dir.glob("*.faiss"))
        doc_names = [f.stem for f in faiss_files]
        
        print(f"\n{'='*60}")
        print(f"ğŸš€ è¯­ä¹‰ç›¸ä¼¼åº¦çŸ©é˜µç”Ÿæˆå™¨")
        print(f"{'='*60}")
        print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"ğŸ“Š å¾…å¤„ç†æ–‡æ¡£: {len(doc_names)} ä¸ª")
        print(f"{'='*60}\n")
        
        success_count = 0
        failed_docs = []
        
        for idx, doc_name in enumerate(doc_names, 1):
            print(f"\nğŸ”„ è¿›åº¦: [{idx}/{len(doc_names)}]")
            try:
                self.process_document(doc_name)
                success_count += 1
            except Exception as e:
                print(f"\nâŒ å¤„ç† {doc_name} æ—¶å‡ºé”™: {e}")
                failed_docs.append(doc_name)
                import traceback
                traceback.print_exc()
        
        # æ€»ç»“
        overall_time = time.time() - overall_start_time
        print(f"\n{'='*60}")
        print(f"âœ… å¤„ç†å®Œæˆï¼")
        print(f"{'='*60}")
        print(f"âœ“ æˆåŠŸ: {success_count}/{len(doc_names)} ä¸ªæ–‡æ¡£")
        if failed_docs:
            print(f"âœ— å¤±è´¥: {', '.join(failed_docs)}")
        print(f"â±ï¸  æ€»è€—æ—¶: {overall_time:.2f}ç§’ ({overall_time/60:.1f}åˆ†é’Ÿ)")
        print(f"{'='*60}\n")
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats_path = self.output_dir / "statistics.json"
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(self.stats, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜è‡³: {stats_path}")
        
        return self.stats


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*70)
    print("  è¯­ä¹‰ç›¸ä¼¼åº¦çŸ©é˜µç”Ÿæˆå™¨ v2.0 (ä¼˜åŒ–ç‰ˆ)")
    print("  Semantic Similarity Matrix Generator")
    print("="*70)
    
    # è®¾ç½®è·¯å¾„
    base_path = Path(__file__).parent.parent / "data" / "val_set"
    vector_db_dir = base_path / "databases" / "vector_dbs"
    documents_dir = base_path / "databases" / "chunked_reports"
    output_dir = Path(__file__).parent / "outputs"
    
    # æ£€æŸ¥è·¯å¾„
    print(f"\nğŸ“‚ æ£€æŸ¥æ•°æ®è·¯å¾„...")
    if not vector_db_dir.exists():
        print(f"âŒ å‘é‡æ•°æ®åº“ç›®å½•ä¸å­˜åœ¨: {vector_db_dir}")
        return
    print(f"âœ“ å‘é‡æ•°æ®åº“ç›®å½•: {vector_db_dir}")
    
    if not documents_dir.exists():
        print(f"âŒ æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨: {documents_dir}")
        return
    print(f"âœ“ æ–‡æ¡£ç›®å½•: {documents_dir}")
    print(f"âœ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    # åˆ›å»ºç”Ÿæˆå™¨å¹¶å¤„ç†æ‰€æœ‰æ–‡æ¡£
    generator = SimilarityMatrixGenerator(vector_db_dir, documents_dir, output_dir)
    stats = generator.process_all_documents()
    
    print(f"\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶ä½ç½®: {output_dir}")
    print(f"ğŸ’¡ æç¤º: ç”¨æµè§ˆå™¨æ‰“å¼€ HTML æ–‡ä»¶å³å¯æŸ¥çœ‹äº¤äº’å¼çƒ­åº¦å›¾\n")


if __name__ == "__main__":
    main()

